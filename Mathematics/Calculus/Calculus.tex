\documentclass[a4paper,12pt]{report}
\setcounter{secnumdepth}{5}
\setcounter{tocdepth}{3}
\input{/usr/share/LaTeX-ToolKit/template.tex}
\begin{document}
\title{Calculus}
\author{沈威宇}
\date{\temtoday}
\titletocdoc
\chapter{Calculus (微積分)}
\section{Limit (極限)}
\subsection{Limits of Real Sequences}
Below, we are discussing limits of sequences with domain $\mathbb{N}$ and codomain $\mathbb{R}$. For sequences $\langle a_n\rangle_{i=l}^\infty$ in $\mathbb{R}$, its limit is the same as the limit of another sequence $\langle b_n=a_{n+l-1}\rangle_{i=1}^\infty$.
\subsubsection{Definition}
For a real sequence \(\langle a_n\rangle\), the limit of \(\langle a_n\rangle\) (as $n$ approaches infinity), denoted as $\lim_{n \to \infty} a_n$ or $\lim_n a_n$, is defined as follows:
\[\lim_{n \to \infty} a_n = L \equiv \forall \epsilon > 0:\, \exists M \in\mathbb{N}\text{\ s.t.\ } n\in\mathbb{N}\land n \geq M\implies |a_n - L| < \epsilon.\]
In other words, as \(n\) becomes arbitrarily large, \(a_n\) gets arbitrarily close to \(L\).

If such $M$ exists, we say the limit exists or the sequence converges (收斂) to $L$; otherwise, we say the limit doesn't exist or the sequence diverge (發散).
\subsubsection{Infinite Limits}
\[\lim_{n\to \infty}a_n=\infty \equiv \forall M > 0, \exists M \in\mathbb{N} \text{\ s.t.\ } n\in\mathbb{N}\land n \geq M \implies a_n > M.\]
\[\lim_{n\to \infty}a_n=-\infty \equiv \forall M < 0, \exists M \in\mathbb{N} \text{\ s.t.\ } n\in\mathbb{N}\land n \geq M \implies a_n < M.\]
Note that if a limit equals to $\infty$ or $-\infty$, the limit doesn't exist.
\sssc{The Uniqueness of Limits}
If a limit of a real sequence exists, it is unique.
\sssc{Preservation of equal to}
Given real sequences $\langle a_n\rangle$ and $\langle b_n\rangle$ which for all $k$ that is greater than or equal to a specific $j\in \mathbb{N}$, $a_k=b_k$. If $\lim_{n\to\infty}a_n=L\in\mathbb{R}\cup\{-\infty,\infty\}$, then
\[\lim_{n\to\infty}b_n=L.\]
\sssc{Preservation of less than or equal to}
Given real sequences $\langle a_n\rangle$ and $\langle b_n\rangle$ which for all $k$ that is greater than or equal to a specific $j\in \mathbb{N}$, $a_k\leq b_k$. If both $\lim_{n\to\infty}a_n$ and $\lim_{n\to\infty}b_n$ are in $\mathbb{R}\cup\{-\infty,\infty\}$, then
\[\lim_{n\to\infty}a_n\leq\lim_{n\to\infty}b_n.\]
If $\lim_{n\to\infty}a_n=\infty$, then $\lim_{n\to\infty}b_n=\infty$; if $\lim_{n\to\infty}b_n=-\infty$, then $\lim_{n\to\infty}a_n=-\infty$.
\sssc{Squeeze (夾擠) theorem or Sandwich (三明治) theorem}
Given real sequences $\langle a_n\rangle$, $\langle b_n\rangle$, and $\langle c_n\rangle$ which for all $k$ that is greater than or equal to a specific $j\in \mathbb{N}$:
\[a_k\leq c_k\leq b_k.\]
If
\[\lim_{n\to\infty}a_n=\lim_{n\to\infty}b_n=L\in\mathbb{R}\cup\{-\infty,\infty\},\]
then
\[\lim_{n\to\infty}c_n=L.\]
\subsubsection{Monotone Convergence Theorem (單調收斂定理) or Completeness (Axiom) of the Real Numbers (實數的完備性)}
\textit{Statement.}
\begin{enumerate}[label=(\Alph*)]
\item For a non-decreasing and bounded-above sequence of real numbers $\langle a_n\rangle_{n\in\mathbb {N}}$:
\[\lim_{n\to\infty}a_n=\sup_n a_n.\]
\item For a non-increasing and bounded-below sequence of real numbers $\langle a_n\rangle_{n\in\mathbb {N}}$:
\[\lim_{n\to\infty}a_n=\inf_n a_n.\]
\end{enumerate}
\begin{proof}
Let $\{a_{n}\}$ be the set of values of $\langle a_n\rangle_{n\in\mathbb {N}}$. By assumption, $\{a_n\}$ is non-empty and bounded-above by $\sup_n a_n$. Let $c=\sup_n a_n$.
\[\forall\epsilon>0:\,\exists M\in\mathbb {N}\text{\ s.t.\ }c\geq a_M>c-\epsilon,\]
since otherwise $c-\epsilon$ is a strictly smaller upper bound of $\langle a_n\rangle$, contradicting the definition of the supremum. 

Then since $\langle a_n\rangle$ is non-decreasing, and $c$ is an upper bound:
\[\forall\epsilon>0:\,\exists M\in\mathbb {N}\text{\ s.t.\ }\forall n\geq M:\,|c-a_n|=c-a_n\leq c-a_M=|c-a_M|<\epsilon.\]
The proof of the (B) part is analogous or follows from (A) by considering $\langle -a_{n}\rangle_{n\in \mathbb{N}}$.
\end{proof}
\textit{Statement.}

If $\langle a_n\rangle_{n\in\mathbb {N}}$ is a monotone sequence of real numbers, i.e., if 
$a_n\leq a_{n+1}$ for every $n\geq 1$ or $a_n\geq a_{n+1}$ for every $n\geq 1$, then this sequence has a finite limit if and only if the sequence is bounded.
\begin{proof}
"If"-direction: The proof follows directly from the proposition.

"Only If"-direction: By $(\epsilon,\delta)$-definition of limit, every sequence $\langle a_n\rangle_{n\in\mathbb {N}}$ with a finite limit $L$ is necessarily bounded.
\end{proof}
\sssc{Bolzano–Weierstrass Theorem (波爾查諾-魏爾斯特拉斯定理)}
Every bounded sequence in an Euclidean space $\bbR^n$ has a convergent subsequence.

\begin{proof}
    For $\bbR$, take any bounded sequence $(x_n)$ in $\bbR$. Then there exist real numbers $m,M$ such that
    \[m\leq x_n\leq M,\quad\forall n.\]
    So the sequence lies in the closed interval $[m,M]$.

    For $\varepsilon=1$. Divide $[m,M]$ into disjoint subintervals with length $\frac{M-m}{2^\varepsilon}$:
    \[\qty[m,\frac{m+M}{2}],\qty[\frac{m+M}{2},M].\]
    Since $(x_n)$ is infinite, by the pigeonhole principle, one of the subintervals must contain infinitely many terms of $(x_n)$. Denote it by $I_1$ and pick the subsequence $\qty(x_n^{(1)})$ of $(x_n)$ that is entirely in $I_1$.

    Repeat it for $\varepsilon=2,3,\ldots,$ by dividing $I_{\varepsilon-1}$ into two halves, we can construct a nested sequence of subsequences
    \[\qty(x_n^{(1)})\supseteq \qty(x_n^{(2)})\supseteq\ldots\]
    where $\qty(x_n^{(k)})$ lies in a closed interval of length $\frac{M-m}{2^k}$, $I_k$.

    Pick the diagonal sequence $y_k=x_k^{(k)}$. Then
    \[y_k\in\qty(x_n^{(k)})\subseteq I_k,\]
    and the sequence $(y_k)$ is a convergent subsequence of $(x_n)$ because
    \[\forall N\in\bbN\colon m,n\geq N\implies |y_m-y_n|\leq\frac{M-m}{2^N}.\]

    To generalized to $\bbR^n$, consider each coordinate sequence in $\bbR$, it has a convergent subsequence. Diagonalization gives a subsequence converging in all coordinates.
\end{proof}
\subsection{Limits of Real Series}
\subsubsection{Definition}
Let:
\[S_n = \sum_{i=1}^n a_i,\]
where \(a_i\) are terms of a real sequence. The limit of \(S_n\), denoted as \(\lim_{n\to\infty}S_n\) or \(\sum_{i=1}^{\infty}a_i\), is defined as the following:
\[\sum_{i=1}^{\infty}a_i = L \equiv \forall \epsilon > 0:\, \exists M \in\mathbb{N}\text{\ s.t.\ } n \geq M\implies |S_n - L| < \epsilon.\]

If such $M$ exists, we say the limit exists or the series converges to $L$; otherwise, we say the limit doesn't exist or the series diverge.
\subsubsection{Absolute convergence and conditional convergence}
A series $S_n=\sum_{i=1}^{\infty}a_i$ converges absolutely to $L$ if $\exists\lim_{n\to\infty}\sum_{i=1}^{\infty}\abs{a_1}\land\lim_{n\to\infty}\sum_{i=1}^{\infty}\abs{a_1}=L$. If $S_n$ is convergent but not convergent absolutely, we say $S_n$ converges conditionally.
\sssc{The Uniqueness of Limits}
If a limit of a real series exists, it is unique.
\sssc{Preservation of equal to}
Given real sequences $\langle a_n\rangle$ and $\langle b_n\rangle$ which for all $k$ that is greater than or equal to a specific $j\in \mathbb{N}$:
\[\sum_{i=1}^ka_i=\sum_{i=1}^kb_i.\]
If $\sum_{i=1}^{\infty}a_i=L\in\mathbb{R}\cup\{-\infty,\infty\}$, then
\[\sum_{i=1}^{\infty}b_i=L.\]
\sssc{Preservation of less than or equal to}
Given real sequences $\langle a_n\rangle$ and $\langle b_n\rangle$ which for all $k$ that is greater than or equal to a specific $j\in \mathbb{N}$:
\[\sum_{i=1}^ka_i\leq\sum_{i=1}^kb_i.\]
If both $\sum_{i=1}^{\infty}a_i$ and $\sum_{i=1}^{\infty}b_i$ are in $\mathbb{R}\cup\{-\infty,\infty\}$, then
\[\sum_{i=1}^{\infty}a_i\leq\sum_{i=1}^{\infty}b_i.\]
\sssc{Squeeze theorem or Sandwich theorem}
Given real sequences $\langle a_n\rangle$, $\langle b_n\rangle$, and $\langle c_n\rangle$ which for all $k$ that is greater than or equal to a specific $j\in \mathbb{N}$:
\[\sum_{i=1}^ka_i\leq\sum_{i=1}^kc_i\leq\sum_{i=1}^kb_i.\]
If
\[\sum_{i=1}^{\infty}a_i=\sum_{i=1}^{\infty}b_i=L\in\mathbb{R}\cup\{-\infty,\infty\},\]
then: 
\[\sum_{i=1}^{\infty}c_i=L.\]
\subsection{Limits of Real Nets}
Below, we are discussing limits of nets in the set of real number.
\subsubsection{Definition}
For a real net $\langle x_a\rangle_{a\in A}$ in which $A$ is a directed set, the limit of $\langle x_a\rangle_{a\in A}$, denoted as $\lim_{a\in A} x_a$ or $\lim_a x_a$, is defined as follows:
\[\lim_ax_a = L \equiv \forall \epsilon > 0:\, \exists a_0 \in A\text{\ s.t.\ } a\in A\land a\ge a_0\implies |x_a - L| < \epsilon.\]

If such $a_0$ exists, we say the limit exists or the net converges to $L$; otherwise, we say the limit doesn't exist.
\subsubsection{Infinite Limits}
\[\lim_{a}x_a=\infty \equiv \forall M > 0, \exists a_0 \in A \text{\ s.t.\ } a\in A\land a\geq a_0 \implies x_a > M.\]
\[\lim_{a}x_a=-\infty \equiv \forall M < 0, \exists a_0 \in A \text{\ s.t.\ } a\in A\land a\geq a_0 \implies x_a < M.\]
Note that if a limit equals to $\infty$ or $-\infty$, the limit doesn't exist.
\sssc{The Uniqueness of Limits}
If a limit of a real net exists, it is unique.
\sssc{Preservation of equal to}
Let $A$ be a directed set. Given real nets $\langle x_a\rangle_{a\in A}$ and $\langle y_a\rangle_{a\in A}$ which for all $k\in A$ such that there exists $j\in A$ with $j\leq k$, $x_k=y_k$. If $\lim_{a}x_a$ exists, then:
\[\lim_{a}x_a=\lim_{a}y_a.\]
\sssc{Preservation of less than or equal to}
Let $A$ be a directed set. Given real nets $\langle x_a\rangle_{a\in A}$ and $\langle y_a\rangle_{a\in A}$ which for all $k\in A$ such that there exists $j\in A$ with $j\leq k$, $x_k\leq y_k$. If both $\lim_{a}x_a$ and $\lim_{a}y_a$ exist, then:
\[\lim_{a}x_a\leq\lim_{a}y_a.\]
\sssc{Squeeze theorem or Sandwich theorem}
Let $A$ be a directed set. Given real nets $\langle x_a\rangle_{a\in A}$, $\langle y_a\rangle_{a\in A}$, and $\langle z_a\rangle_{a\in A}$ which for all $k\in A$ such that there exists $j\in A$ with $j\leq k$:
\[x_k\leq z_k\leq y_k.\]
If
\[\lim_{a}x_a=\lim_{a}y_a=L,\]
then: 
\[\lim_{a}z_a=L.\]
\begin{proof}
Start from the limits of $x_a$ and $y_a$, by definition of limit:

For every $\varepsilon\in\mathbb{R}_{>0}$, there exists some $a_1\in A$ such that for every $b\in A$ with $b\geq a_1$, the point $L-\varepsilon\leq x_b\leq L+\varepsilon$.

For every $\varepsilon\in\mathbb{R}_{>0}$, there exists some $a_2\in A$ such that for every $b\in A$ with $b\geq a_2$, the point $L-\varepsilon\leq y_b\leq L+\varepsilon$.

Choose $a\in A$ such that $a_1\leq a$ and $a_2\leq a$.

so,
\[L-\varepsilon\leq x_{a}\leq z_{a}\leq y_{a}\leq L+\varepsilon.\]

Since $\varepsilon > 0$ was arbitrary, by definition of limit:
\[\lim_{a}z_a=L.\]
\end{proof}
\subsection{Limits of Functions with Real Domains}
\subsubsection{Definition at Finity}
Let \(I\) be an interval containing the point \(a\). Let \( f(x) \) be a function defined on \(I\), except possibly at \(a\) itself. The limit of \( f(x) \) as \( x \) approaches \( a \) is defined as follows:
\[\lim_{x \to a} f(x) = L \equiv \forall \epsilon > 0:\, \exists \delta > 0 \text{\ s.t.\ } 0 < |x - a| < \delta \implies |f(x) - L| < \epsilon.\]
In other words, as \(x\) becomes arbitrarily close to \(a\), \(f(x)\) gets arbitrarily close to \(L\).

If such $\delta$'s exist, we say the limit exists; otherwise, we say the limit doesn't exist.
\subsubsection{Definition at Infinity}
Let \(I\) be a left-bounded, right-unbounded interval with the point \(a\) being its endpoint on the left. Let \( f(x) \) be a function defined on \(I\). The limit of \( f(x) \) as \( x \) approaches \( \infty \) is defined as follows:
\[\lim_{x \to \infty} f(x) = L \equiv \forall \epsilon > 0: \, \exists M > a \text{\ s.t.\ } x > M \implies |f(x) - L| < \epsilon.\]
In other words, as \(x\) becomes arbitrarily large, \(f(x)\) gets arbitrarily close to \(L\). We say $f(x)$ converge to $L$ as $x\to\infty$ if $\lim_{x \to \infty} f(x) = L$.

Let \(I\) be a right-bounded, left-unbounded interval with the point \(a\) being its endpoint on the right. Let \( f(x) \) be a function defined on \(I\). The limit of \( f(x) \) as \( x \) approaches \( -\infty \) is defined as follows:
\[\lim_{x \to -\infty} f(x) = L \equiv \forall \epsilon > 0: \, \exists M < a \text{\ s.t.\ } x < M \implies |f(x) - L| < \epsilon.\]
In other words, as \(x\) becomes arbitrarily small, \(f(x)\) gets arbitrarily close to \(L\). We say $f(x)$ converge to $L$ as $x\to-\infty$ if $\lim_{x \to -\infty} f(x) = L$.
\subsubsection{Horizontal asymptote (水平漸近線)}
We say $y=L$ is a horizontal asymptote of $y=f(x)$ iff $\lim_{x \to \infty} f(x)=L$ or $\lim_{x \to -\infty} f(x)=L$.
\subsubsection{Slant asymptote (斜漸近線)}
We say $y=mx+b$ with $m\neq 0$ is a slant asymptote of $y=f(x)$ iff $\lim_{x \to \infty} \qty(f(x)-(mx+b))=0$ or $\lim_{x \to -\infty} \qty(f(x)-(mx+b))=0$.
\subsubsection{One-side Limits}
\tb{Right-hand Limit (右極限)}: Let \(I\) be a left-open interval with the point \(a\) being its endpoint on the left. Let \( f(x) \) be a function defined on \(I\). The right-hand limit of \( f(x) \) as \( x \) approaches \( a \) is defined as follows:
\[\lim_{x \to a^+} f(x) = L \equiv \forall \epsilon > 0 :\,\exists \delta > 0 \text{\ s.t.\ } 0 < x - a < \delta \implies |f(x) - L| < \epsilon.\]
In other words, as \(x\) becomes arbitrarily close to \(a\) and is greater than \(a\), \(f(x)\) gets arbitrarily close to \(L\). For a function $f(x)$, $\lim_{x\to a^+}f(x)$ can also be denoted as $f(a^+)$.

\tb{Left-hand Limit (左極限)}: Let \(I\) be a right-open interval with the point \(a\) being its endpoint on the right. Let \( f(x) \) be a function defined on \(I\). The left-hand limit of \( f(x) \) as \( x \) approaches \( a \) is defined as follows:
\[\lim_{x \to a^-} f(x) = L \equiv \forall \epsilon > 0 :\,\exists \delta > 0 \text{\ s.t.\ } 0 < a - x < \delta \implies |f(x) - L| < \epsilon.\]
In other words, as \(x\) becomes arbitrarily close to \(a\) and is less than \(a\), \(f(x)\) gets arbitrarily close to \(L\). For a function $f(x)$, $\lim_{x\to a^-}f(x)$ can also be denoted as $f(a^-)$.
\subsubsection{Infinite Limits}
\[\lim_{x\to a}f(x)=\infty \equiv \forall N > 0:\, \exists \delta > 0 \text{\ s.t.\ } 0 < |x - a| < \delta \implies f(x) > N.\]
\[\lim_{x\to a^+}f(x)=\infty \equiv \forall N > 0:\, \exists \delta > 0 \text{\ s.t.\ } 0 < x - a < \delta \implies f(x) > N.\]
\[\lim_{x\to a^-}f(x)=\infty \equiv \forall N > 0:\, \exists \delta > 0 \text{\ s.t.\ } 0 < a - x < \delta \implies f(x) > N.\]
\[\lim_{x\to a}f(x)=-\infty \equiv \forall N < 0:\, \exists \delta > 0 \text{\ s.t.\ } 0 < |x - a| < \delta \implies f(x) < N.\]
\[\lim_{x\to a^+}f(x)=-\infty \equiv \forall N < 0:\, \exists \delta > 0 \text{\ s.t.\ } 0 < x - a < \delta \implies f(x) < N.\]
\[\lim_{x\to a^-}f(x)=-\infty \equiv \forall N < 0:\, \exists \delta > 0 \text{\ s.t.\ } 0 < a - x < \delta \implies f(x) < N.\]
\[\lim_{x\to\infty}f(x)=\infty \equiv \forall N > 0:\, \exists M > 0 \text{\ s.t.\ } x > M \implies f(x) > N.\]
\[\lim_{x\to\infty}f(x)=-\infty \equiv \forall N > 0:\, \exists M > 0 \text{\ s.t.\ } x > M \implies f(x) < N.\]
\[\lim_{x\to-\infty}f(x)=\infty \equiv \forall N > 0:\, \exists M < 0 \text{\ s.t.\ } x < M \implies f(x) > N.\]
\[\lim_{x\to-\infty}f(x)=-\infty \equiv \forall N > 0:\, \exists M < 0 \text{\ s.t.\ } x < M \implies f(x) < N.\]
Note that if a limit equals to $\infty$ or $-\infty$, the limit doesn't exist.
\sssc{Vertical asymptote (鉛直漸近線)}
We say $x=a$ is a vertical asymptote of $y=f(x)$ iff $\abs{\lim_{x \to a^+} f(x)}=\infty$ or $\abs{\lim_{x \to a^-} f(x)}=\infty$.
\sssc{The Uniqueness of Limits}
If a limit of a real function exists, it is unique.
\sssc{Preservation of equal to}
Let $I$ be an open interval and $a\in I$, and $f(x)$ and $g(x)$ be functions defined on $I\setminus\{a\}$ which for all $x\in I\land x\neq a$, $f(x)=g(x)$. If $\lim_{x\to a}f(x)=L\in\mathbb{R}\cup\{-\infty,\infty\}$, then
\[\lim_{x\to a}g(x)=L.\]

Let $f(x)$ and $g(x)$ be functions defined on $(a,b)$ with $a<b$ which for all $x\in I$, $f(x)=g(x)$. If $\lim_{x\to a^+}f(x)=L\in\mathbb{R}\cup\{-\infty,\infty\}$, then
\[\lim_{x\to a^+}g(x)=L.\]

Let $f(x)$ and $g(x)$ be functions defined on $(b,a)$ with $a>b$ which for all $x\in I$, $f(x)=g(x)$. If $\lim_{x\to a^-}f(x)=L\in\mathbb{R}\cup\{-\infty,\infty\}$, then
\[\lim_{x\to a^-}g(x)=L.\]

Let $f(x)$ and $g(x)$ be functions defined on $(a,\infty)$ which for all $x\in I$, $f(x)=g(x)$. If $\lim_{x\to\infty}f(x)=L\in\mathbb{R}\cup\{-\infty,\infty\}$, then
\[\lim_{x\to\infty}g(x)=L.\]

Let $f(x)$ and $g(x)$ be functions defined on $(-\infty,a)$ which for all $x\in I$, $f(x)=g(x)$. If $\lim_{x\to-\infty}f(x)=L\in\mathbb{R}\cup\{-\infty,\infty\}$, then
\[\lim_{x\to-\infty}g(x)=L.\]
\sssc{Preservation of less than or equal to}
Let $I$ be an open interval and $a\in I$, and $f(x)$ and $g(x)$ be functions defined on $I\setminus\{a\}$ which for all $x\in I\land x\neq a$, $f(x)\leq g(x)$. If both $\lim_{x\to a}f(x)$ and $\lim_{x\to a}g(x)$ are in $\mathbb{R}\setminus\{-\infty,\infty\}$, then
\[\lim_{x\to a}f(x)\leq\lim_{x\to a}g(x).\]
If $\lim_{x\to a}f(x)=\infty$, then $\lim_{x\to a}g(x)=\infty$; if $\lim_{x\to a}g(x)=-\infty$, then $\lim_{x\to a}f(x)=-\infty$.

Let $f(x)$ and $g(x)$ be functions defined on $(a,b)$ with $a<b$ which for all $x\in I$, $f(x)\leq g(x)$. If both $\lim_{x\to a^+}f(x)$ and $\lim_{x\to a^+}g(x)$ are in $\mathbb{R}\cup\{-\infty,\infty\}$, then
\[\lim_{x\to a^+}f(x)\leq\lim_{x\to a^+}g(x).\]
If $\lim_{x\to a^+}f(x)=\infty$, then $\lim_{x\to a^+}g(x)=\infty$; if $\lim_{x\to a^+}g(x)=-\infty$, then $\lim_{x\to a^+}f(x)=-\infty$.

Let $f(x)$ and $g(x)$ be functions defined on $(b,a)$ with $a>b$ which for all $x\in I$, $f(x)\leq g(x)$. If both $\lim_{x\to a^-}f(x)$ and $\lim_{x\to a^-}g(x)$ are in $\mathbb{R}\cup\{-\infty,\infty\}$, then
\[\lim_{x\to a^-}f(x)\leq\lim_{x\to a^-}g(x).\]
If $\lim_{x\to a^-}f(x)=\infty$, then $\lim_{x\to a^-}g(x)=\infty$; if $\lim_{x\to a^-}g(x)=-\infty$, then $\lim_{x\to a^-}f(x)=-\infty$.

Let $f(x)$ and $g(x)$ be functions defined on $(a,\infty)$ which for all $x\in I$, $f(x)\leq g(x)$. If both $\lim_{x\to\infty}f(x)$ and $\lim_{x\to\infty}g(x)$ are in $\mathbb{R}\cup\{-\infty,\infty\}$, then
\[\lim_{x\to\infty}f(x)\leq\lim_{x\to\infty}g(x).\]
If $\lim_{x\to\infty}f(x)=\infty$, then $\lim_{x\to\infty}g(x)=\infty$; if $\lim_{x\to\infty}g(x)=-\infty$, then $\lim_{x\to\infty}f(x)=-\infty$.

Let $f(x)$ and $g(x)$ be functions defined on $(-\infty,a)$ which for all $x\in I$, $f(x)\leq g(x)$. If both $\lim_{x\to-\infty}f(x)$ and $\lim_{x\to-\infty}g(x)$ are in $\mathbb{R}\cup\{-\infty,\infty\}$, then
\[\lim_{x\to-\infty}f(x)\leq\lim_{x\to-\infty}g(x).\]
If $\lim_{x\to-\infty}f(x)=\infty$, then $\lim_{x\to-\infty}g(x)=\infty$; if $\lim_{x\to-\infty}g(x)=-\infty$, then $\lim_{x\to-\infty}f(x)=-\infty$.
\subsubsection{Squeeze theorem or Sandwich theorem}
Let $I$ be an open interval and $a\in I$, and $f(x)$, $g(x)$, and $h(x)$ be functions defined on $I\setminus\{a\}$ which for all $x\in I\land x\neq a$, $f(x)\leq h(x)\leq g(x)$. If
\[\lim_{x\to a}f(x)=\lim_{x\to a}g(x)=L\in\mathbb{R}\setminus\{-\infty,\infty\},\]
then
\[\lim_{x\to a}h(x)=L.\]

Let $f(x)$, $g(x)$, and $h(x)$ be functions defined on $(a,b)$ with $a<b$ which for all $x\in I$, $f(x)\leq h(x)\leq g(x)$. If
\[\lim_{x\to a^+}f(x)=\lim_{x\to a^+}g(x)=L\in\mathbb{R}\setminus\{-\infty,\infty\},\]
then
\[\lim_{x\to a^+}h(x)=L.\]

Let $f(x)$, $g(x)$, and $h(x)$ be functions defined on $(b,a)$ with $a>b$ which for all $x\in I$, $f(x)\leq h(x)\leq g(x)$. If
\[\lim_{x\to a^-}f(x)=\lim_{x\to a^-}g(x)=L\in\mathbb{R}\setminus\{-\infty,\infty\},\]
then
\[\lim_{x\to a^-}h(x)=L.\]

Let $f(x)$, $g(x)$, and $h(x)$ be functions defined on $(a,\infty)$ which for all $x\in I$, $f(x)\leq h(x)\leq g(x)$. If
\[\lim_{x\to\infty}f(x)=\lim_{x\to\infty}g(x)=L\in\mathbb{R}\setminus\{-\infty,\infty\},\]
then
\[\lim_{x\to\infty}h(x)=L.\]

Let $f(x)$, $g(x)$, and $h(x)$ be functions defined on $(-\infty,a)$ which for all $x\in I$, $f(x)\leq h(x)\leq g(x)$. If
\[\lim_{x\to-\infty}f(x)=\lim_{x\to-\infty}g(x)=L\in\mathbb{R}\setminus\{-\infty,\infty\},\]
then
\[\lim_{x\to-\infty}h(x)=L.\]
\begin{proof}
Take the first statement for example. The other statements can be proven similarly.

Start from the limits of $f$ and $g$, by definition of limit:
\[\forall\varepsilon>0, \exists\delta_1>0\text{\ s.t.\ }0<|x-a|<\delta_1\implies|f(x)-L|<\varepsilon,\]
and
\[\forall\varepsilon>0, \exists\delta_2>0\text{\ s.t.\ }0<|x-a|<\delta_2\implies|g(x)-L|<\varepsilon,\]

Choose:
\[\delta\coloneq\min(\delta_1,\delta_2),\]

so,
\[L-\varepsilon<f(x)\leq h(x)\leq g(x)<L+\varepsilon.\]

Since $\varepsilon > 0$ was arbitrary, by definition of limit:
\[\lim_{x\to a}h(x)=L.\]
\end{proof}
\sssc{Limits involving quotient functions}
Let \( a \) and \( b \) be real numbers, set
\[A=\left\{f\colon U\subseteq\mathbb{R}\to\mathbb{R} \middle | f(x) = x \lor \ln(f(x)) \in A \lor e^{f\left(x\right)}  \in A \right\},\]
and function $f\in A$. Then:
\[\lim_{x \to \infty} \frac{\left(f\left(x\right)\right)^a}{\left(f\left(x\right)\right)^b} = \infty, \quad a > b \]
\[ \lim_{x \to \infty} \frac{af\left(x\right)}{bf\left(x\right)} = \frac{a}{b}, \quad b \neq 0 \]
\[ \lim_{x \to \infty}\frac{n^{af\left(x\right)}}{bf\left(x\right)} = \infty, \quad a,b > 0 \land  n > 1 \]
\[ \lim_{x \to \infty}\frac{n^{af\left(x\right)}}{bf\left(x\right)} = 0, \quad a,b > 0 \land  0\leq n<1 \]
\[ \lim_{x \to \infty}\frac{af\left(x\right)}{b\log_n f\left(x\right)} = \infty, \quad a,b > 0 \land  n > 1 \]
\ssc{Limit of nets of fuctions}
\sssc{Pointwise convergence (逐點收斂)}
Let $V$ be a set and $Y$ be a topological space. A net of functions $\langle f_n\rangle_{n\in A}$ all having the same domain $X\subseteq V$ and codomain $Y$ is said to converge pointwise to a given function $f\colon X\to Y$, denoted as 
\[\lim_nf_n=f\text{\ pointwise},\]
if and only if
\[\forall x\in X\colon\lim_nf_n(x)=f(x).\]
The function $f$ is said to be the pointwise limit function of $\langle f_n\rangle$.
\sssc{Uniform convergence (一致收斂)}
Let $V$ be a set and $(Y,d)$ be a metric topological space. A net of functions $\langle f_n\rangle_{n\in A}$ all having the same domain $X\subseteq V$ and codomain $Y$ is said to converge uniformly to a given function $f\colon X\to Y$, denoted as 
\[\lim_nf_n=f\text{\ uniformly},\]
if and only if
\[\forall\varepsilon\in\mathbb{R}_{>0}\colon\exists N\in A\text{\ s.t.\ }\left(n\in A\land N\leq n\implies\sup_{x\in X}d(f_n(x),f(x))<\varepsilon\right).\]
The function $f$ is said to be the uniform limit function of $\langle f_n\rangle$.
\ssc{Limit Laws}
If the limit (including one-sided ones for functions with real domains) of sequences, series, nets, or functions with real domains, hereinafter referred to as a function, in one side exists in $\mathbb{R}\cup\{-\infty,\infty\}$, then the limit in the other side exists in $\mathbb{R}\cup\{-\infty,\infty\}$ and follows the following law.
\sssc{Linearity or sum law, difference law, and constant multiple law}
The limit of a sum of constant multiples of functions is the sum of the constant multiples of the limits of the functions.
\sssc{Product law}
The limit of a product of functions is the product of the limits of the functions.
\sssc{Quotient law}
The limit of a quotient of functions is the quotient of the limits of the functions, provided that the limit of the denominator is not 0, in which the reciprocals of $\infty$ and $-\infty$ are $0$.
\sssc{Power law}
The limit of the $n$th power of a function, in which $n$ is a positive integer, is the $n$th power of the limit of the function.
\sssc{Root law}
The limit of the $n$th root of a function, in which $n$ is a positive integer, is the $n$th root of the limit of the function.
\sssc{General limit law}
For any function $f\colon D\subseteq\bbR\to\bbR$ that is continuous at a limit of a function $g$, $f$ of the limit of $g$ is the limit of $f$ of $g$, where $f$ is called continuous at $\infty$ if $\lim_{x\to\infty}f(x)\in\bbR\cup\{-\infty,\infty\}$ and called continuous at $-\infty$ if $\lim_{x\to-\infty}f(x)\in\bbR\cup\{-\infty,\infty\}$.



\section{Continuity (連續性)}
\ssc{Continuity}
\sssc{Definition of continuity of real functions}
\begin{itemize}
\item For a point $a$ in the domain of a function $f$, if and only if $\exists\lim_{x\to a}f(x)$ and $\lim_{x\to a}f(x)=f(a)$, we say $f(x)$ is continuous (連續的) at $a$.
\item For a point $a$ in the domain of a function $f$, if and only if $\exists\lim_{x\to a^+}f(x)$ and $\lim_{x\to a^+}f(x)=f(a)$, we say $f(x)$ is continuous from the right at $a$; if and only if $\exists \lim_{x\to a^-} f(x)$ and $\lim_{x\to a^-} f(x)=f(a)$, we say $f(x)$ is continuous from the left at $a$.
\item For an open interval $I$ that is a subset of the domain of a function $f$, if and only if $f(x)$ is continuous at all points in $I$, we say $f(x)$ is continuous on $I$.
\item For an right-open interval $[a,b)$ that is a subset of the domain of a function $f$, if and only if $f(x)$ is continuous on $(a,b)$ and continuous from the right at $a$, we say $f(x)$ is continuous on $[a,b)$.
\item For an left-open interval $(a,b]$ that is a subset of the domain of a function $f$, if and only if $f(x)$ is continuous on $(a,b)$ and continuous from the left at $b$, we say $f(x)$ is continuous on $(a,b]$.
\item For an closed interval $[a,b]$ that is a subset of the domain of a function $f$, if and only if $f(x)$ is continuous on both $[a,b)$ and $(a,b]$, we say $f(x)$ is continuous on $[a,b]$.
\item if and only if $f(x)$ is continuous at all points in its domain, we say $f(x)$ is continuous.
\eit
\sssc{Definition of continuity of functions between topological spaces}
\begin{itemize}
\item A function $f\colon I\subseteq X\to Y$ where $X$ and $Y$ are topological spaces is continuous at a point $x\in I$ if and only if for any neighborhood $V$ of $f(x)$ in $Y$, there is a neighborhood $U$ of $x$ such that $f(U)\subseteq V$.
\item A function $f\colon I\subseteq X\to Y$ where $X$ and $Y$ are topological spaces is continuous if and only if for any open subset $V$ of $Y$, the preimage of $f$ on $V$ is an open subset of $X$.
\item A function $f\colon I\subseteq X\to Y$ where $X$ and $Y$ are topological spaces is continuous on a subset $J$ of $I$ if and only if for any open subset $V$ of $Y$, the joint set of the preimage of $f$ on $V$ and $J$ is open in the subspace topology of $X$ in $J$.
\eit
\sssc{Discontinuity (不連續（點）)}
A point is a discontinuity of a function $f$ if it is in the domain of $f$ but $f$ is not continuous on it.
\sssc{Type of discontinuity of real functions}
For a function $f$ with domain $U\subseteq\mathbb{R}$ and a point $a\in U$ that $f$ is discontinuous at, the discontinuity $a$ of $f$ can be classified as below:
\bit
\item \tb{Removable (可去) discontinuity}: If $\exists\lim_{x\to a}f(x)\land\lim_{x\to a}f(x)\neq f(a)$, we call $f(a)$ a removable discontinuity. A discontinuity that is not a removable discontinuity is called a non-removable discontinuity.
\item \tb{Jump (跳躍) discontinuity}: If $\exists\lim_{x\to a^-}f(x)\land\exists\lim_{x\to a^+}f(x)\land\lim_{x\to a^-}f(x)\neq\lim_{x\to a^+}f(x)$, we call $f(a)$ a jump discontinuity.
\item\tb{Infinite (無窮) discontinuity}: If at least one of $\lim_{x\to a^-}f(x)$ and $\lim_{x\to a^+}f(x)$ does not exist, and that those in $\lim_{x\to a^-}f(x)$ and $\lim_{x\to a^+}f(x)$ that do not exist are either $\infty$ or $-\infty$, we call $f(a)$ an infinite discontinuity.
\item\tb{Type I discontinuity}: A discontinuity of $f$ that is either a removable discontinuity or a jump discontinuity is called a type I discontinuity of $f$.
\item\tb{Essential (本質) discontinuity or type II discontinuity}: A discontinuity of $f$ that is not a type I discontinuity is called an essential discontinuity or a type II discontinuity of $f$.
\eit
\sssc{Removable discontinuity of functions between topological spaces}
Let $f\colon I\subseteq X\to Y$ be a function where $X$ and $Y$ are topological spaces. We say that $f(a)$ is a removable discontinuity of $f$ iff $f$ is discontinuous at $a$ and there exists $L\in Y$ such that the function
\[g(x)=\begin{cases}L,\quad&x=a\\f(x),\quad&x\in I\setminus\{a\}\end{cases}\]
is continuous at $a$. A discontinuity that is not a removable discontinuity is called a non-removable discontinuity.
\sssc{Singularity or singular point (奇點)}
A point is a singularity or a singular point of a function $f$ if it is in the closure of its domain but not in its domain, or it is a discontinuity of $f$.
\sssc{Arithmetic Laws}
If $f$ and $g$ are continuous at $a$ and $c$ is a constant, then the following functions are also continuous at $a$:
\[f+g;\quad f-g;\quad cf;\quad fg;\]
\[\frac{f}{g}\text{\ if\ }g(a)\neq 0.\]
\sssc{Composte Laws}
If $g$ is continuous at $a$ and $f$ is continuous at $g(a)$, then the composite function $f\circ g$ is continuous at $a$.
\sssc{Examples of continuous real functions}
The following types of functions are continuous at every number in their domains: algebraic functions, trigonometric functions, inverse trigonometric functions, exponential functions, logarithmic functions.
\ssc{Intermediate Value Theorem (IVT) (中間值定理)}
\sssc{Intermediate value theorem of real functions}
Let $f\colon I\subseteq\mathbb{R}\to\mathbb{R}$ be a function, then for any interval $[a,b]\subseteq I$ such that $f$ is continuous on $[a,b]$ and $f(a)\neq f(b)$, $N$ is strictly between $f(a)$ and $f(b)$ implies there exists $c\in (a,b)$ such that $f(c)=N$.
\begin{proof}
Without loss of generality, assume $f(a)<N<f(b)$.

Consider the set
\[S=\{x\in [a,b]\mid f(x)\leq N\}.\]
Since $f(a)<N$, $a\in S$; since $f(b)>N$, $b\notin S$. So $S$ is nonempty and bounded-above by $b$.

Let $c=\sup S$. We claim that $f(c)=N$. Argue by contradiction.

Assume $f(c)<N$. By continuity, there exists $\delta>0$ such that for all $x\in (c,c+\delta)\cap [a,b]$, $|f(x)-f(c)|<N-f(c)$. So $f(x)<N$ for some $x>c$. Contradicting that $c$ is the least upper bound.

Assume $f(c)>N$. By continuity, there exists $\delta>0$ such that for all $x\in (c-\delta,c)\cap [a,b]$, $|f(x)-f(c)|<f(c)-N$. So $f(x)>N$ for some $x<c$. Contradicting that $c$ is the least upper bound.

Therefore, $f(c)=N$.
\end{proof}
\sssc{Intermediate value theorem of functions between topological spaces}
Let $X$ and $Y$ be topological spaces and function $f\colon I\subseteq X\to Y$ be continuous on a subset $J$ of $I$, then for any connected subset $K$ of $J$, $f(K)$ is a connected subset of $Y$.

Let $X$ and $Y$ be topological spaces and function $f\colon I\subseteq X\to Y$ be continuous on a subset $J$ of $I$, then for any path-connected subset $K$ of $J$, $f(K)$ is a path-connected subset of $Y$.
\ssc{Piecewise continuity}
\sssc{Piecewise continuity of real functions}
Let $f\colon D\subseteq\mathbb{R}\to\mathbb{R}$ be a function. If there exists a family $\{[a_i,b_i]\mid i\in I\}$ of closed intervals such that:
\begin{itemize}
\item for every closed interval $V$, the set $\{i\in I\mid V\cap [a_i,b_i]\neq\varnothing\}$ is finite (some sources require instead that $n(I)$ is finite, which is stronger),
\item $D=\bigcup_{i\in I}[a_i,b_i]$,
\item function $f\big\vert_{a_i}^{b_i}\colon(a_i,b_i)\to Y$ is continuous for every $i\in I$, and
\item there exist finite $\lim_{x\to a_i^{\phantom{i}+}}f(x)$ and $\lim_{x\to b_i^{\phantom{i}-}}f(x)$ for every $i\in I$.
\end{itemize},
we say $f$ is piecewise continuous (on $\bigcup_{i\in I}[a_i,b_i]$).
\sssc{Piecewise continuity of functions between topological spaces}
Let $X$ and $Y$ be topological spaces, and $f\colon D\subseteq X\to Y$ be a function. If there exists a locally finite (some sources require instead finite, which is stronger) family $\{U_i\mid i\in I\}$ of closed subsets of $X$ such that:
\begin{itemize}
\item $D=\bigcup_{i\in I}U_i$, and
\item there exists a function $g_i\colon U_i\to Y$ that is continuous on $U_i$ such that $\forall x\in\operatorname{int}\left(U_i\right)f(x)=g_i(x)$ for every $i\in I$.
\end{itemize},
we say $f$ is piecewise continuous (on $\bigcup_{i\in I}U_i$).
\ssc{Uniform continuity (一致連續)}
\sssc{Uniform continuity of functions between metric spaces}
Let $(X,d_1)$ and $(Y,d_2)$ be metric spaces, and $f\colon D\subseteq X\to Y$ be a function. $f$ is called to be uniformly continuous if for every real number $\varepsilon >0$ there exists a real number $\delta >0$ such that $x,y\in D$ with $d_{1}(x,y)<\delta$ implies $d_{2}(f(x),f(y))<\varepsilon$.

Let $(X,d_1)$ and $(Y,d_2)$ be metric spaces, and $f\colon O\subseteq X\to Y$ be a function. $f$ is called to be uniformly continuous on $D\subseteq O$ if for every real number $\varepsilon >0$ there exists a real number $\delta >0$ such that $x,y\in D$ with $d_{1}(x,y)<\delta$ implies $d_{2}(f(x),f(y))<\varepsilon$.

Uniform continuity implies continuity.
\sssc{Uniform continuity of functions between topological vector spaces}
Let $X$ and $Y$ be topological vector spaces, and $f\colon D\subseteq X\to Y$ be a function. $f$ is called to be uniformly continuous if for every neighborhood $V$ of the zero vector in $Y$, there exists a neighborhood $U$ of the zero vector in $X$ such that $x,y\in D$ with $x-y\in U$ implies $f(x)-f(y)\in V$.

Let $X$ and $Y$ be topological vector spaces, and $f\colon O\subseteq X\to Y$ be a function. $f$ is called to be uniformly continuous on $D\subseteq O$ if for every neighborhood $V$ of the zero vector in $Y$, there exists a neighborhood $U$ of the zero vector in $X$ such that $x,y\in D$ with $x-y\in U$ implies $f(x)-f(y)\in V$.

Uniform continuity implies continuity.
\ssc{Absolute continuity (絕對連續) of functions from an interval to a metric space}
Let $(X, d)$ be a metric space and $I\subseteq\mathbb{R}$ be an interval. A function $f\colon I \to X$ is absolutely continuous on $I$ if for every positive number $\varepsilon$, there exists a positive number $\delta$ such that for any finite sequence of disjoint subintervals $[x_k, y_k]$ of $I$,
\[\sum _{k}\left|y_{k}-x_{k}\right|<\delta\]
implies
\[\sum _{k}d\left(f(y_{k}),f(x_{k})\right)<\varepsilon.\]
The collection of all absolutely continuous functions from $I$ into $X$ is denoted $AC(I; X)$.

Let $(X, d)$ be a metric space and $I\subseteq\mathbb{R}$ be an interval. A function $f\colon I \to X$ is absolutely continuous on $J\subseteq I$ if for every positive number $\varepsilon$, there exists a positive number $\delta$ such that for any finite sequence of disjoint subintervals $[x_k, y_k]$ of $J$,
\[\sum _{k}\left|y_{k}-x_{k}\right|<\delta\]
implies
\[\sum _{k}d\left(f(y_{k}),f(x_{k})\right)<\varepsilon.\]

Absolute continuity implies uniform continuity.



\section{Derivative (導數)}
\ssc{Notation}
\sssc{Leibniz's notation (萊布尼茲符號) for derivatives}
Suppose a dependent variable $y$ represents a function $f$ of an independent variable $x$, that is,
\[y=f(x).\]
Then, the derivative function of $y$ or $f$ (with respect to (w.r.t.) $x$) can be written as
\[\frac{\mathrm{d}y}{\mathrm{d}x},\quad\frac{\mathrm{d}}{\mathrm{d}x}y,\quad\frac{\mathrm{d}\qty(f(x))}{\mathrm{d}x},\quad\text{or\ }\frac{\mathrm{d}}{\mathrm{d}x}\qty(f(x)),\]
in which $\frac{\mathrm{d}}{\mathrm{d}x}$ is called a differential operator (微分運算子) or a derivative operator (導數運算子);

the $n$th derivative function of $y$ or $f$ (with respect to $x$) can be written as
\[\frac{\mathrm{d}^ny}{\mathrm{d}x^n},\quad\frac{\mathrm{d}^n}{\mathrm{d}x^n}y,\quad\frac{\mathrm{d}^n\qty(f(x))}{\mathrm{d}x^n},\quad\tx{or\ }\frac{\mathrm{d}^n}{\mathrm{d}x^n}\qty(f(x)),\]
in which $\frac{\mathrm{d}^n}{\mathrm{d}x^n}$ is called a differential operator or a derivative operator.
\sssc{Leibniz's notation for partial derivatives}
Suppose a dependent variable $y$ represents a function $f$ of an independent variable vector $\mb{x}=(x_1,x_2,\dots x_n)$, that is,
\[y=f(\mb{x}).\]
Then, the partial derivative function of $y$ or $f$ with respect to $x_i$ can be written as
\[\frac{\partial y}{\partial x_i},\quad\frac{\partial }{\partial x_i}y,\quad\frac{\partial \qty(f(x))}{\partial x_i},\quad\text{or\ }\frac{\partial }{\partial x_i}\qty(f(x)),\]
in which $\frac{\partial}{\partial x_i}$ is called a partial differential operator (偏微分運算子) or a partial derivative operator (偏導數運算子);

the $n$th partial derivative function of $y$ or $f$ with respect to $x_i$ can be written as
\[\frac{\partial^ny}{\partial x_i^{\phantom{i}n}},\quad\frac{\partial^n}{\partial x_i^{\phantom{i}n}}y,\quad\frac{\partial^n\qty(f(x))}{\partial x_i^{\phantom{i}n}}\text{or\ }\frac{\partial^n}{\partial x_i^{\phantom{i}n}}\qty(f(x)),\]
in which $\frac{\partial^n}{\partial x_i^{\phantom{i}n}}$ is called a partial differential operator or a partial derivative operator;

the $n$th mixed partial derivative function of $y$ or $f$ $m_{i_1}$ times with respect to $x_{i_1}$, $m_{i_1}$ times with respect to $x_{i_2}$, $\dots $, $m_{i_k}$ times with respect to $x_{i_k}$, in which $\sum_{j=1}^km_{i_j}=n$, can be written as
\[\frac{\partial^ny}{\partial^{m_{i_1}}x_{i_1}^{\phantom{i_1}m_{i_1}}\partial^{m_{i_2}}x_{i_2}^{\phantom{i_2}m_{i_2}}\dots \partial^{m_{i_k}}x_{i_k}^{\phantom{i_k}m_{i_k}}},\]
\[\frac{\partial^n}{\partial^{m_{i_1}}x_{i_1}^{\phantom{i_1}m_{i_1}}\partial^{m_{i_2}}x_{i_2}^{\phantom{i_2}m_{i_2}}\dots \partial^{m_{i_k}}x_{i_k}^{\phantom{i_k}m_{i_k}}}y,\]
\[\frac{\partial^nf\qty(\mb{x})}{\partial^{m_{i_1}}x_{i_1}^{\phantom{i_1}m_{i_1}}\partial^{m_{i_2}}x_{i_2}^{\phantom{i_2}m_{i_2}}\dots \partial^{m_{i_k}}x_{i_k}^{\phantom{i_k}m_{i_k}}},\quad \tx{or}\]
\[\frac{\partial^n}{\partial^{m_{i_1}}x_{i_1}^{\phantom{i_1}m_{i_1}}\partial^{m_{i_2}}x_{i_2}^{\phantom{i_2}m_{i_2}}\dots \partial^{m_{i_k}}x_{i_k}^{\phantom{i_k}m_{i_k}}}f(\mb{x}),\]
in which $\frac{\partial^n}{\partial^{m_{i_1}}x_{i_1}^{\phantom{i_1}m_{i_1}}\partial^{m_{i_2}}x_{i_2}^{\phantom{i_2}m_{i_2}}\dots \partial^{m_{i_k}}x_{i_k}^{\phantom{i_k}m_{i_k}}}$ is called a partial differential operator or a partial derivative operator.
\sssc{Lagrange's notation (拉格朗日符號) or Prime notation for derivatives}
Suppose a dependent variable $y$ represents a function $f$ of an independent variable $x$, that is,
\[y=f(x).\]
Then, the derivative function of $y$ or $f$ (with respect to $x$) can be written as
\[y',\quad\tx{or\ }f'(x);\]
the $n$th derivative function of $y$ or $f$ (with respect to $x$) can be written as
\[y^{(n)},\quad\tx{or\ }f^{(n)}(x),\]
in which $^{(n)}$ can be replaced with $n$ primes ($'$) for usually $n<4$ (For example, $y''$ is equivalent to $y^{(2)}$, and $f''$ is equivalent to $f^{(2)}$.).
\sssc{Newton's notation (牛頓符號), dot notation, flyspeck notation, or fluxions for derivatives}
Suppose a dependent variable $y$ represents a function $f$ of an independent variable $t$, that is,
\[y=f(t),\]
where $t$ usually represents time.

Then, the derivative function of $y$ or $f$ (with respect to $t$) of the function $f$ can be written as
\[\dot{y},\]
the second derivative function of $y$ or $f$ (with respect to $t$) of the function $f$ can be written as
\[\ddot{y},\]
and so on.
\sssc{Subscript notation for partial derivatives}
Suppose a dependent variable $y$ represents a function $f$ of an independent variable vector $\mb{x}=(x_1,x_2,\dots x_n)$, that is,
\[y=f(\mb{x}).\]
Then, the partial derivative function of $y$ or $f$ with respect to $x_i$ can be written as
\[y_{x_i},\quad y'_{x_i},\quad f_{x_i},\quad \tx{or\ }f'_{x_i};\]
the $n$th partial derivative of the function $f$ with respect to $x_i$ can be written as
\[y_{x_ix_i\dots  x_i},\quad y^{(n)}_{\pht{(n)}x_ix_i\dots  x_i},\quad f_{x_ix_i\dots  x_i},\tx{or\ }\quad f^{(n)}_{\pht{(n)}x_ix_i\dots  x_i},\]
in which the subscript are $n$ $x_i$s and $^{(n)}$ can be replaced with $n$ primes ($'$) for usually $n<4$ (For example, $y''$ is equivalent to $y^{(2)}$, and $f''$ is equivalent to $f^{(2)}$.);

the $n$th mixed partial derivative of the function $f$ $m_{i_1}$ times with respect to $x_{i_1}$, $m_{i_1}$ times with respect to $x_{i_2}$, $\dots $, $m_{i_k}$ times with respect to $x_{i_k}$, in which $\sum_{j=1}^km_{i_j}=n$, can be written as
\[y_{x_{i_1}x_{i_1}\dots  x_{i_2}x_{i_2}x_{i_2}\dots  x_{i_2}\dots  x_{i_k}x_{i_k}\dots  x_{i_k}},\quad y^{(n)}_{\pht{(n)}x_{i_1}x_{i_1}\dots  x_{i_2}x_{i_2}x_{i_2}\dots  x_{i_2}\dots  x_{i_k}x_{i_k}\dots  x_{i_k}},\]
\[f_{x_{i_1}x_{i_1}\dots  x_{i_2}x_{i_2}x_{i_2}\dots  x_{i_2}\dots  x_{i_k}x_{i_k}\dots  x_{i_k}},\quad \tx{or\ }f^{(n)}_{\pht{(n)}x_{i_1}x_{i_1}\dots  x_{i_2}x_{i_2}x_{i_2}\dots  x_{i_2}\dots  x_{i_k}x_{i_k}\dots  x_{i_k}},\]
in which the subscript are $m_{i_j}$ $x_{i_j}$s for all $x_{i_j}$.
\sssc{Euler's notation (歐拉符號) for derivatives}
Suppose a dependent variable $y$ represents a function $f$ of an independent variable $x$, that is,
\[y=f(x).\]
Then, the derivative function of $y$ or $f$ (with respect to $x$) can be written as
\[Df(x),\quad(Df)(x),\quad D_xf(x),\quad (D_xf)(x),\quad \mathrm{d}f(x),\quad(\mathrm{d}f)(x),\quad \mathrm{d}_xf(x),\quad \tx{or\ }(\mathrm{d}_xf)(x),\]
in which $D$, $D_x$, $\mathrm{d}$, or $\mathrm{d}_x$ is called a (unary) differential operators or a derivative operator, and  $Df$, $D_xf$, $\mathrm{d}f$, or $\mathrm{d}_xf$ are called the differential (微分子) of $y$ or $f$ (with respect to $x$);

the $n$th derivative function of $y$ or $f$ (with respect to $x$) can be written as
\[D^nf(x),\quad(D^nf)(x),\quad D^n_{\pht{(n)}xx\dots x}f(x),\quad\qty(D^n_{\pht{(n)}xx\dots x}f)(x),\quad \mathrm{d}^nf(x),\quad(\mathrm{d}^nf)(x),\quad \mathrm{d}^n_{\pht{(n)}xx\dots x}f(x),\quad \tx{or\ }\qty(\mathrm{d}^n_{\pht{(n)}xx\dots x}f)(x),\]
in which subscript are $n$ $x$s, $D^n$, $D^n_{\pht{(n)}xx\dots x}$, $\mathrm{d}^n$, or $\mathrm{d}^n_{\pht{(n)}xx\dots x}$ is called a (unary) differential operators or a derivative operators, and $D^nf$, $D^n_{\pht{(n)}xx\dots x}f$, $\mathrm{d}^nf$, or $\mathrm{d}^n_{\pht{(n)}xx\dots x}f$ is called the differential of the $(n-1)$th derivative function of $y$ or $f$.
\sssc{Euler's notation for partial derivatives}
Suppose a dependent variable $y$ represents a function $f$ of an independent variable vector $\mb{x}=(x_1,x_2,\dots x_n)$, that is,
\[y=f(\mb{x}).\]
Then, the partial derivative function of $y$ or $f$ with respect to $x_i$ can be written as
\[\partial_{x_i}f(\mb{x}),\quad \qty(\partial_{x_i}f)(\mb{x}),\quad \mathrm{d}_{x_i}f(\mb{x})\tx{or\ }\qty(\mathrm{d}_{x_i}f)(\mb{x}),\quad D_{x_i}f(\mb{x})\tx{or\ }\qty(D_{x_i}f)(\mb{x}),\]
in which $\partial_{x_i}$, $\mathrm{d}_{x_i}$, or $D_{x_i}$ is called a partial differential operator or a partial derivative operator;

the $n$th partial derivative function of $y$ or $f$ with respect to $x_i$ can be written as
\[\partial_{x_ix_i\dots  x_i}f(\mb{x}),\quad\partial^{(n)}_{\pht{(n)}x_ix_i\dots  x_i}f(\mb{x}),\quad D_{x_ix_i\dots  x_i}f(\mb{x}),\quad D^{(n)}_{\pht{(n)}x_ix_i\dots  x_i}f(\mb{x}),\quad \mathrm{d}_{x_ix_i\dots  x_i}f(\mb{x}),\quad\tx{or\ }\mathrm{d}^{(n)}_{\pht{(n)}x_ix_i\dots  x_i}f(\mb{x}),\]
in which subscript are $n$ $x_i$s, $^{(n)}$ can be replaced with $n$ primes ($'$) for usually $n<4$ (For example, $\partial''$ is equivalent to $\partial^{(2)}$, and $D''$ is equivalent to $D^{(2)}$.), and $\partial_{x_ix_i\dots  x_i}$, $\partial^{(n)}_{\pht{(n)}x_ix_i\dots  x_i}$, $D_{x_ix_i\dots  x_i}$, $D^{(n)}_{\pht{(n)}x_ix_i\dots  x_i}$, $\mathrm{d}D_{x_ix_i\dots  x_i}$, or $\mathrm{d}^{(n)}_{\pht{(n)}x_ix_i\dots  x_i}$ is called a partial differential operator or a partial derivative operator;

the $n$th mixed partial derivative function of $y$ or $f$ $m_{i_1}$ times with respect to $x_{i_1}$, $m_{i_1}$ times with respect to $x_{i_2}$, $\dots $, $m_{i_k}$ times with respect to $x_{i_k}$, in which $\sum_{j=1}^km_{i_j}=n$, can be written as
C\[\partial_{x_{i_1}x_{i_1}\dots  x_{i_2}x_{i_2}x_{i_2}\dots  x_{i_2}\dots  x_{i_k}x_{i_k}\dots  x_{i_k}}f(\mb{x}),\quad\partial^{(n)}_{\pht{(n)}x_{i_1}x_{i_1}\dots  x_{i_2}x_{i_2}x_{i_2}\dots  x_{i_2}\dots  x_{i_k}x_{i_k}\dots  x_{i_k}}f(\mb{x}),\]
\[ D_{x_{i_1}x_{i_1}\dots  x_{i_2}x_{i_2}x_{i_2}\dots  x_{i_2}\dots  x_{i_k}x_{i_k}\dots  x_{i_k}}f(\mb{x}),\quad D^{(n)}_{\pht{(n)}x_{i_1}x_{i_1}\dots  x_{i_2}x_{i_2}x_{i_2}\dots  x_{i_2}\dots  x_{i_k}x_{i_k}\dots  x_{i_k}}f(\mb{x}),\]
\[ \mathrm{d}_{x_{i_1}x_{i_1}\dots  x_{i_2}x_{i_2}x_{i_2}\dots  x_{i_2}\dots  x_{i_k}x_{i_k}\dots  x_{i_k}}f(\mb{x}),\quad\tx{or\ }\mathrm{d}^{(n)}_{\pht{(n)}x_{i_1}x_{i_1}\dots  x_{i_2}x_{i_2}x_{i_2}\dots  x_{i_2}\dots  x_{i_k}x_{i_k}\dots  x_{i_k}}f(\mb{x}),\]
in which subscript are $m_{i_j}$ $x_{i_j}$s for all $x_{i_j}$, $^{(n)}$ can be replaced with $n$ primes ($'$) for usually $n<4$ (For example, $\partial''$ is equivalent to $\partial^{(2)}$, and $D''$ is equivalent to $D^{(2)}$.), and $\partial_{x_ix_i\dots  x_i}$, $\partial^{(n)}_{\pht{(n)}x_ix_i\dots  x_i}$, $D_{x_ix_i\dots  x_i}$, $D^{(n)}_{\pht{(n)}x_ix_i\dots  x_i}$, $\mathrm{d}_{x_ix_i\dots  x_i}$, and $\mathrm{d}^{(n)}_{\pht{(n)}x_ix_i\dots  x_i}$ is called a partial differential operator or a partial derivative operator.
\ssc{Ordinary Derivatives of Functions with Real Domain}
Let $W$ be a topological vector space. The ordinary derivative (常導數) or derivative (導數) $f'(x)$ of a function $f\colon U\subseteq\mathbb{R}\to W$ (with respect to $x$) at $x\in U$ is defined as
\[f'(x)=\lim_{h\to 0}\frac{f(x+h)-f(x)}{h}\]
if the limit exists. If such limit exists, we say $f$ is differentiable (可微的) at $x$.

We define the (first(-order)) ordinary derivative (function) (常導（函）數) or derivative (function) (導（函）數) of $f$ (with respect to $x$) as a function $f'$ with codomain $W$ such that for any $x\in U$ at which $f$ is differentiable, $f'$ maps $x$ to the derivative of $f$ at $x$.

The derivative of the $k$th(-order) ($k\in\mathbb{N}$) derivative function of $f$ at $x\in U$ is called the $(k+1)$th(-order) derivative of $f$ (with respect to $x$) at $x\in U$. The derivative function of the $k$th(-order) ($k\in\mathbb{N}$) derivative function of $f$ is called the $k+1$th(-order) derivative function of $f$ (with respect to $x$).

If for any $n\in\mathbb{N}$, the $n$th(-order) derivative of a function $f$ at a point $x$ in its domain exists, we say $f$ is infinitely differentiable at $x$.

If $f$ is differentiable at all point in $I\subseteq U$, we say $f$ is differentiable on $I$; if $f$ is differentiable on $U$, we say $f$ is differentiable. If $f^{(n-1)}$ exists and is differentiable at all point in $I\subseteq U$, we say $f$ is $n$-times differentiable on $I$; if $f^{(n-1)}$ exists and is differentiable on $U$, we say $f$ is $n$-times differentiable.

The operation of finding the derivative or derivative function is called ordinary differentiation (常微分) or differentiation (微分).

Specifically, the $0$th(-order) derivative of $f$ is $f$ itself.

The derivative of a $f$ at $x$ represents the slope (斜率) at $x$ or the lineal element at $x$ (a miniature tangent line at $x$).
\ssc{One-sided Derivative of Functions with Real Domain}
Let $W$ be a topological vector space and $f\colon U\subseteq\mathbb{R}\to W$ be a function.

The left-hand derivative of $f$ at $x\in U$, denoted as $f'_-(x)$, is defined as
\[f'_-(x)=\lim_{h\to 0^-}\frac{f(x+h)-f(x)}{h}\]
if the limit exists.

The right-hand derivative of $f$ at $x\in U$, denoted as $f'_+(x)$, is defined as
\[f'_+(x)=\lim_{h\to 0^+}\frac{f(x+h)-f(x)}{h}\]
if the limit exists.
\ssc{Ordinary derivatives of Functions with Real Vector Domain}
Let $W$ be a normed vector space. A function $f\colon U\subseteq\mathbb{R}^n\to W$ with $n\in\bbN$ is differentiable at $x\in U$ if there exists a bounded linear operator $A\colon\bbR^n\to W$ such that
\[\lim_{\|h\|\to 0}\frac{\|f(x+h)-f(x)-A(h)\|_W}{\|h\|}=0.\]
If there exists such an operator $A$, it is unique, so we define the (first(-order)) ordinary derivative or derivative of $f$ (with respect to $x$) at $x$, denoted as $Df(x)$, as $A$.

We define the (first(-order)) ordinary derivative (function) or derivative (function) of $f$ (with respect to $x$) as a function $Df$ with codomain $B(\bbR^n,W)$, in which $B(\bbR^n,W)$ is the space of all bounded linear operators from $\bbR^n$ to $W$, such that for any $x\in U$ at which $f$ is differentiable, $Df$ maps $x$ to the derivative of $f$ at $x$.

The derivative of the $k$th(-order) ($k\in\mathbb{N}$) derivative function of $f$ at $x\in U$ is called the $k+1$th(-order) derivative of $f$ (with respect to $x$) at $x\in U$, denoted as $(D^{k+1}f)(x)$. The derivative function of the $k$th(-order) ($k\in\mathbb{N}$) derivative function of $f$ is called the $k+1$th(-order) derivative function of $f$ (with respect to $x$), denoted as $D^{k+1}f$.

If $f$ is differentiable at all point in $I\subseteq U$, we say $f$ is differentiable on $I$; if $f$ is differentiable on $U$, we say $f$ is differentiable. If $D^{(n-1)}f$ exists and is differentiable at all point in $I\subseteq U$, we say $f$ is $n$-times differentiable on $I$; if $D^{(n-1)}f$ exists and is differentiable on $U$, we say $f$ is $n$-times differentiable.

The operation of finding the derivative or derivative function is called ordinary differentiation or differentiation.

Specifically, the $0$th(-order) Fréchet derivative of $f$ is $f$ itself.
\ssc{Partial derivatives of Functions with Real Vector Domain}
Let $W$ be a topological vector space and $f\colon U\subseteq\mathbb{R}^n\to W$ be a function, $\mb{x}$ be the independent variable vector of $f$, and $X$ be the set of all independent variables of $f$.

The (first(-order)) partial derivative (偏導數) $\pdv{f}{x_i}$ of $f$ with respect to $x_i\in X$ at $u\in U$ is defined as
\bma
\pdv{f}{x_i}&=\lim_{h\to 0}\frac{f(u+h\mb{e}_i)-f(u)}{h}\\
&=\frac{\mathrm{d}}{\mathrm{d}h}f(u+h\mb{e}_i)\big\vert_{h=0}
\end{aligned},\]
in which $\mb{e}_i\in\mathbb{R}^n$ is the unit vector in the direction of $x_i$.

The partial derivative of the $k$th(-order) ($k\in\mathbb{N}$) partial derivative (function) (偏導（函）數) of $f$ with respect to $ x \in X$ with respect to $ x \in X$ at $u\in U$ is called the $k+1$th(-order) partial derivative of $f$ with respect to $ x$ at $u\in U$. The partial derivative function of the $k$th(-order) ($k\in\mathbb{N}$) partial derivative function of $f$ with respect to $ x \in X$ with respect to $ x \in X$ is called the $k+1$th(-order) partial derivative function of $f$ with respect to $ x$.

The partial derivative of the $k$th(-order) partial derivative function of $f$ with respect to $ x_1\in X$ with respect to $ x_2\in X$ at $u\in U$ is called the $(k+1)$th(-order) mixed partial derivative of $f$ with respect to $ x_1, x_1,\dots  x_1$ ($k$ times) and $ x_2$ at $u\in U$. The partial derivative function of the $k$th(-order) partial derivative function of $f$ with respect to $ x_1\in X$ with respect to $ x_2\in X$ at $u\in U$ is called the $(k+1)$th(-order) mixed partial derivative function of $f$ with respect to $ x_1, x_1,\dots  x_1$ ($k$ times) and $ x_2$.

The partial derivative of the $k$th(-order) mixed partial derivative function of $f$ with respect to $ x_1, x_2,\dots  x_k\in X$ with respect to $ x_{k+1}\in X$ at $u\in U$ is called the $(k+1)$th(-order) mixed partial derivative of $f$ with respect to $ x_1, x_2,\dots  x_k, x_{k+1}$ at $u\in U$. The partial derivative function of the $k$th(-order) mixed partial derivative function of $f$ with respect to $ x_1, x_2,\dots  x_k\in X$ with respect to $ x_{k+1}\in X$ is called the $(k+1)$th(-order) mixed partial derivative function of $f$ with respect to $ x_1, x_2,\dots  x_k, x_{k+1}$.

The operation of finding the partial derivative or partial derivative function is called partial differentiation (偏微分).

Specifically, the $0$th(-order) partial derivative of $f$ is $f$ itself.
\ssc{Fréchet derivative (弗蘭歇導數)}
\sssc{Fréchet derivative}
Let $V$ and $W$ be normed vector spaces and $U$ be an open subset of $V$. A function $f\colon U\to W$ is Fréchet differentiable (弗蘭歇可微的) or differentiable at $x\in U$ if there exists a bounded linear operator $A\colon V\to W$ such that
\[\lim_{\|h\|_V\to 0}\frac{\|f(x+h)-f(x)-A(h)\|_W}{\|h\|_V}=0.\]
If there exists such an operator $A$, it is unique, so we define the (first(-order)) Fréchet derivative, ordinary derivative, or derivative of $f$ (with respect to $x$) at $x$, denoted as $Df(x)$, as $A$.

We define the (first(-order)) Fréchet derivative (function) (弗蘭歇導（函）數), ordinary derivative (function), or derivative (function) of $f$ (with respect to $x$) as a function $Df$ with codomain $B(V,W)$, in which $B(V,W)$ is the space of all bounded linear operators from $V$ to $W$, such that for any $x\in U$ at which $f$ is Fréchet differentiable, $Df$ maps $x$ to the Fréchet derivative of $f$ at $x$.

The Fréchet derivative of the $k$th(-order) ($k\in\mathbb{N}$) Fréchet derivative function of $f$ at $x\in U$ is called the $k+1$th(-order) Fréchet derivative of $f$ (with respect to $x$) at $x\in U$, denoted as $(D^{k+1}f)(x)$. The Fréchet derivative function of the $k$th(-order) ($k\in\mathbb{N}$) Fréchet derivative function of $f$ is called the $k+1$th(-order) Fréchet derivative function of $f$ (with respect to $x$), denoted as $D^{k+1}f$.

If $f$ is Fréchet differentiable at all point in $I\subseteq U$, we say $f$ is Fréchet differentiable on $I$; if $f$ is Fréchet differentiable on $U$, we say $f$ is Fréchet differentiable. If $D^{(n-1)}f$ exists and is Fréchet differentiable at all point in $I\subseteq U$, we say $f$ is $n$-times Fréchet differentiable on $I$; if $D^{(n-1)}f$ exists and is Fréchet differentiable on $U$, we say $f$ is $n$-times Fréchet differentiable.

The operation of finding the Fréchet derivative or Fréchet derivative function is called Fréchet differentiation (弗蘭歇微分), ordinary differentiation, or differentiation.

Specifically, the $0$th(-order) Fréchet derivative of $f$ is $f$ itself.
\ssc{Smoothness (光滑性 or 平滑性)}
\sssc{Continuously differentiable functions}
A function is called continuously ($n$-times) differentiable iff it is ($n$-times) differentiable and its ($n$th) derivative is continuous.
\sssc{Smoothness}
A function $f$ with real domain that has a derivative that is continuous on its domain is said to be of class $C^1$ or continuously differentiable, denoted as $f\in C^1$, or be a $C^1$-function.

A function $f$ that has a derivative that is continuous on a subset $I$ of its domain is said to be of class $C^1$ on $I$, of class $C^k(I)$, or continuously differentiable on $I$, denoted as $f\in C^k(I)$.

A function $f$ with real domain that has a $k$th derivative that is continuous on its domain is said to be of class $C^k$ or $k$-times continuously differentiable, denoted as $f\in C^k$, or be a $C^k$-function.

A function $f$ that has a $k$th derivative that is continuous on a subset $I$ of its domain is said to be of class $C^k$ on $I$ or of class $C^k(I)$, denoted as $f\in C^k(I)$.

Generally, the term smooth function refers to a $C^{\infty}$-function, that is a function that is of class $C^k$ for any $k\in\bbR$. However, it may also mean "sufficiently differentiable" for the problem under consideration.
\sssc{Piecewise smoothness of real functions}
Let $f\colon D\subseteq\mathbb{R}\to\mathbb{R}$ be a function. If there exists a family $\{[a_i,b_i]\mid i\in I\}$ of closed intervals such that:
\begin{itemize}
\item for every closed interval $V$, the set $\{i\in I\mid V\cap [a_i,b_i]\neq\varnothing\}$ is finite (some sources require instead that $n(I)$ is finite, which is stronger),
\item $D\subseteq\bigcup_{i\in I}[a_i,b_i]$,
\item function $f\big\vert_{a_i}^{b_i}\colon(a_i,b_i)\to Y$ is of class $C^k((a_i,b_i))$ for every $i\in I$, and
\item there exist finite $\lim_{x\to a_i^{\phantom{i}+}}f(x)$ and $\lim_{x\to b_i^{\phantom{i}-}}f(x)$ for every $i\in I$.
\end{itemize},
we say $f$ is of class piecewise $C^k$ (or of class piecewise $C^k\qty(\bigcup_{i\in I}[a_i,b_i])$); when $k=\infty$ or sufficiently large, we say $f$ is piecewise smooth (on $\bigcup_{i\in I}U_i$).
\sssc{Piecewise smoothness of functions between topological spaces}
Let $X$ and $Y$ be topological spaces, and $f\colon D\subseteq X\to Y$ be a function. If there exists a locally finite (some sources require instead finite, which is stronger) family $\{U_i\mid i\in I\}$ of closed subsets of $X$ such that:
\begin{itemize}
\item $D\subseteq\bigcup_{i\in I}U_i$, and
\item there exists a function $g_i\colon U_i\to Y$ that is of class $C^k\qty(U_i)$ such that $\forall x\in\operatorname{int}\left(U_i\right)f(x)=g_i(x)$ for every $i\in I$.
\end{itemize},
we say $f$ is of class piecewise $C^k$ (or of class piecewise $C^k\qty(\bigcup_{i\in I}U_i)$); when $k=\infty$ or sufficiently large, we say $f$ is piecewise smooth (on $\bigcup_{i\in I}U_i$).
\ssc{Gateaux Differentiation (加托微分)}
\sssc{Gateaux derivative (加托導數) and partial derivatives}
Let $V$ be a locally convex topological vector spaces (LCTVS), $W$ be a topological vector space, $U$ be an open subset of $V$, and $f\colon U\to W$ be a function.

The (first(-order)) Gateaux derivative $df(u;\,\psi)$ of $f$ at $u\in U$ in the direction $\psi \in V$ is defined to be
\[\begin{aligned}
df(u;\,\psi) &= \lim_{\tau\to 0}\frac{f(u+\tau \psi)-f(u)}{\tau}\\
&= \frac{\mathrm{d}}{\mathrm{d}\tau}f(u+\tau \psi)\big\vert_{\tau =0}
\end{aligned}\]
if the limit exists. If for any $\psi \in V$, the Gateaux derivative exists, then it is said that $f$ is Gateaux differentiable (加托可微的) at $u$.

We define the (first(-order)) Gateaux derivative (function) (加托導（函）數) of $f$ in the direction $\psi \in V$, denoted as $df$, as a function $df\colon U\to W$, such that for any $u\in U$ at which $f$ is Gateaux differentiable, $df$ maps $u$ to the Gateaux derivative of $f$ at $u$ in the direction $\psi \in V$.

The Gateaux derivative of the $k$th(-order) ($k\in\mathbb{N}$) Gateaux derivative function of $f$ in the direction $\psi \in V$ in the direction $\psi \in V$ at $u\in U$ is called the $k+1$th(-order) Gateaux derivative of $f$ in the direction $\psi$ at $u\in U$. The Gateaux derivative function of the $k$th(-order) ($k\in\mathbb{N}$) Gateaux derivative function of $f$ in the direction $\psi \in V$ in the direction $\psi \in V$ is called the $k+1$th(-order) Gateaux derivative function of $f$ in the direction $\psi$.

The Gateaux derivative of the $k$th(-order) Gateaux derivative function of $f$ in the direction $\psi_1\in V$ in the direction $\psi_2\in V$ at $u\in U$ is called the $(k+1)$th(-order) mixed Gateaux derivative of $f$ in the direction $\psi_1,\psi_1,\dots \psi_1$ ($k$ times) and $\psi_2$ at $u\in U$. The Gateaux derivative function of the $k$th(-order) Gateaux derivative function of $f$ in the direction $\psi_1\in V$ in the direction $\psi_2\in V$ is called the $(k+1)$th(-order) mixed Gateaux derivative function of $f$ in the direction $\psi_1,\psi_1,\dots \psi_1$ ($k$ times) and $\psi_2$.

The Gateaux derivative of the $k$th(-order) mixed Gateaux derivative function of $f$ in the direction $\psi_1,\psi_2,\dots \psi_k\in V$ in the direction $\psi_{k+1}\in V$ at $u\in U$ is called the $(k+1)$th(-order) mixed Gateaux derivative of $f$ in the direction $\psi_1,\psi_2,\dots \psi_k,\psi_{k+1}$ at $u\in U$. The Gateaux derivative function of the $k$th(-order) mixed Gateaux derivative function of $f$ in the direction $\psi_1,\psi_2,\dots \psi_k\in V$ in the direction $\psi_{k+1}\in V$ is called the $(k+1)$th(-order) mixed Gateaux derivative function of $f$ in the direction $\psi_1,\psi_2,\dots \psi_k,\psi_{k+1}$.

The operation of finding the Gateaux derivative or Gateaux derivative function is called Gateaux differentiation (加托微分).

Specifically, the $0$th(-order) Gateaux derivative of $f$ is $f$ itself.
\sssc{Partial derivative}
Let $V$ be a locally convex topological vector spaces (LCTVS), $W$ be a topological vector space, $U$ be an open subset of $V$, and $f\colon U\to W$ be a function, $\mb{x}$ be the independent variable vector of $f$, and $X$ be the set of all independent variables of $f$.

The (first(-order)) partial derivative of $f$ with respect to $x_i\in X$ at $u\in U$ is defined as the Gateaux derivative of $f$ in the direction of $x_i$ at $u$ if it exists.

The partial derivative of the $k$th(-order) ($k\in\mathbb{N}$) partial derivative (function) of $f$ with respect to $ x \in X$ with respect to $ x \in X$ at $u\in U$ is called the $k+1$th(-order) partial derivative of $f$ with respect to $ x$ at $u\in U$. The partial derivative function of the $k$th(-order) ($k\in\mathbb{N}$) partial derivative function of $f$ with respect to $ x \in X$ with respect to $ x \in X$ is called the $k+1$th(-order) partial derivative function of $f$ with respect to $ x$.

The partial derivative of the $k$th(-order) partial derivative function of $f$ with respect to $ x_1\in X$ with respect to $ x_2\in X$ at $u\in U$ is called the $(k+1)$th(-order) mixed partial derivative of $f$ with respect to $ x_1, x_1,\dots  x_1$ ($k$ times) and $ x_2$ at $u\in U$. The partial derivative function of the $k$th(-order) partial derivative function of $f$ with respect to $ x_1\in X$ with respect to $ x_2\in X$ at $u\in U$ is called the $(k+1)$th(-order) mixed partial derivative function of $f$ with respect to $ x_1, x_1,\dots  x_1$ ($k$ times) and $ x_2$.

The partial derivative of the $k$th(-order) mixed partial derivative function of $f$ with respect to $ x_1, x_2,\dots  x_k\in X$ with respect to $ x_{k+1}\in X$ at $u\in U$ is called the $(k+1)$th(-order) mixed partial derivative of $f$ with respect to $ x_1, x_2,\dots  x_k, x_{k+1}$ at $u\in U$. The partial derivative function of the $k$th(-order) mixed partial derivative function of $f$ with respect to $ x_1, x_2,\dots  x_k\in X$ with respect to $ x_{k+1}\in X$ is called the $(k+1)$th(-order) mixed partial derivative function of $f$ with respect to $ x_1, x_2,\dots  x_k, x_{k+1}$.

The operation of finding the partial derivative or partial derivative function is called partial differentiation.

Specifically, the $0$th(-order) partial derivative of $f$ is $f$ itself.
\ssc{Applicational interpretation}
\sssc{Rate of change}
If $y=f(x)$, then $f'(a)$ where defined is called the instantaneous rate of change of $y$ with repsect to $x$ at $a$.
A function $f\colon U\to W$ is Fréchet differentiable (弗蘭歇可微的) or differentiable at $x\in U$ if there exists a bounded linear operator $A\colon V\to W$ such that
\[\lim_{\|h\|_V\to 0}\frac{\|f(x+h)-f(x)-A(h)\|_W}{\|h\|_V}=0.\]
If there exists such an operator $A$, it is unique, so we define the (first(-order)) Fréchet derivative, ordinary derivative, or derivative of $f$ (with respect to $x$) at $x$, denoted as $Df(x)$, as $A$.

We define the (first(-order)) Fréchet derivative (function) (弗蘭歇導（函）數), ordinary derivative (function), or derivative (function) of $f$ (with respect to $x$) as a function $Df$ with codomain $B(V,W)$, in which $B(V,W)$ is the space of all bounded linear operators from $V$ to $W$, such that for any $x\in U$ at which $f$ is Fréchet differentiable, $Df$ maps $x$ to the Fréchet derivative of $f$ at $x$.

The Fréchet derivative of the $k$th(-order) ($k\in\mathbb{N}$) Fréchet derivative function of $f$ at $x\in U$ is called the $k+1$th(-order) Fréchet derivative of $f$ (with respect to $x$) at $x\in U$, denoted as $(D^{k+1}f)(x)$. The Fréchet derivative function of the $k$th(-order) ($k\in\mathbb{N}$) Fréchet derivative function of $f$ is called the $k+1$th(-order) Fréchet derivative function of $f$ (with respect to $x$), denoted as $D^{k+1}f$.

If $f$ is Fréchet differentiable at all point in $I\subseteq U$, we say $f$ is Fréchet differentiable on $I$; if $f$ is Fréchet differentiable on $U$, we say $f$ is Fréchet differentiable. If $D^{(n-1)}f$ exists and is Fréchet differentiable at all point in $I\subseteq U$, we say $f$ is $n$-times Fréchet differentiable on $I$; if $D^{(n-1)}f$ exists and is Fréchet differentiable on $U$, we say $f$ is $n$-times Fréchet differentiable.

The operation of finding the Fréchet derivative or Fréchet derivative function is called Fréchet differentiation (弗蘭歇微分), ordinary differentiation, or differentiation.

Specifically, the $0$th(-order) Fréchet derivative of $f$ is $f$ itself.

If $y=f(x)$ is defined on some interval $[a,b]$ with $b>a$, then $\frac{f(b)-f(a)}{b-a}$ is called the average rate of change of $y$ with repsect to $x$ over $[a,b]$.
\sssc{Physical interpretation}
Let $f$ be a function with domain being $\bbR$ or $\bbR_{\geq 0}$. If we interprets $f(t)$ as the position of $y$ at time $t$, then:
\bit
\item $f$ is called the position function of $y$,
\item if $f(x)$ is defined on some interval $[a,b]$ with $b>a$, then $f(b)-f(a)$ is called the displacement of $y$ over $[a,b]$,
\item $f'$ is called the velocity function of $y$,
\item $f'(a)$ where defined is called the instantaneous velocity of $y$ at $a$,
\item if $f(x)$ is defined on some interval $[a,b]$ with $b>a$, then $\frac{f(b)-f(a)}{b-a}$ is called the average velocity of $y$ over $[a,b]$,
\item $f''$ is called the acceleration function of $y$,
\item $f''(a)$ where defined is called the instantaneous acceleration of $y$ at $a$,
\item if $f'(x)$ is defined on some interval $[a,b]$ with $b>a$, then $\frac{f'(b)-f'(a)}{b-a}$ is called the average acceleration of $y$ over $[a,b]$,
\item $f'''$ is called the jerk function of $y$,
\item $f'''(a)$ where defined is called the instantaneous jerk of $y$ at $a$, and
\item if $f''(x)$ is defined on some interval $[a,b]$ with $b>a$, then $\frac{f''(b)-f''(a)}{b-a}$ is called the average jerk of $y$ over $[a,b]$.
\eit
\sssc{Biological interpretation}
Let $f$ be a function with domain being $\bbR$ or $\bbR_{\geq 0}$. If we interprets $f(t)$ as the number of some population $y$ at time $t$, then:
\bit
\item $f$ is called the growth (function) of $y$,
\item $f'(a)$ where defined is called the instantaneous rate of growth of $y$ at $a$, and
\item if $f(x)$ is defined on some interval $[a,b]$ with $b>a$, then $\frac{f(b)-f(a)}{b-a}$ is called the average rate of growth of $y$ over $[a,b]$.
\eit
\sssc{Economical interpretation}
Let $f$ be a function with domain being $\bbR$ or $\bbR_{\geq 0}$. If we interprets $f(t)$ as the benefit (aka revenue) or cost of consuming or producing $y$ of quantity $t$, then:
\bit
\item $f$ is called benefit (aka revenue) or cost (function) of $y$, and
\item $f'(a)$ where defined is called the margin benefit (aka revenue) or cost of $y$ at $a$.
\eit
\ssc{Differentials (微分子)}
Suppose a dependent variable $y$ represents a function $f$ of an independent variable $x$, that is,
\[y=f(x),\]
with the domain $V$ of $f$ being a subset of some vector space.

If $y$ or $f$ is defined and differentiable on some set $U\subseteq V$ with the derivative function denoted as $y'$ or $f'(x)$, then we define the differential of them, denoted as $\dd{y}$ and $\dd{f}$ respectively, in terms of the differential of $x$, denoted as $\dd{x}$, by the equations
\[\dd{y}=y'\dd{x},\quad \tx{and\ }\dd{f}=f'\dd{x}\]
respectively, in which the differentials of $x$, $y$, or $f$ are often called to be an infinitesimal changes in $x$, $y$, or $f$.
\ssc{Antiderivative (反導函數), inverse derivative, primitive function, primitive integral, or indefinite integral (不定積分)}
\sssc{Definition}
An antiderivative (aka inverse derivative, primitive function, primitive integral, or indefinite integral) of a function $f(x)$ on subset $I$ of the domain of $f$ is a differentiable function $F(x)$ such that
\[F'(x)=f(x)\]
for all $x\in I$.

The process of solving for antiderivatives is called antidifferentiation (反微分) or indefinite integration (不定積分).
\sssc{Theorem}
If a function $F(x)$ with codomain $Y$ is an antiderivative of a function $f(x)$ on subset $I$ of the domain of $f$, then the family of all antiderivatives of $f(x)$ on $I$ is
\[\{F(x)+C\mid C\in Y\}.\]
\sssc{Notation}
If a function $F(x)$ with codomain $Y$ is an antiderivative of a function $f(x)$ on subset $I$ of the domain of $f$, we write
\[\int f(x)\,\mathrm{d}x=F(x)+C,\]
where $C$ denotes an arbitrary constant in $Y$ that does not depend on $x$, called constant of integration.
\subsection{Taylor series (泰勒級數) or Taylor expansion (泰勒展開)}
Assume that $F:\,\mathbb{R}\to\mathbb{R}$ is an infinitely differentiable function, and its derivatives of every order exist on $\mathbb{R}$, then the Taylor series of $F$ at $a$ is
\[F(x) = \sum_{n\in\mathbb{N}_0} \frac{F^{(n)}(a)}{n!}(x-a)^n,\]
that is,
\[F(x) = \sum^k_{n=0} \frac{F^{(n)}(a)}{n!}(x-a)^n+\int_0^1\frac{(1-t)^k}{k!}F^{(k+1)}(a+t(x-a))(x-a)^{k+1}\,\mathrm{d}t.\]
Also, the $k$th-order approximation of $f$ near $a$ is
\[F(x) \approx \sum^k_{n=0} \frac{F^{(n)}(a)}{n!}(x-a)^n,\]
and the first-order (aka linear or tangent line) approximation of $f$ near $a$ is
\[F(x) \approx F(a)+F'(a)(x-a).\]
THe linearization of $f$ near $a$ is
\[L(x)=F(a)+F'(a)(x-a).\]
The Taylor series of $F$ at $0$ is called Maclaurin series (馬克勞林級數) or Maclaurin expansion (馬克勞林展開).
\ssc{Differential equation (微分方程)}
\sssc{Differential Equation}
An equation containing the derivatives of one or more unknown functions or dependent variables, with respect to one or more independent variables, is said to be a differential equation (DE).
\sssc{Ordinary differential equation (ODE) (常微分方程)}
If a differential equation contains only ordinary derivatives of one or more unknown functions with respect to a single independent variable, it is said to be an ordinary differential equation (ODE).
\sssc{Partial differential equation (PDE) (偏微分方程)}
If a DE contains partial derivatives of one or more unknown functions of two or more independent variables is called a partial differential equation (PDE).
\sssc{Order of a DE}
The order of a differential equation is the order of the highest derivative in the equation.
\ssc{Real analyticity (實解析性) of real-codomain functions}
A real-codomain function $f$ is real analytic at a point $x_0$ in its domain, if it is infinitely differentiable at $x_0$ and that the Taylor expansion of $f$ at $x_0$ converges to $f(x)$ pointwise for any $x$ in a neighborhood of $x_0$.

A real-codomain function is called to be real analytic on an interval $I$ that is a subset of its domain if it is real analytic at any point in $I$.

A real-codomain function is called to be real analytic if it is real analytic at any point in its domain.



\section{Definte Integration (定積分)}
\ssc{Notation}
The integral of a function $f(x)$ with domain being a subset of $\bbR$, called integrand (被積函數), with respect to $x$, called integration variable (積分變數), from $a$ to $b$, which the open interval between $a$ and $b$ is called the domain of integration (積分域) or interval of integration (積分區間) and $a,b$ are called limits of integration (積分極限 or 積分上下限), is denoted as
\[\int_a^bf(x)\dd{x},\]
in which when $a>b$, the integral is defined as
\[\int_a^bf(x)\dd{x}\coloneq-\int_b^af(x)\dd{x}.\]

The integral of a function $f(\omega)$ with respect to $\omega$ over $\Omega$, called the domain of integration and which the limit points of $\Omega$ are called are called limits of integration (積分極限), is denoted as
\[\int_{\Omega}f(\omega)\dd{\omega}.\]
\subsection{(Proper) Riemann integral (黎曼積分) and Darboux integral (達布積分)}
\sssc{Premise}
(Proper) Riemann integral and Darboux integral are two equivalant definitions of definite integral of functions over compact intervals of $\mathbb{R}$ to $\mathbb{R}$.
\subsubsection{Partition of an interval}
A partition $P(x, n)$ of a compact interval $[a,b]$ is a finite sequence of numbers of the form
\[P(x, n):=\{x_i\colon x_0=a\land x_n=b\land\forall 1\leq i<j\leq n\colon x_i<x_j\}_{i=0}^n.\]

Each $[x_i, x_{i+1}]$ is called a subinterval of the partition. The length of a closed interval $[c,d]$ is defined as $d-c$. The mesh or norm of a partition is defined as
\[\max_i\left(x_{i+1}-x_{i}\right)\]
for every integer $i\in [0,n-1]$.

A tagged partition $P(x,n,\xi)$ of a interval $(a,b)$ is a partition together with a choice of a sample point within each of all $n$ subintervals, that is, numbers $\{\xi_i\}_{i=0}^{n-1}$ with $\xi_i\in [x_i,x_{i+1}]$ for each integer $i\in [0,n-1]$. The mesh of a tagged partition is the same as that of an ordinary partition.

Suppose that two tagged partitions $P(x,n,\xi)$ and $Q(y,m,\zeta)$ are both partitions of the interval $[a,b]$. We say that $Q(y,m,\zeta)$ is a refinement of $P(x,n,\xi)$ if for each integer $i\in [0,n-1]$, there exists an integer $r(i)\in [0,m-1]$ such that $x_i = y_{r(i)}$ and that $\forall i\in [0,n-1]\colon\exists j\in [r(i),r(i + 1)] \text{\ s.t.\ }\xi_i = \zeta_j$. That is, a tagged partition breaks up some of the subintervals and adds sample points where necessary, "refining" the accuracy of the partition.

We can turn the set of all tagged partitions into a directed set by saying that one tagged partition is greater than or equal to another if the former is a refinement of the latter.
\subsubsection{Riemann sum}
Let $f$ be a real-valued function defined on a interval $[a,b]$. The Riemann sum of $f$ with respect to a tagged partition $P(x,n,\xi)$ of $[a,b]$ is defined to be
\[R(f,P):=\sum_{i=0}^{n-1}f(\xi_i)\left(x_{i+1}-x_i\right).\]
Each term in the sum is the product of the value of the function at a given point and the length of an interval. Consequently, each term represents the signed area of a rectangle with height $f(\xi_i)$ and width $x_{i + 1} − x_i$. Thus the Riemann sum is the signed area of all the rectangles.
\subsubsection{Darboux sum}
Let $f$ be a real-valued function defined on a interval $[a,b]$. Lower and upper Darboux sums of $f$ with respect to a partition $P(x,n)$ of $[a,b]$ are two specific Riemann sums of which the tags are chosen to be the infimum and supremum (respectively) of $f$ on each subinterval:
\[\begin{aligned}
L(f,P)&:=\sum_{i=0}^{n-1}\inf_{\xi\in [x_i,x_{i+1}]}f(\xi)(x_{i+1}-x_i),\\
U(f,P)&:=\sum_{i=0}^{n-1}\sup_{\xi\in [x_i,x_{i+1}]}f(\xi)(x_{i+1}-x_i).
\end{aligned}\] 
\subsubsection{Riemann integral}
Let $f$ be a real-valued function defined on a interval $[a,b]$. The Riemann integral of $f$ on $[a,b]$ exists and equals $s$ if for all $\varepsilon > 0$, there exists $\delta > 0$ such that for any tagged partition $P(x,n,\xi)$ whose mesh is less than $\delta$,
\[\abs{R(f,P)-s}<\varepsilon .\]
\subsubsection{Darboux integral}
Let $f$ be a real-valued function defined on a interval $[a,b]$. The Darboux integral of $f$ on $[a,b]$ exists and equals $s$ if for all $\varepsilon > 0$, there exists $\delta > 0$ such that for any partition $P$ whose mesh is less than $\delta$,
\[\abs{U(f,P)-s}<\varepsilon \land \abs{L(f,P)-s}<\varepsilon.\]
\sssc{Lebesgue-Vitali theorem (of characterization of the Riemann integrable functions)}
A function on bounded a compact interval $I$ is Riemann integrable (i.e. Darboux integrable) over $I$ if and only if it is continuous almost everywhere in $I$.
\ssc{Extensions of Riemann integral}
\sssc{Improper (Riemann) integral (瑕（黎曼）積分)}
An integral $\int _{a}^{b}f(x)\dd{x}$ is an improper integral if one or more of the below conditions occur:
\ben
\item $a=-\infty$,
\item $b=\infty$,
\item $f(x)$ is unbounded or undefined somewhere in $[a,b]$.
\een

The improper integrals are defined by limits as:
\bit
\item for $a=-\infty$:
\[\int _{-\infty }^bf(x)\dd{x}=\lim _{a\to -\infty }\int _{a}^{b}f(x)\dd{x},\]
\item for $b=\infty$:
\[\int _{a}^{\infty }f(x)\dd{x}=\lim _{b\to \infty }\int _{a}^{b}f(x)\dd{x},\]
\item for $f(x)$ that is unbounded or undefined at $a$:
\[\int_a^bf(x)\dd{x}=\lim_{c\to a^+}\int_c^bf(x)\dd{x},\]
\item for $f(x)$ that is unbounded or undefined at $b$:
\[\int_a^bf(x)\dd{x}=\lim_{c\to b^-}\int_a^cf(x)\dd{x},\]
\item for $f(x)$ that is unbounded or undefined at $c\in(a,b)$:
\[\int _{a}^{b}f(x)\dd{x}=\lim_{t\to c^-}\int _{a}^{t}f(x)\dd{x}+\lim_{t\to c^+}\int _{t}^{b}f(x)\dd{x},\]
\eit
in which if any of the terms diverge or is undefined, the improper integral diverges and is undefined; otherwise, the improper integral exists finitely and converges to a finite value.

$\lim _{a\to -\infty }\int _{a}^{b}f(x)\dd{x}$ converges absolutely to $L$ if $\exists\lim _{a\to -\infty }\int _{a}^{b}\abs{f(x)}\dd{x}\land\lim _{a\to -\infty }\int _{a}^{b}\abs{f(x)}\dd{x}=L$.

$\lim _{b\to \infty }\int _{a}^{b}f(x)\dd{x}$ converges absolutely to $L$ if $\exists\lim _{b\to \infty }\int _{a}^{b}f(x)\dd{x}\land\lim _{b\to \infty }\int _{a}^{b}f(x)\dd{x}=L$.

If an improper integral is convergent but not convergent absolutely, we say it converges conditionally.
\sssc{Cauchy principal value (柯西主值) or PV integral}
The Cauchy principal value or PV integral, denoted as p.v., is a weaker notion of convergence, defined by taking symmetric limits. 

The p.v. of $\int _{-\infty}^{\infty}f(x)\dd{x}$, denoted as $\text{p.v.\ }\int_{-\infty}^{\infty} f(x)\dd{x}$, is defined with:
\[\text{p.v.\ }\int_{-\infty}^{\infty} f(x)\dd{x}\coloneq\lim_{R\to\infty} \int_{-R}^{R} f(x)\dd{x},\]
if the limit exists.

The p.v. of $\int _{a}^{b}f(x)\dd{x}$, in which $f(x)$ is unbounded or undefined at $c\in (a,b)$, denoted as $\text{p.v.\ }\int_{a}^{b} f(x)\dd{x}$, is defined with:
\[\text{p.v.\ }\int_{a}^{b} f(x)\dd{x}\coloneq\lim_{\epsilon\to 0}\qty(\int_a^{c-\epsilon}f(x)\dd{x}+\int_{c+\epsilon}^bf(x)\dd{x}),\]
if the limit exists.

If an improper integral converges, the p.v. of it converges.
\subsection{Lebesgue integral (勒貝格積分)}
A definition of definite integral.
\sssc{Premise}
Let $(E,\Sigma,\mu)$ be a measure space. Below, we will define the Lebesgue integral of measurable functions on $E$ to $\mathbb{R}\cup\{-\infty,\infty\}$.
\subsubsection{Of an indicator functions}
The integral of an indicator function $1_S$ of a measurable subset $S$ of $E$ is defined to be
\[\int 1_{S}\,\mathrm{d}{\mu} =\mu (S).\]
\sssc{Of a nonnegative simple function}
A simple function $s$ is a finite real linear combinations of indicator functions of disjoint measurable subsets of $E$, that is,
\[s\colon\sum_ka_k1_{S_k},\]
where the coefficients $a_k$ are real numbers and $S_k$ are disjoint measurable sets. When the coefficients $a_k$ are positive real numbers, $s$ is called nonnegative.

The integral of a nonnegative simple function $s=\sum_ka_k1_{S_k}$ over $E$ is defined to be
\[\int_Es\,\mathrm{d}\mu=\sum_ka_k\int 1_{S_k}\,\mathrm{d}\mu=\sum_ka_k\mu(S_k),\]
where this sum can be finite or $\infty$.

The integral of a nonnegative simple function $s=\sum_ka_k1_{S_k}$ over a subset $B$ of $E$ is defined to be:
\[\int_Bs\,\mathrm{d}\mu=\sum_ka_k\mu \qty(S_k\cap B).\]
\subsubsection{Of a nonnegative measurable function}
Let $f$ be a nonnegative function on some measurable subset $B$ of $E$ into $\mathbb{R}\cup\{-\infty,\infty\}$, called a nonnegative measurable function. We define
\[\int_Bf\,\mathrm{d}\mu=\sup\left\{\int_Bs\,\mathrm{d}\mu\mid\forall x\in B\colon 0\leq s(x)\leq f(x)\land s\text{\ is a nonnegative simple function}\right\}.\]
\subsubsection{Of a measurable function}
Let $f$ be a function on some measurable subset $B$ of $E$ into $\mathbb{R}\cup\{-\infty,\infty\}$, called a measurable function. We first define
\[\begin{aligned}
f^{+}(x)&=
\begin{cases}
f(x),\quad&\text{if\ }f(x)>0\\
0,\quad &\text{otherwise}
\end{cases},\quad\tx{and}\\
f^{-}(x)&=
\begin{cases}
-f(x),\quad&\text{if\ }f(x)<0\\
0,\quad&\text{otherwise}
\end{cases}.
\end{aligned}\]
Note that both $f^+$ and $f^-$ are nonnegative and that
\[f=f^+-f^-,\quad |f|=f^++f^-.\]
Then we define the Lebesgue integral of $f$ to exist if
\[ \min \left(\int f^{+}\,\mathrm{d}\mu ,\int f^{-}\,\mathrm{d}\mu \right)<\infty.\]
In this case we define
\[ \int f\,\mathrm{d}\mu =\int f^{+}\,\mathrm{d}\mu -\int f^{-}\,\mathrm{d}\mu.\]
\sssc{Integrability}
Let $f$ be a function on some measurable subset $B$ of $E$ into $\mathbb{R}\cup\{-\infty,\infty\}$.

If
\[\int |f|\,\mathrm {d} \mu <\infty ,\]
we say that $f$ is Lebesgue integrable.
\sssc{$L^p$ spaces ($L^p$ 空間)}
PLACEHOLDER
\ssc{Bochner integral (博赫納積分)}
\sssc{Premise}
Let $(E,\Sigma ,\mu )$ be a measure space and $X$ be a Banach space with norm $\|\cdot\|_X$. Below, we will define the Bochner integral of measurable functions on $E$ to $X$.
\subsubsection{Of an indicator functions}
The integral of an indicator function $1_S$ of a measurable subset $S$ of $E$ is defined to be
\[\int 1_{S}\,\mathrm{d}{\mu} =\mu (S).\]
\sssc{Of a simple function}
PLACEHOLDER
\sssc{Of a measurable function}
PLACEHOLDER
\sssc{Bochner spaces (博赫納空間)}
Let $(E,\Sigma,\mu)$ be a measure space and $X$ be a topological space. 

The Bochner space $L^p(E;X)$ with $p\in\mathbb{N}\cup\{\infty\}$ is defined to be the quotient space of the space of all Bochner measurable functions $f(t)\colon E\to X$ such that the norm $\|f(t)\|_{L^p(E;X)}$ of it, defined with
\[\|f(t)\|_{L^p(E;X)}\coloneq\left(\int_E\|f(t)\|_X^{\phantom{X}p}\,\mathrm{d}\mu\right)^{1/p},\quad p\in\mathbb{N},\]
and
\[\|f(t)\|_{L^{\infty }(E;X)}\coloneq\operatorname{ess\,sup}_{t\in E}\|f(t)\|_{X},\]
is finite by the equivalence relation of equality almost everywhere.



\section{Multivariable (多變數 or 多變量 or 多元) Calculus}
\subsection{Operators}
\begin{itemize}
\item Dot product (點積) operator: $\cdot$
\item Cross product (叉積) operator: $\times$
\item Gradient (梯度) operator: $\nabla$
\item Jacobian matrix (雅可比矩陣): $\nabla$, or $\mathbf{J}(\mathbf{F})$, $J(\mathbf{F})$, $\mb{J}_{\mb{F}}$, or $J_{\mathbf{F}}$ for operand $\mb{F}$.
\item Divergence (散度) operator: $\nabla \cdot$
\item Curl (旋度) operator: $\nabla \times$
\item Directional derivative (方向導數) operator: $\cdot\nabla$
\item Hessian (黑塞) (matrix): $\nabla\nabla$, $\nabla^2$, $\nabla\otimes\nabla$, $D^2$, or $\mb{H}(f)$, $H(f)$, $\mb{H}_f$ or $H_f$ for operand $f$, or simply as second derivative.
\item Laplace operator (拉普拉斯算子): $\nabla\cdot\nabla$, $\nabla^2$, or $\Delta$. For clarity, we use $\nabla\cdot\nabla$ here.
\item Line or Path integral operator: $\int$
\item Surface integral operator: $\iint$
\item Volume integration operator: $\iiint$
\item Closed line integral operator: $\oint$
\item Closed surface Integral Operator: $\oiint$
\item $\int\mathbf{F}\cdot\mathrm{d}\mathbf{S}$ is used as a shorthand for $\int(\mathbf{F}\cdot\mathbf{\hat{n}})\,\mathrm{d}S$, where $\hat{n}$ is the outward pointing unit normal at almost each point on $S$.
\end{itemize}
\subsection{Convention}
If not otherwise specified:
\begin{itemize}
\item The domain of the funcitons or maps below are subsets of a Euclidean vector space. If not otherwise specified, the coordinates are the Cartesian coordinates, the norms are the Euclidean norms, and the measures are the Lebesgue measures.
\item $\mathbf{0}$ or $0$ refers to the zero tensor (零張量) in the interested Euclidean tensor space $V$, that is, it satisfies 
\[\forall\mathbf{v}\in V:\,\mathbf{v}+\mathbf{0}=\mathbf{v}.\]
\item Unit vector (單位向量): $\mathbf{e}_i$ is the unit vector in the $i$th direction, i.e., a vector with zero norm.
\item Independent variable vector: $\mathbf{x}=(x_1,x_2,\dots,x_n)$
\item Vector fields: $\mathbf{F}(\mathbf{x}) = \sum_{i=1}^n F_i(\mathbf{x}) \mathbf{e}_i$、$\mathbf{G}$
\item Scalar fields: $A(\mathbf{x})$、$B(\mathbf{x})$
\item Tensor fields: $f(\mathbf{x})$、$g(\mathbf{x})$
\item Three-dimensional tensor space field: $\mathbf{T}(\mathbf{x})$
\item The $i$th component of the map $f$: $f_i$
\end{itemize}
\ssc{First order}
\sssc{Gradient (梯度) or first derivative, and Jacobian matrix}
\[
\nabla f = \begin{pmatrix}\qty(\pdv{f}{x_1})^T & \qty(\pdv{f}{x_2})^T & \dots  & \qty(\pdv{f}{x_n})^T\end{pmatrix}
\]
The gradient of a scalar field is a vector field, the gradient of a vector field is a second-order tensor (matrix) field, and the gradient of a $k$-order tensor field is a $k+1$-order tensor field. 

In particular, the gradient of a scalar field $A$ is
\[
\nabla A = \sum_{i=1}^n \pdv{A}{x_i}e_i.
\]
And the gradient of a vector field $\mathbf{F}$, also called the Jacobian matrix (雅可比矩陣) of $\mb{F}$, is a matrix such that the $( i,j )$th entry is
\[(J(\mb{F}))_{ij}=\frac{\partial F_i}{\partial x_j}.\]
The determinant of Jacobian matrix is called Jacobian determinant, or Jacobian for short.\sssc{Divergence (散度)}
\[
\nabla \cdot f = \sum_{i=1}^n\frac{\partial f_i}{\partial x_i}
\]
The divergence of a vector field is a scalar field, the gradient of a second-order tensor (i.e. matrix) field is a vector field, and the divergence of a $k+1$-order tensor field is a $k$-order tensor field.
\sssc{Curl (旋度)}
The curl is only defined on three-dimensional vector field.
\[
\nabla \times \mathbf{T} = 
\begin{pmatrix}
\mathbf{e}_1 & \mathbf{e}_2 & \mathbf{e}_3 \\
\frac{\partial}{\partial x_1} & \frac{\partial}{\partial x_2} & \frac{\partial}{\partial x_3} \\
T_1 & T_2 & T_3 \\
\end{pmatrix}
\]
The curl of a three-dimensional vector field is a three-dimensional vector field.
\sssc{Directional derivative (方向導數)}
\[(\mathbf{f}\cdot\nabla)\mathbf{g}=\sum_{i=1}^n f_i\pdv{g}{x_i}\]
\ssc{Higher order}
\sssc{Hessian (黑塞) (matrix) or second derivative}
The Hessian or second derivative $H_f$ of field $f$ is defined as a matrix such that
\[(H(f))_{ij}=\frac{\partial f}{\partial x_i\partial x_j},\]
that is,
\[H(f)=\nabla\qty((\nabla f)^\top).\]
\sssc{Laplace operator (拉普拉斯算子)}
\[\nabla\cdot\nabla f = \nabla \cdot (\nabla f) = \tr(H(f))=\sum_{i=1}^n\frac{\partial^2 f}{\partial x_i^{\phantom{i}2}}\]
The Laplace operator applied to a tensor field is a tensor field of the same order and same dimension.
\sssc{Poisson's equation (卜瓦松 or 帕松 or 泊松方程)}
\[
\nabla\cdot\nabla A = B(\mathbf{x})
\]
\sssc{Laplace's equation (拉普拉斯方程)}
\[
\nabla\cdot\nabla A = 0
\]
A real function $A$ with real independent variables that is second-order differentiable for all independent variables is called a harmonic function if $A$ satisfies Laplace's equation.
\sssc{Multi-index notation (多重指標記號)}
Suppose there are \( n \) variables \( x_1, x_2, \dots, x_n \), then a multi-index $\alpha$ is a vector of \( n \) nonnegative integers: 
\[
\alpha = (\alpha_1, \alpha_2,\ldots,, \alpha_n), \quad \text{where\ } \alpha_i \in \mathbb{N}_0.
\]
Define: 
\begin{itemize}
\item Norm \( \|\alpha\| \): 
\[
\|\alpha\| = \alpha_1 + \alpha_2 + \ldots\alpha_n.
\]
\item Factorial \( \alpha! \): 
\[
\alpha! = \alpha_1! \cdot \alpha_2! \cdot \ldots \alpha_n!.
\]
\item Power \( \mathbf{x}^\alpha \): 
If \( \mathbf{x} = (x_1, x_2,\ldots, x_n) \), then
\[
\mathbf{x}^\alpha = x_1^{\alpha_1} \cdot x_2^{\alpha_2} \ldots  x_n^{\alpha_n}.
\]
\item Higher-order mixed partial derivatives $D^\alpha f$: 
\[
D^\alpha f = \frac{\partial^{\|\alpha\|} f}{\partial x_1^{\alpha_1} \partial x_2^{\alpha_2} \ldots  \partial x_n^{\alpha_n}}.
\]
\end{itemize}
\sssc{Higher-order derivative}
The $k$th order derivative of $\mathbf{F}:\,\mathbb{R}^n\to\mathbb{R}^m$, denoted as $\mathbf{F}^{(k)}(\mathbf{x})$ or $D^{k}\mathbf{F}(\mathbf{x})$, is a $(\mathbb{R}^n)^k\to\mathbb{R}$ function, where $(\mathbb{R}^n)^k$ is a Cartesian product of $k$ copies of $\mathbb{R}^n$ vector, that is,
\[D^{k}\mathbf{F}(\mathbf{x})=\sum_{\|\alpha\|=k} \left(D^\alpha \mathbf{F}(\mathbf{a})\right).\]
In particular, the first-order derivative of $\mathbf{F}$ is the gradient of it.
\sssc{Taylor expansion}
Assume that $\mathbf{F}:\,\mathbb{R}^n\to\mathbb{R}^m$ is an infinitely differentiable function, and its partial derivatives of every order exist on $\mathbb{R}^n$, then the Taylor expansion of $\mathbf{F}$ at $\mathbf{a}$ is
\[\mathbf{F}(\mathbf{x}) = \sum_{\|\alpha\|\in\mathbb{N}_0} \frac{D^\alpha \mathbf{F}(\mathbf{a})}{\alpha!} (\mathbf{x} - \mathbf{a})^\alpha,\]
that is,
\[\mathbf{F}(\mathbf{x}) = \sum_{\|\alpha\|\leq k} \frac{D^\alpha \mathbf{F}(\mathbf{a})}{\alpha!} (\mathbf{x} - \mathbf{a})^\alpha+\int_0^1\frac{(1-t)^k}{k!} D^{k+1}\mathbf{F}(\mathbf{a} + t(\mathbf{x} - \mathbf{a})) (\mathbf{x} - \mathbf{a})^{k+1} \, \mathrm{d}t.\]
Also, the $k$th-order approximation of $\mathbf{F}$ near $\mathbf{a}$ is
\[\mathbf{F}(\mathbf{x}) \approx \sum_{\|\alpha\|\leq k} \frac{D^\alpha \mathbf{F}(\mathbf{a})}{\alpha!} (\mathbf{x} - \mathbf{a})^\alpha,\]
the first-order (aka linear or tangent line) approximation of $f$ near $\mathbf{a}$ is
\[\mathbf{F}(\mathbf{x}) \approx \mathbf{F}(\mathbf{a}) + \nabla \mathbf{F}(\mathbf{a}) \cdot (\mathbf{x} - \mathbf{a}),\]
and the second-order approximation of $f$ near $\mathbf{a}$ is
\[\mathbf{F}(\mathbf{x}) \approx \mathbf{F}(\mathbf{a}) + \nabla \mathbf{F}(\mathbf{a}) \cdot (\mathbf{x} - \mathbf{a}) + \frac{1}{2}(\mathbf{x} - \mathbf{a})^\top H(\mb{F})(\mb{a})(\mathbf{x} - \mathbf{a}).\]
The linearization of $f$ near $\mathbf{a}$ is
\[\mathbf{L}(\mathbf{x})= \mathbf{F}(\mathbf{a}) + \nabla \mathbf{F}(\mathbf{a}) \cdot (\mathbf{x} - \mathbf{a}).\]
\sssc{Definiteness (正定性)}
\bit
\item Positive definite: We say the Hessian $H(\mb{F})$ of $\mb{F}$ at $\mb{a}$ is positive definite if the second derivative term in Taylor series is positive for all inifinisimal changes $\delta\mb{x}$ near $\mb{a}$, that is,
\[(\delta\mb{x})^\top H(\mb{F})(\mb{a})\delta\mb{x}>0,\quad\forall \delta\mb{x}.\]
\item Negative definite: We say the Hessian $H(\mb{F})$ of $\mb{F}$ at $\mb{a}$ is positive definite if the second derivative term in Taylor series is negative for all inifinisimal changes $\delta\mb{x}$ near $\mb{a}$, that is,
\[(\delta\mb{x})^\top H(\mb{F})(\mb{a})\delta\mb{x}<0,\quad\forall \delta\mb{x}.\]
\item Neutral semidefinite: We say the Hessian $H(\mb{F})$ of $\mb{F}$ at $\mb{a}$ is zero semidefinite if the second derivative term in Taylor series is zero for all inifinisimal changes $\delta\mb{x}$ near $\mb{a}$, that is,
\[(\delta\mb{x})^\top H(\mb{F})(\mb{a})\delta\mb{x}=0,\quad\forall \delta\mb{x}.\]
\item Nonnegative semidefinite: We say the Hessian $H(\mb{F})$ of $\mb{F}$ at $\mb{a}$ is nonnegative semidefinite if the second derivative term in Taylor series is nonnegative for all inifinisimal changes $\delta\mb{x}$ near $\mb{a}$ but not all positive, that is,
\[(\delta\mb{x})^\top H(\mb{F})(\mb{a})\delta\mb{x}\geq 0,\quad\forall \delta\mb{x},\quad\land\]
\[(\delta\mb{x})^\top H(\mb{F})(\mb{a})\delta\mb{x}=0,\quad\exists \delta\mb{x}.\]
\item Nonpositive semidefinite: We say the Hessian $H(\mb{F})$ of $\mb{F}$ at $\mb{a}$ is nonpositive semidefinite if the second derivative term in Taylor series is nonpositive for all inifinisimal changes $\delta\mb{x}$ near $\mb{a}$ but not all negative, that is,
\[(\delta\mb{x})^\top H(\mb{F})(\mb{a})\delta\mb{x}\leq 0,\quad\forall \delta\mb{x},\quad\land\]
\[(\delta\mb{x})^\top H(\mb{F})(\mb{a})\delta\mb{x}=0,\quad\exists \delta\mb{x}.\]
\item Indefinite: We say the Hessian $H(\mb{F})$ of $\mb{F}$ at $\mb{a}$ is indefinite if it is neither definite nor semidefinite.
\eit
\ssc{Line integral (線積分) or Path integral (路徑積分)}
\sssc{Scalar field line or path integral}
For a scalar field $A : \,U\subseteq \mathbb {R} ^{n}\to \mathbb {R}$ and the path $C \in U$, the line integral of $A$ is: 
\[\int _{C}A\,\mathrm {d} s=\int _{a}^{b}A(\mathbf{r}(t))\|\dv{t}\mathbf {r} (t)\|\,\mathrm {d} t,\]
where $\mathbf{r}:\, [a, b] \to C$ is a one-to-one parametric function with $\mathbf{r}(a)$ and $\mathbf{r}(b)$ being the two endpoints of the path $C$. 

$A$ is called the integral function, $C$ is called the integral path, and the result of the line integration does not depend on the parametric function $r$.
\sssc{Vector field line or path integral}
willie169-ckhs-ntuee-notesFor a scalar field $\mathbf{F}: \,U\subseteq \mathbb {R} ^{n}\to \mathbb {R}^n$ and the path $C \in U$, the line integral of $\mathbf{F}$ is: 
\[\int _{C}\mathbf {F} (\mathbf {r} )\cdot \,\mathrm {d} \mathbf{r}=\int _{a}^{b}\mathbf {F} (\mathbf {r} (t))\cdot \dv{t}\mathbf {r} (t)\,\mathrm {d} t\]
where $\mathbf{r}:\, [a, b] \to C$ is a one-to-one parametric function with $\mathbf{r}(a)$ and $\mathbf{r}(b)$ being the two endpoints of the path $C$. 

$\mathbf{F}$ is called the integral function, $C$ is called the integral path, and the result of the line integration does not depend on the parametric function $\mathbf{r}$.
\sssc{Conservative field (保守場)}
A field $f$ whose domain is a subset $U$ of a Euclidean tensor space is called a conservative field if for all paths $C$ between point $a$ and $b$, the integral 
\[\int_Cf(\mathbf{x})\cdot\mathrm{d}\mathbf{x}\]
are the same.

This implies 
\begin{itemize}
\item For any closed path $C$, 
\[\int_Cf(\mathbf{x})\cdot\mathrm{d}\mathbf{x}=0.\]
\item If $\operatorname{dim}(U)=3$, then for any subset of $U$ where $f$ is smooth,
\[\nabla\times f=0.\]
\end{itemize}
\ssc{Fundamental theorem of multivariable calculus (多變數微積分基本定理)}
\sssc{Gradient theorem (梯度定理)}
Suppose $r$ is a oriented differentiable curve that starts at a point $\mathbf{p}$ and ends at a point $\mathbf{q}$. If $\mathbf{F}$ is a differentiable tensor field defined on a neighborhood of $\mathbf{F}$, then,
\[\int_r(\nabla\mathbf{F})\cdot\mathrm{d}\mathbf{r}=\mathbf{F}\left(\mathbf{q}\right)-\mathbf{F}\left(\mathbf{p}\right).\]
Gradient theorem is a special case of generalized Stokes theorem.
\sssc{Divergence theorem, Gauss's theorem, or Ostrogradsky's theorem (高斯散度定理)}
Suppose $V\subseteq\mathbb{R}^n$ is compact and has a piecewise smooth boundary $S$ (also indicated with $\partial V=S$). The closed, measurable set $\partial V$ is oriented by outward-pointing normals. If $F$ is a continuously differentiable vector field defined on a neighborhood of $V$, then,
\[\iiint_V\left(\nabla\cdot\mathbf {F}\right)\,\mathrm{d}V=\oiint_S\mathbf{F}\cdot\mathrm{d}\mathbf{S}\]
Divergence theorem is a special case of generalized Stokes theorem.
\sssc{Stokes' theorem (斯托克斯定理) or Kelvin–Stokes theorem}
Let $S$ be a positively oriented, piecewise smooth surface in $\mathbb{R}^3$ with boundary $\partial S\equiv L$. If a vector field $\mathbf{F}:\,\mathbb{R}^3\rightarrow\mathbb{R}^3$ is defined and has continuous first order partial derivatives in a region containing $S$, then,
\[\iint_S(\nabla\times\mathbf{F})\cdot \mathrm{d}\mathbf{S}=\oint_{L}\mathbf{F}\cdot\mathrm{d}\mathbf{L}\]
Stokes' theorem is a special case of generalized Stokes theorem.
\sssc{Green's theorem (格林定理或綠定理)}
Let $S$ be a positively oriented, piecewise smooth surface in $\mathbb{R}^2$ with boundary $\partial S\equiv L$. If scalar function $P,(x,y)\,Q(x,y)$ are defined and has continuous first order partial derivatives in a region containing $S$, then,
\[\oint_L (P\mathrm{d}x+Q\mathrm{d}y)=\iint_S\left(\frac{\partial Q}{\partial x}-\frac{\partial P}{\partial y}\right)\mathrm{d}x\mathrm{d}y\]
where the path of integration along C is counterclockwise.

Green's theorem is a special case of Stokes' theorem.
\sssc{Generalized Stokes theorem, Stokes–Cartan theorem, or fundamental theorem of multivariable calculus}
The generalized Stokes theorem says that the integral of a differential form $\omega$ over the boundary $\partial\Omega$ of some orientable manifold $\Omega$ is equal to the integral of its exterior derivative $\mathrm{d}\boldsymbol{\omega}$ over the whole of $\Omega$, i.e.,
\[\int _{\partial\Omega}\omega=\int_{\Omega}\mathrm{d}\boldsymbol{\omega}\]



\section{Differential theorems and methods}
\ssc{Theorems and methods of differentiation}
\sssc{Linearity, or sum rule, difference rule, and constant multiple rule}
If functions $f$ and $g$ are differentiable on $I$ and $a,b\in\mathbb{R}$, then $af(x)+bg(x)$ is also differentiable on $I$ and its derivative is given by
\[\dv{}{x}\qty(af(x)+bg(x))=af'(x)+bg'(x).\]
\begin{proof}
\[\ba
\dv{}{x}\qty(af(x)+bg(x))&=\lim_{h\to 0}\frac{af(x+h)+bg(x+h)-af(x)-bg(x)}{h}\\
&=af'(x)+bg'(x)
\ea\]
\end{proof}
\sssc{Product rule (乘法定則)}
If functions $f$ and $g$ are differentiable on $I$, then the product $fg$ is also differentiable on $I$ and its derivative is given by
\[\dv{x}\qty(f(x)g(x))=f'(x)g(x)+f(x)g'(x).\]
\begin{proof}
\[\begin{aligned}
\dv{f(x)g(x)}{x}&=\lim_{h\to 0}\frac{f(x+h)g(x+h)-f(x)g(x)}{h}\\
&=\lim_{h\to 0}\frac{f(x+h)(g(x+h)-g(x))+g(x)(f(x+h)-f(x))}{h}\\
&=f(x)g'(x)+f'(x)g(x).
\end{aligned}\]
\end{proof}
\sssc{General Leibniz rule}
If functions $f$ and $g$ are $n$-times differentiable on $I$, then the product $fg$ is also $n$-times differentiable on $I$ and its derivative is given by
\[(fg)^{(n)}=\sum _{k=0}^{n}{n \choose k}f^{(n-k)}g^{(k)}.\]
\sssc{Quotient rule (除法定則)}
If functions $f$ and $g$ are differentiable on $I$ and $g\neq 0$ on $I$, then the quotient $\frac{f}{g}$ is also differentiable on $I$ and its derivative is given by
\[\dv{x}\qty(\frac{f(x)}{g(x)})=\frac{f'(x)g(x)-f(x)g'(x)}{\qty(g(x))^2}.\]
\begin{proof}
\[\begin{aligned}
\dv{}{x}\frac{f(x)}{g(x)}&=\lim_{h\to 0}\frac{\frac{f(x+h)}{g(x+h)}-\frac{f(x)}{g(x)}}{h}\\
&=\lim_{h\to 0}\frac{f(x+h)g(x)-f(x)g(x+h)}{h\,g(x+h)g(x)}\\
&=\lim_{h\to 0}\frac{(f(x+h)-f(x))g(x)-f(x)(g(x+h)-g(x))}{h\,g(x+h)g(x)}\\
&=\lim_{h\to 0}\frac{f(x+h)-f(x)}{h\,g(x+h)}-f(x)\lim_{h\to 0}\frac{g(x+h)-g(x)}{h\,g(x+h)g(x)}\\
&=\frac{f'(x)g(x)-f(x)g'(x)}{g(x)^2}
\end{aligned}\]
\end{proof}
\sssc{Chain rule (連鎖律)}
If functions $f$ and $g$ are differentiable on $I$, then the composite $f\circ g$ is also differentiable on $I$ and its derivative is given by
\[\dv{}{x}\qty((f\circ g)(x))=f'\qty(g(x))g'(x).\]
\begin{proof}
\[\ba
\dv{}{x}\qty((f\circ g)(x))&=\lim_{h\to 0}\frac{f\qty(g(x+h))-f\qty(g(x))}{h}\\
&=\lim_{h\to 0}\frac{f\qty(g(x+h))-f\qty(g(x))}{g(x+h)-g(x)}\cdot\frac{g(x+h)-g(x)}{h}\\
&=f'\qty(g(x))g'(x)
\ea\]
\end{proof}
\sssc{Faà di Bruno's formula}
\bma
&\frac{\mathrm{d}^n}{\mathrm{d}x}f\left(g(x)\right)\\
=&\sum_{\sum_{i=1}^nim_i=n,\quad m_i\in\mathbb{N}_0}\frac{n!}{\prod_{j=1}^nm_j!}\cdot\\
&f^{\qty(\sum_{j=1}^nm_j)}\left(g(x)\right)\cdot\\
&\prod_{j=1}^n\left(\frac {g^{(j)}(x)}{j!}\right)^{m_j}.
\eam
\sssc{Exponential Bell polynomials}
Partially or incomplete exponential Bell polynomials $B_{n,k}\qty(x_1,x_2,\ldots, x_{n-k+1})$ is defined as
\[B_{n,k}\qty(x_1,x_2,\ldots, x_{n-k+1})=n!\sum_{\langle j_i\rangle\in C}\prod_{i=1}^{n-k+1}\frac{x_i^{\phantom{i}j_i}}{(i!)^{j_i}j_i!},\]
where
\[C=\left\{\langle j_i\rangle_{i=1}^{n-k+1}\middle|\sum_{i=1}^{n-k+1}=k\land\sum_{i=1}^{n-k+1}ij_i=n\right\}.\]
Complete exponential Bell polynomial $B_n\qty(x_1,x_2,\ldots, x_n)$ is defined as
\[B_n\qty(x_1,x_2,\ldots, x_n)=n!\sum_{k=0}^nB_{n,k}\qty(x_1,x_2,\ldots, x_{n-k+1}).\]
\sssc{Derivative of inverse functions}
\[\qty(f^{-1})'(x)=\frac{1}{f'\qty(f^{-1}(x)).\]
\begin{proof}
\[f\qty(f^{-1}(x))=x.\]
Differentiate both sides:
\[f'\qty(f^{-1}(x))\qty(f^{-1})'(x)=1.\]
\end{proof}
\[\qty(f^{-1})''(x)=\frac{f''\qty(f^{-1}(x))}{\qty(f'\qty(f^{-1}(x)))^3}.\]
\begin{proof}
\[f'\qty(f^{-1}(x))\qty(f^{-1})'(x)=1.\]
Differentiate both sides:
\[f''\qty(f^{-1}(x))\qty(\qty(f^{-1})'(x))^2+f'\qty(f^{-1}(x))\qty(f^{-1})''(x)=0.\]
\[\qty(f^{-1})''(x)=\frac{f''\qty(f^{-1}(x))\qty(\qty(f^{-1})'(x))^2}{f'\qty(f^{-1}(x))}=\frac{f''\qty(f^{-1}(x))}{\qty(f'\qty(f^{-1}(x)))^3}.\]
\end{proof}
\[\dv[n]{f^{-1}(x)}{x}=\frac{(-1)^{n-1}}{\qty(f'\qty(f^{-1}(x)))^{2n-1}}B_{n-1}\sum_{i=2}^{n}f^{(i)}\qty(f^{-1}(x)),\quad n\in\bbN,\]
where $B_n$ is complete exponential Bell polynomial.
\sssc{Implicit differentiation (隱函數微分)}
If $x$ and $y$ are related by an equation $F(x,y)=0$, then we can differentiate both sides with respect to $x$, treating $y$ as a function of $x$.

Whenever we differentiate a term involving $y$, we apply the chain rule, which introduces a factor of $\dv{y}{x}$.

We can compute derivatives of $y$ with respect to $x$ without solving for $y$ by implicit differentiation.
\sssc{Derivative of natural logarithm of absolute value of function}
If functions $f$ is differentiable and nonzero on $I$, then $\ln|f(x)|$ is also differentiable on $I$ and its derivative is given by
\[\dv{\ln|f(x)|}{x}=\frac{f'(x)}{f(x)}.\]
\sssc{Logarithmic differentiation}
Logarithmic differentiation is a method for finding derivatives by taking the natural logarithm of (the absolute values of) both sides of an equation first, and then differentiating.

It’s especially useful when the function involves products or quotients of many factors, or variables in both the base and the exponent.
\sssc{Derivative of variable base, constant exponent funcion}
If $n\in\bbR$ and $f(x)$ is differentiable on $I$, then $\qty(f(x))^n$ is also differentiable on $I$ and its derivative is given by
\[\dv{}{x}\qty(f(x))^n=n\qty(f(x))^{n-1}f'(x).\]
\sssc{Derivative of constant base, variable exponent funcion}
If $n\in\bbR$ and $f(x)$ is differentiable on $I$, then $n^{f(x)}$ is also differentiable on $I$ and its derivative is given by
\[\dv{}{x}n^{f(x)}=n^{f(x)}\qty(\ln n)f'(x).\]
\sssc{Derivative of variable base, variable exponent funcion}
\[\dv{}{x}\qty(f(x))^{g(x)}=\qty(f(x))^{g(x)}\qty(g'(x)\ln\qty(f(x))+g(x)\frac{f'(x)}{f(x)}).\]
\begin{proof}
\[y\coloneq\qty(f(x))^{g(x)}\]
Take natural logarithm of both sides:
\[\ln y=g(x)\ln\qty(f(x)).\]
Take differentiation of both sides:
\[\frac{y'}{y}=g'(x)\ln\qty(f(x))+g(x)\frac{f'(x)}{f(x)}.\]
\[y'=\qty(f(x))^{g(x)}\qty(g'(x)\ln\qty(f(x))+g(x)\frac{f'(x)}{f(x)}).\]
\end{proof}
\ssc{Graph-related}
\sssc{Differentiability implies continuity}
Let $V$ and $W$ be normed vector spaces, $U$ be an open subset of $V$, and $f\colon U\subseteq V\to W$ be a function. For a point $a\in U$, $f$ is differentiable at $a$ implies it is continuous at $a$; for an open subset $I\subseteq U$, $f$ is differentiable on $I$ implies it is continuous on $I$.
\begin{proof}
$f$ is differentiable at $a$ iff there exists bounded linear operator $A\colon V\to W$ such that
\[\lim_{\|h\|_V\to 0}\frac{\|f(a+h)-f(a)-A(h)\|_W}{\|h\|_V}=0,\]
that is,
\[\forall\varepsilon>0\colon\exists\delta>0\text{\ s.t.\ }\|h\|_V<\delta\implies\|f(a+h)-f(a)-A(h)\|_W\leq\varepsion\|h\|_V.\]
By the triangular identity,
\[\|f(a+h)-f(a)\|_W\leq\|A(h)\|_W+\|f(a+h)-f(a)-A(h)\|_W.\]
By the definition of bounded linear operator, there exists $C\in\bbR_{>0}$ such that
\[\|A(h)\|_W\leq C\|h\|_V.\]
\[\|f(a+h)-f(a)\|_W\leq (C+\varepsilon)\|h\|_V.\]
Thus,
\[\forall\varepsilon>0\colon\exists\delta'=\frac{\varepsilon}{C+\varepsilon}\text{\ s.t.\ }\|h\|_V<\delta'\implies\|f(a+h)-f(a)\|_W\leq(C+\varepsilon)\|h\|_V<(C+\varepsilon)\delta'=\varepsilon.\]
\end{proof}
\sssc{Increasing (遞增) and Decreasing (遞減)}
Let $f\colon X\subseteq\bbR\to\bbR$ be a function.
\bit
\item $f$ is called strictly increasing on $I\subseteq X$ if
\[\forall a,b\in I\colon a<b\implies f(a)<f(b).\]
\item $f$ is called strictly increasing if
\[\forall a,b\in X\colon a<b\implies f(a)<f(b).\]
\item $f$ is called strictly decreasing on $I\subseteq X$ if
\[\forall a,b\in I\colon a<b\implies f(a)>f(b).\]
\item $f$ is called strictly decreasing if
\[\forall a,b\in X\colon a<b\implies f(a)>f(b).\]
\item $f$ is called non-decreasing or monotone (單調) increasing on $I\subseteq X$ if
\[\forall a,b\in I\colon a<b\implies f(a)\leq f(b).\]
\item $f$ is called non-decreasing or monotone increasing if
\[\forall a,b\in X\colon a<b\implies f(a)\leq f(b).\]
\item $f$ is called non-increasing or monotone decreasing on $I\subseteq X$ if
\[\forall a,b\in I\colon a<b\implies f(a)\geq f(b).\]
\item $f$ is called non-increasing or monotone decreasing if
\[\forall a,b\in X\colon a<b\implies f(a)\geq f(b).\]
\item $f$ is called monotone on $I\subseteq X$ if it is either monotone increasing or monotone decreasing on $I$.
\item $f$ is called monotone if it is either monotone increasing or monotone decreasing.
\eit
The terms without monotone and strictly are define to be either monotone or strictly in different contexts; here we define them to be strictly.
\sssc{Critical point (臨界點)}
Let $f$ be a function with codomain $\bbR$ and \( c \) be a point in the domain of $f$, if \( f'(c) = 0 \) or \( f' \) does not exist at \( c \), then \( c \) is a critical point, of \( f \), and $f(c)$ is called a critical value of $f$.

If the domain of $f$ is a subset of $\bbR$ or $\bbC$, a critical point is also called a critical number.
\sssc{Relative extremum (相對極值), local extremum (局部極值), or extremum (極值)}
Let $f\colon D\subseteq X\to\bbR$ be a function with $X$ being a topological space. It is said that a relative maximum (相對極大值) or maximum (極大值) \( f(c) \) of \(f\) occurs at $c\in D$ if there exists an open subset $I\ni c$ of $X$ such that \( \forall x\in I\cap D\colon f(c) \geq f(x) \).

Let $f\colon D\subseteq X\to\bbR$ be a function with $X$ being a topological space. It is said that a relative minimum (相對極小值) or minimum (極小值) \( f(c) \) of \(f\) occurs at $c\in D$ if there exists an open subset $I\ni c$ of $X$ such that \( \forall x\in I\cap D\colon f(c) \leq f(x) \).

Relative maximum and relative minimum are collectively called relative extreme.

Let $f\colon D\subseteq X\to\bbR$ be a function with $X$ being a topological space. It is said that a strict relative maximum or strict maximum \( f(c) \) of \(f\) occurs at $c\in D$ if there exists an open subset $I\ni c$ of $X$ such that \( \forall x\in I\cap D\colon f(c) > f(x) \).

Let $f\colon D\subseteq X\to\bbR$ be a function with $X$ being a topological space. It is said that a strict relative minimum or strict minimum \( f(c) \) of \(f\) occurs at $c\in D$ if there exists an open subset $I\ni c$ of $X$ such that \( \forall x\in I\cap D\colon f(c) < f(x) \).

Strict relative maximum and strict relative minimum are collectively called strict relative extreme.
\sssc{Absolute extremum (絕對極值 or 最值) or global extremum (全域極值)}
Let $f\colon D\to\bbR$ be a function. It is said that a absolute maximum (絕對極大值 or 最大值) \( f(c) \) of \(f\) occurs at $c\in D$ if \( \forall x\in D\colon f(c) \geq f(x) \).

Let $f\colon D\to\bbR$ be a function. It is said that a absolute minimum (絕對極小值 or 最小值) \( f(c) \) of \(f\) occurs at $c\in D$ if \( \forall x\in D\colon f(c) \leq f(x) \).

Absolute maximum and absolute minimum are collectively called absolute extreme.
\sssc{Saddle point (鞍點)}
Let $f$ be a function with codomain $\bbR$ and \( c \) be a point in the domain of $f$ , if \( f'(c) = 0 \) and $c$ is not a local extremum of $f$, then $c$ is a saddle point of $f$.
\sssc{Extreme Value Theorem (EVT) (極值定理)}
Let function $f\colon I\subseteq\mathbb{R}\to\mathbb{R}$ be continuous on an interval $[a,b]\subseteq I$, then
\[\exists c,d \in [a, b] \text{\ s.t.\ }\forall x \in [a, b]\colon f(c)\geq f(x)\geq f(d).\]
\begin{proof}
We first claim that $f$ is bounded on $[a,b]$. Argue by contradiction. Suppose $f$ is not bounded-above.

For any $n\in\mathbb{N}$, there exists $x_n\in [a,b]$ such that $f\qty(x_n)>n$. This defines a sequence $\langle x_n\rangle$.

By the Bolzano-Weierstrass Theorem and that $[a,b]$ is bounded, it has a convergent subsequence $x_{n_k}$ that converges to some limit $L$. Since $[a,b]$ is closed, $L\in [a,b]$.

Since $f$ is continuous on $[a,b]$, the sequence $f\qty(x_{n_k})$ converges to $f(L)$, which is finite.

This contradicts $f\qty(x_n)>n$ for every $n\in\mathbb{N}$. So the set
\[S=\{f(x)\mid x\in [a,b]\}.\]
must be bounded-above.

By considering $-f$, we obtain that $S$ is also bounded-below.

By the Completeness of the Real Numbers, every non-empty set bounded-above has a least upper bound; every non-empty set bounded-below has a greatest lower bound.

Let
\[M=\sup S,\quad m=\inf S.\]

Since $M$ is the least upper bound of $S$, for any $n\in\mathbb{N}$, $M-\frac{1}{n}$ is not an upper bound of $S$. This means that for each $n\in\mathbb{N}$, there exists a point $y_n\in [a,b]$ such that:
\[M-\frac{1}{n}<f(y_n)\leq M.\]
This defines another sequence $\langle y_n\rangle$ within $[a, b]$.

By the Bolzano-Weierstrass Theorem and that $[a,b]$ is bounded, it has a convergent subsequence $y_{n_k}$ that converges to some limit $c$. Since $[a,b]$ is closed, $c\in [a,b]$.
\[\lim_{n_k\to \infty}\qty(M - \frac{1}{n_k}) = M.\]
By the Squeeze Theorem:
\[\lim_{n_k \to \infty} f\qty(y_{n_k}) = M.\]
Since $f$ is continuous on $[a,b]$, the sequence $f\qty(y_{n_k})$ converges to $f(c)$.

Similarly, we can prove that there exists $d\in [a,b]$ such that $f(d)=m$.
\end{proof}
\sssc{Extreme Value Theorem (EVT) for compact set}
Let $X$ be a topological space, and function $f\colon I\subset X\to\mathbb{R}$ be continuous on a compact set $D\subseteq I$, then
\[\exists a,b \in D \text{\ s.t.\ }\forall x \in D\colon f(a)\geq f(x)\geq f(b).\]
\sssc{Fermat's Theorem (費馬引理) or Interior Extremum Theorem (內部極值定理)}
Let function $f\colon D\subseteq\mathbb{R}\to\mathbb{R}$ be differentiable on an open interval $I\subseteq D$ and $c\in I$ be a local extreme of $f$, then
\[f'(c)=0.\]
\begin{proof}
Without loss of generality, suppose $c$ is a local extreme. Then there exists $\delta>0$ such that for all $x\in (c-\delta,c+\delta)\colon f(x)\leq f(c)$.

Thus,
\[\lim_{h\to 0^+}\frac{f(x+h)-f(x)}{h}\leq 0\]
and
\[\lim_{h\to 0^-}\frac{f(x+h)-f(x)}{h}\geq 0.\]

Since $f$ is differentiable at $c$,
\[\lim_{h\to 0^+}\frac{f(x+h)-f(x)}{h}=\lim_{h\to 0^-}\frac{f(x+h)-f(x)}{h}=0=f'(c).\]
\end{proof}
In terms of critical point, Fermat's Theorem can be rephrased as, if function $f\colon D\subseteq\mathbb{R}\to\mathbb{R}$ has a local extreme $c$ of $f$, then $c$ is a critical point of $f$.
\sssc{Rolle's Theorem (羅爾定理)}
Let function $f\colon I\subseteq\mathbb{R}\to\mathbb{R}$ be continuous on an interval $[a, b]\subseteq I$ and differentiable on $(a,b)$, and $f(a)=f(b)$, then
\[\exists c\in (a, b)\text{\ s.t.\ }f'(c)=0.\]
\begin{proof}
By the Extreme Value Theorem, $f$ must have maximum and minimum on $[a,b]$.

Let
\[M=\max_{x\in [a,b]}f(x),\quad m=\min_{x\in [a,b]}f(x).\]
If $M=m$, then $f$ is constant on $[a,b]$, then $f'(x)=0$ for any $c\in (a,b)$.

Otherwise, $M>m$. Because $f(a)=f(b)$. 

We claim that there must be $c\in (a,b)$ such that $M=f(c)$ or $m=f(c)$. Argue by contradiction. If the claim is false, then $M=m=f(a)=f(b)$, contradicting $M>m$.

By Fermat's Theorem,
\[f'(c)=0.\]
\end{proof}
\sssc{Mean Value Theorem (MVT) (均值定理) (for Derivatives)}
Let function $f\colon I\subseteq\mathbb{R}\to\mathbb{R}$ be continuous on an interval $[a, b]\subseteq I$ and differentiable on $(a,b)$ with $a<b$, then
\[\exists c\in (a, b)\text{\ s.t.\ }f'(c)=\frac{f(b)-f(a)}{b-a}.\]
\begin{proof}
Let
\[\phi(x) = f(x) - \frac{f(b)-f(a)}{b-a} (x-a).\]
\[\phi(a)=\phi(b)=f(a).\]
By Rolle's Theorem, there exists $c\in (a,b)$ such that $\phi'(c)=0$.
\[\phi'(c)=f'(c)-\frac{f(b)-f(a)}{b-a}.\]
\end{proof}
\ssc{Cauchy's Mean Value Theorem (CMVT) (柯西均值定理)}
Let functions $f\colon I\subseteq\mathbb{R}\to\mathbb{R}$ and $g\colon J\subseteq\mathbb{R}\to\mathbb{R}$ be continuous on an interval $[a, b]\subseteq I\cap J$ and differentiable on $(a,b)$ with $a<b$, then
\[\exists c\in (a,b)\text{\ s.t.\ }\qty(f(b)-f(a))g'(c)=\qty(g(b)-g(a))f'(c).\]
\begin{proof}
    If $f(b)-f(a)=g(b)-g(a)=0$, it holds. For the other cases, without loss of generality, assume $g(b)-g(a)\neq 0$.

    Define a function $\phi(x)$
    \[\phi(x)=f(x)-\frac{f(b)-f(a)}{g(b)-g(a)}g(x).\]
    \[\phi(b)-\phi(a)=\qty(f(b)-f(a))-\frac{f(b)-f(a)}{g(b)-g(a)}\qty(g(b)-g(a))=0.\]
    By Rolle's Theorem, there exists $c\in (a,b)$ such that $\phi'(c)=0$.
    \[\phi'(x)=f'(x)-\frac{f(b)-f(a)}{g(b)-g(a)}g'(x).\]
\end{proof}
\sssc{Generalized Racetrack principle}
If $f'(x)>g'(x)$ for all $x\in (a,b)$ with $b\in\bbR_{>a}\cup\{\infty\}$, and $f(a)=g(a)$, then $f(x)>g(x)$ for all $x\in(a,b)$.

If $f'(x)>g'(x)$ for all $x\in (a,b)$ with $b\in\bbR_{>a}\cup\{\infty\}$, $f$ is continuous on $(a,b]$, and $f(a)=g(a)$, then $f(x)>g(x)$ for all $x\in(a,b]$.

If $f'(x)\geq g'(x)$ for all $x\in (a,b)$ with $b\in\bbR_{>a}\cup\{\infty\}$, and $f(a)=g(a)$, then $f(x)\geq g(x)$ for all $x\in(a,b]$.

If $f'(x)\geq g'(x)$ for all $x\in (a,b)$ with $b\in\bbR_{>a}\cup\{\infty\}$, $f$ is continuous on $(a,b]$, and $f(a)=g(a)$, then $f(x)\geq g(x)$ for all $x\in(a,b]$.
\begin{proof}
    For the first and second statements, define a function
    \[h(x)=f(x)-g(x).\]
    \[h'(x)=f'(x)-g'(x)>0,\quad\forall x\in(a,b).\]
    \[h(a)=f(a)-g(a)=0.\]
    By MVT, for all $x\in(a,b)$ for the first statement and $x\in(a,b]$ for the second statement, there exists $c\in(a,x)$ such that
    \[\frac{h(x)-h(a)}{x-a}=h'(c)>0.\]
    \[f(x)-g(x)=h(x)=(x-a)h'(c)>0.\]
    The other statements can be proven similarly.
\end{proof}
\sssc{Increasing/Decreasing Test (I/D Test) Theorem}
Let function $f\colon I\subseteq\bbR\to\bbR$ be differentiable on an open interval $(a,b)\subseteq I$.
\bit
\item if $f'(x)>0$ for any $x\in (a,b)$, then $f$ is strictly increasing on $(a,b)$.
\item If $f'(x)>0$ for any $x\in (a,b)$ and $f$ is continuous on $[a,b)$, then $f$ is strictly increasing on $[a,b)$.
\item If $f'(x)>0$ for any $x\in (a,b)$ and $f$ is continuous on $(a,b]$, then $f$ is strictly increasing on $(a,b]$.
\item if $f'(x)<0$ for any $x\in (a,b)$, then $f$ is strictly decreasing on $(a,b)$.
\item If $f'(x)<0$ for any $x\in (a,b)$ and $f$ is continuous on $[a,b)$, then $f$ is strictly decreasing on $[a,b)$.
\item If $f'(x)<0$ for any $x\in (a,b)$ and $f$ is continuous on $(a,b]$, then $f$ is strictly decreasing on $(a,b]$.
\item $f'(x)\geq 0$ for any $x\in (a,b)$ iff $f$ is non-decreasing on $(a,b)$.
\item If $f'(x)\geq 0$ for any $x\in (a,b)$ and $f$ is continuous on $[a,b)$, then $f$ is non-decreasing on $[a,b)$.
\item If $f'(x)\geq 0$ for any $x\in (a,b)$ and $f$ is continuous on $(a,b]$, then $f$ is non-decreasing on $(a,b]$.
\item $f'(x)\leq 0$ for any $x\in (a,b)$ iff $f$ is non-increasing on $(a,b)$.
\item If $f'(x)\leq 0$ for any $x\in (a,b)$ and $f$ is continuous on $[a,b)$, then $f$ is non-increasing on $[a,b)$.
\item If $f'(x)\leq 0$ for any $x\in (a,b)$ and $f$ is continuous on $(a,b]$, then $f$ is non-increasing on $(a,b]$.
\eit
\begin{proof}
    For the first statement, we want to show that $\forall c,d\in (a,b)\colon c<d\implies f(c)<f(d)$ if $f'(x)>0$ for any $x\in (a,b)$.

    Take any two points $c,d\in (a,b)$ with $c<d$. By MVT, there exists $g\in (c,d)$ such that
    \[f'(g)=\frac{f(d)-f(c)}{d-c}.\]
    Because $f'(g)>0$ and $d-c>0$, we get $f(d)-f(c)>0$.
    
    The second and third statements are obviously true as the first holds.
    
    The "if" directions of the other statements can be proven similarly.
    
    For the "only if" direction of the seventh statement, we want to show that $f'(x)\geq 0$ if $\forall c,d\in (a,b)\colon c<d\implies f(c)\leq f(d)$ and $f$ is differentiable on $(a,b)$.
    
    Fix any $x\in (a,b)$. For any $h>0$ with $(x+h)\in(a,b)$, we have
    \[f(x)\leq f(x+h).\]
    For any $h<0$ with $(x+h)\in(a,b)$, we have
    \[f(x)\geq f(x+h).\]
    Thus for any $h$ with $(x+h)\in(a,b)$, we have
    \[\frac{x+h}-f(x)}{h}\geq 0.\]
    The "only if" direction of the tenth statement can be proven similarly.
\end{proof}
\sssc{First Derivative Test for Local Extrema Theorem}
Let $f\colon I\subseteq\mathbb{R}\to\mathbb{R}$ be a function, $c$ be a critical point of $f$, and $f'$ be continuous at $c$.
\bit
\item If exist $a<c<b$ such that $f'(c)>0$ for any $x\in (a,c)$ and $f'(c)<0$ for any $x\in (c,b)$, then $f$ has a strict local maximum at $c$.
\item If exist $a<c<b$ such that $f'(c)\geq 0$ for any $x\in (a,c)$ and $f'(c)\leq 0$ for any $x\in (c,b)$, then $f$ has a local maximum at $c$.
\item If exist $a<c<b$ such that $f'(c)<0$ for any $x\in (a,c)$ and $f'(c)>0$ for any $x\in (c,b)$, then $f$ has a strict local minimum at $c$.
\item If exist $a<c<b$ such that $f'(c)\leq 0$ for any $x\in (a,c)$ and $f'(c)\geq 0$ for any $x\in (c,b)$, then $f$ has a local minimum at $c$.
\item If exist $a<c<b$ such that $f'(c)>0$ for any $x\in (a,b)$, then $f$ has no local exteme at $c$.
\item If exist $a<c<b$ such that $f'(c)<0$ for any $x\in (a,b)$, then $f$ has no local ext
eme at $c$.
\eit
\begin{proof}
    By definitions and the Increasing/Decreasing Test Theorem, it is obvious.
\end{proof}
\sssc{Eigenvalues Test of Definiteness for Symmetric Hessians}
If a Hessian $H$ of a scalar field is symmetric, that is,
\[H=H^\top,\]
then:
\bit
\item $H$ is positive definte if $\lambda>0$ for all eigenvalues $\lambda$ of $H$.
\item $H$ is negative definte if $\lambda<0$ for all eigenvalues $\lambda$ of $H$.
\item $H$ is nonnegative semidefinte if $\lambda\geq 0$ for all eigenvalues $\lambda$ of $H$.
\item $H$ is nonpositive semidefinte if $\lambda\leq 0$ for all eigenvalues $\lambda$ of $H$.
\item $H$ is neutral semidefinte if $\lambda=0$ for all eigenvalues $\lambda$ of $H$.
\item $H$ is indefinte if none of above conditions is true.
\eit
\sssc{Second Derivative Test for Local Extrema Theorem}
Let $f\colon I\subseteq\mathbb{R}^n\to\mathbb{R}$ be a function, $c$ be a critical point of $f$, and $f$ be twice differentiable at $c$.
\bit
\item If $f''$ is negative definite, then $f$ has a strict local maximum at $c$.
\item If $f''$ is nonpositive definite, then $f$ has a local maximum at $c$.
\item If $f''$ is positive definite, then $f$ has a strict local minimum at $c$.
\item If $f''$ is nonnegative definite, then $f$ has a local minimum at $c$.
\item If $f''$ is indefinite, then $f$ has no local exteme at $c$.
\eit
\sssc{Concavity (凹性)}
Let $V$ be a vector space and function $f\colon J\subseteq V\to\bbR$ be continuous on a convex set $I\subseteq J$.
\bit
\item If for any $x,y\in I$ and $\alpha\in(0,1)$, $f((1-\alpha)x+\alpha y)<(1-\alpha)f(x)+\alpha f(y)$, often called "$f$ lies strictly below all of its secant lines on $I$", then $f$ is called to be strictly concave up/upward/upwards (CU), strictly convex, strictly convex up/upward/upwards, or strictly cup-shaped.
\item If for any $x,y\in I$ and $\alpha\in(0,1)$, $f((1-\alpha)x+\alpha y)>(1-\alpha)f(x)+\alpha f(y)$, often called "$f$ lies strictly above all its secant lines on $I$", then $f$ is called to be strictly concave, strictly concave down/downward/downwards (CD), strictly convex down/downward/downwards, or strictly cap-shaped.
\item If for any $x,y\in I$ and $\alpha\in(0,1)$, $f((1-\alpha)x+\alpha y)\leq (1-\alpha)f(x)+\alpha f(y)$, often called "$f$ lies weakly below all of its secant lines on $I$", then $f$ is called to be weakly concave up/upward/upwards (CU), weakly convex, weakly convex up/upward/upwards, or weakly cup-shaped.
\item If for any $x,y\in I$ and $\alpha\in(0,1)$, $f((1-\alpha)x+\alpha y)\geq (1-\alpha)f(x)+\alpha f(y)$, often called "$f$ lies weakly above all of its secant lines on $I$", then $f$ is called to be weakly concave, weakly concave down/downward/downwards (CD), weakly convex down/downward/downwards, or weakly cap-shaped.
\eit
The terms without weakly and strictly are define to be either weakly or strictly in different contexts; here we define them to be strictly.
\sssc{Point of inflection or inflection point (反曲點 or 拐點)}
Let $V$ be a vector space, $f\colon J\subseteq V\to\bbR$ be a function, and $I\subseteq J$ be a convex set. If $f$ is concave upward on $(a,b)$ and concave downward on $(b,c)$, or, concave downward on $(a,b)$ and concave upward on $(b,c)$, then $(b,f(b))$ is called an inflection point of $f$.
\sssc{First Derivative Test for Concavity Theorem}
Let $f\colon I\subseteq\bbR\to\bbR$ be a function and $(a,b)\subseteq I$ be an open interval.
\bit
\item If $f'$ is strictly increasing on $(a,b)$, often called "$f$ lies strictly below all its tagent lines on $(a,b)$", then $f$ is strictly concave upward on $(a,b)$.
\item If $f'$ is strictly increasing on $(a,b)$ and $f$ is continuous on $[a,b)$, then $f$ is strictly concave upward on $[a,b)$.
\item If $f'$ is strictly increasing on $(a,b)$ and $f$ is continuous on $(a,b]$, then $f$ is strictly concave upward on $(a,b]$.
\item If $f'$ is strictly decreasing on $I$, often called "$f$ lies strictly above all its tagent lines on $(a,b)$", then $f$ is strictly concave downward on $(a,b)$.
\item If $f'$ is strictly decreasing on $(a,b)$ and $f$ is continuous on $[a,b)$, then $f$ is strictly concave downward on $[a,b)$.
\item If $f'$ is strictly decreasing on $(a,b)$ and $f$ is continuous on $(a,b]$, then $f$ is strictly concave downward on $(a,b]$.
\item $f'$ is non-decreasing on $(a,b)$, often called "$f$ lies weakly below all its tagent lines on $(a,b)$", iff $f$ is weakly concave upward on $(a,b)$.
\item If $f'$ is non-decreasing on $(a,b)$ and $f$ is continuous on $[a,b)$, then $f$ is weakly concave upward on $[a,b)$.
\item If $f'$ is non-decreasing on $(a,b)$ and $f$ is continuous on $(a,b]$, then $f$ is weakly concave upward on $(a,b]$.
\item $f'$ is non-increasing on $(a,b)$, often called "$f$ lies weakly above all its tagent lines on $(a,b)$", iff $f$ is weakly concave downward on $(a,b)$.
\item If $f'$ is non-increasing on $(a,b)$ and $f$ is continuous on $[a,b)$, then $f$ is weakly concave downward on $[a,b)$.
\item If $f'$ is non-increasing on $(a,b)$ and $f$ is continuous on $(a,b]$, then $f$ is weakly concave downward on $(a,b]$.
\eit
\begin{proof}
    For the first statement, we want to show that for any $c,d\in (a,b)$ and $\alpha\in(0,1)$, $f((1-\alpha)c+\alpha d)<(1-\alpha)f(c)+\alpha f(d)$ if for any $a\leq A<B\leq b$, $f'(A)<f'(B)$.

    Fix any $a\leq c<x<d\leq b$. By MVT, there exists $A\in (c,x)$ such that
    \[f'(A)=\frac{f(x)-f(c)}{x-c}.\]
    By MVT, there exists $B\in (b,c)$ such that
    \[f'(B)=\frac{f(d)-f(x)}{d-x}.\]
    Since $A<B$, $f'(A)\leq f'(B)$.
    \[\frac{f(x)-f(c)}{x-c}\leq\frac{f(d)-f(x)}{d-x}.\]
    \[(f(x)-f(c))(d-x)\leq(f(d)-f(x))(x-c).\]
    \[(d-c)f(x)\leq(d-x)f(c)+(x-c)f(d).\]
    \[f(x)\leq\frac{(d-x)}{(d-c)}f(c)+\frac{(x-c)}{(d-c)}f(d).\]
    Set
    \[\alpha=\frac{(x-c)}{(d-c)}\in(0,1).\]
    Then
    \[x=(1-\alpha)c+\alpha d.\]
    Thus the inequality becomes
    \[f\qty((1-\alpha)c+\alpha d)\leq(1-\alpha)f(c)+\alpha f(d).\]
    
    For the second statement, argue by contradiction. Assume that there exists $d\in(a,b)$ and $\alpha\in(0,1)$ such that
    \[f((1-\alpha)a+\alpha d)\geq (1-\alpha)f(a)+\alpha f(d).\]
    Let $x=(1-\alpha)a+\alpha d$.
    \[f(x)-f(a)\geq\alpha(f(d)-f(a)).\]
    By MVT, there exists $B\in (a,x)$ such that
    \[f(x)-f(a)=f'(B)(x-a)=f'(B)\alpha (d-a).\]
    By MVT, there exists $c\in (x,d)$ such that
    \[f(d)-f(x)=f'(C)(d-x)=f'(C)(1-\alpha)(d-a).\]
    \[f(d)-f(a)=(f'(B)\alpha+f'(C)(1-\alpha))(d-a).\]
    \[f(x)-f(a)=f'(B)\alpha (d-a)\geq\alpha(f'(B)\alpha+f'(C)(1-\alpha))(d-a).\]
    \[f'(B)\geq f'(B)\alpha+f'(C)(1-\alpha).\]
    \[f'(B)\geq f'(C).\]
    This contradicts the condition that $B<C\implies f'(B)<f'(C)$ for any $a<B<C<b$.
    
    The third statement can be proven similarly.
    
    The "if" directions of the other statements can be proven similarly.
    
    For the "only if" direction of the seventh statement, we want to show that for any $c,d\in (a,b)$ and $\alpha\in(0,1)$, $f((1-\alpha)c+\alpha d)\leq (1-\alpha)f(c)+\alpha f(d)$ if $\forall c,d\in (a,b)\colon c<d\implies f'(c)\leq f'(d)$.
    
    Fix any $c,d\in(a,b)$ with $c<d$.
    
    By MVT, there exists $g\in(c,((1-\alpha)c+\alpha d))$ such that
    \[f((1-\alpha)c+\alpha d)-f(c)=f'(g)((1-\alpha)c+\alpha d-c)=f'(g)\alpha(d-c).\]
    By MVT, there exists $h\in(((1-\alpha)c+\alpha d)),d)$ such that
    \[f(d)-f((1-\alpha)c+\alpha d)=f'(h)(d-(1-\alpha)c-\alpha d)=f'(h)(1-\alpha)(d-c).\]
    Thus,
    \[f((1-\alpha)c+\alpha d)=f(c)+f'(g)\alpha(d-c)=f(d)-f'(h)(1-\alpha)(d-c).\]
    \[f'(g)\alpha+f'(h)(1-\alpha)=\frac{f(d)-f(c)}{d-c}.\]
    Because $f'(g)\leq f'(h)$, we have
    \[f'(g)(d-c)\leq f(d)-f(c).\]
    Substitute:
    \[f((1-\alpha)c+\alpha d)\leq f(c)+\alpha(f(d)-f(c))=(1-\alpha)f(c)+\alpha f(d).\]
    The "only if" direction of the tenth statement can be proven similarly.
\end{proof}
\sssc{Concavity Test Theorem or Second Derivative Test for Concavity Theorem}
Let function $f\colon I\subseteq\bbR\to\bbR$ be twice differentiable on an open interval $(a,b)\subseteq I$.
\bit
\item If $f''>0$ for any $x\in (a,b)$, then the graph of $f$ is strictly concave upward on $(a,b)$.
\item If $f''>0$ for any $x\in (a,b)$ and $f'$ is continuous on $[a,b)$, then the graph of $f$ is strictly concave upward on $[a,b)$.
\item If $f''>0$ for any $x\in (a,b)$ and $f'$ is continuous on $(a,b]$, then the graph of $f$ is strictly concave upward on $(a,b]$.
\item If $f''<0$ for any $x\in (a,b)$, then the graph of $f$ is strictly concave downward on $(a,b)$.
\item If $f''<0$ for any $x\in (a,b)$ and $f'$ is continuous on $[a,b)$, then the graph of $f$ is strictly concave downward on $[a,b)$.
\item If $f''<0$ for any $x\in (a,b)$ and $f'$ is continuous on $(a,b]$, then the graph of $f$ is strictly concave downward on $(a,b]$.
\item $f''\geq 0$ for any $x\in (a,b)$ iff the graph of $f$ is weakly concave upward on $(a,b)$.
\item If $f''\geq 0$ for any $x\in (a,b)$ and $f'$ is continuous on $[a,b)$, then the graph of $f$ is weakly concave upward on $[a,b)$.
\item If $f''\geq 0$ for any $x\in (a,b)$ and $f'$ is continuous on $(a,b]$, then the graph of $f$ is weakly concave upward on $(a,b]$.
\item $f''\leq 0$ for any $x\in (a,b)$ iff the graph of $f$ is weakly concave downward on $(a,b)$.
\item If $f''\leq 0$ for any $x\in (a,b)$ and $f'$ is continuous on $[a,b)$, then the graph of $f$ is weakly concave downward on $[a,b)$.
\item If $f''\leq 0$ for any $x\in (a,b)$ and $f'$ is continuous on $(a,b]$, then the graph of $f$ is weakly concave downward on $(a,b]$.
\eit
\begin{proof}
By Increasing/Decreasing Test Theorem and First Derivative Test for Concavity Theorem, it is obvious.
\end{proof}
\sssc{Inflection Point Theorem}
Let $f\colon I\subseteq\bbR\to\bbR$ be a function. If $(c,f(c))$ is an inflection point of $f$ and $f$ is differentiable at $c$, then $c$ is a critical point of $f'$.
\begin{proof}
By the First Derivative Test for Concavity Theorem, it is obvious.
\end{proof}
\sssc{Kink}
A kink is a point $x$ on the graph of a real function $f$ such that $f$ is continuous but not differentiable at $x$.
\sssc{Corner or sharp corner}
A corner or sharp corner is a point $x$ on the graph of a real function $f$ such that $f$ is continuous but not differentiable at $x$ and that the left-hand derivative and the right-hand derivative of $f$ at $x$ both exist and are finite.
\sssc{Vertical tangent}
A vertical tangent is a point $x$ on the graph of a real function $f$ such that $f$ is continuous at $x$ and that the left-hand derivative and the right-hand derivative of $f$ at $x$ are both $\infty$ or are both $-\infty$.
\sssc{Cusp}
A cusp is a point $x$ on the graph of a real function $f$ such that:
\bit
\item $f$ is continuous at $x$,
\item at least one of the left-hand derivative and the right-hand derivative of $f$ at $x$ are $\infty$ or $-\infty$,
\item $x$ is not a vertical tangent of $f$, and
\item the left-hand derivative and the right-hand derivative of $f$ at $x$ are both either finite, $\infty$, or $-\infty$.
\eit
\ssc{Indeterminate forms and L'Hôpital's rule (羅必達法則) or Bernoulli's rule}
\sssc{(General form of) L'Hôpital's rule or Bernoulli's rule and indeterminate quotients}
Let $I$ be an open interval and $a\in I$, and $f(x)$ and $g(x)$ be functions differentiable on $I\setminus\{a\}$. If
\bit
\item $\lim_{x\to a}f(x)=\lim_{x\to a}g(x)=0$, where $\lim_{x\to a}\frac{f(x)}{g(x)}$ is called indeterminate form of type $\frac{0}{0}$, or $\abs{\lim_{x\to a}f(x)}=\abs{\lim_{x\to a}g(x)}=\infty$, where $\lim_{x\to a}\frac{f(x)}{g(x)}$ is called indeterminate form of type $\frac{\infty}{\infty}$, and
\item $\lim_{x\to a}\frac{f'(x)}{g'(x)}\in\mathbb{R}\cup\{-\infty,\infty\}$,
\eit
then:
\[\lim_{x\to a}\frac{f(x)}{g(x)}=\lim_{x\to a}\frac{f'(x)}{g'(x)}.\]

Let $f(x)$ and $g(x)$ be functions differentiable on $(a,b)$ with $a<b$. If
\bit
\item $\lim_{x\to a^+}f(x)=\lim_{x\to a^+}g(x)=0$, where $\lim_{x\to a^+}\frac{f(x)}{g(x)}$ is called indeterminate form of type $\frac{0}{0}$, or $\abs{\lim_{x\to a^+}f(x)}=\abs{\lim_{x\to a^+}g(x)}=\infty$, where $\lim_{x\to a^+}\frac{f(x)}{g(x)}$ is called indeterminate form of type $\frac{\infty}{\infty}$, and
\item $\lim_{x\to a^+}\frac{f'(x)}{g'(x)}\in\mathbb{R}\cup\{-\infty,\infty\}$,
\eit
then:
\[\lim_{x\to a^+}\frac{f(x)}{g(x)}=\lim_{x\to a^+}\frac{f'(x)}{g'(x)}.\]

Let $f(x)$ and $g(x)$ be functions differentiable on $(b,a)$ with $a>b$. If
\bit
\item $\lim_{x\to a^-}f(x)=\lim_{x\to a^-}g(x)=0$, where $\lim_{x\to a^-}\frac{f(x)}{g(x)}$ is called indeterminate form of type $\frac{0}{0}$, or $\abs{\lim_{x\to a^-}f(x)}=\abs{\lim_{x\to a^-}g(x)}=\infty$, where $\lim_{x\to a^-}\frac{f(x)}{g(x)}$ is called indeterminate form of type $\frac{\infty}{\infty}$, and
\item $\lim_{x\to a^-}\frac{f'(x)}{g'(x)}\in\mathbb{R}\cup\{-\infty,\infty\}$,
\eit
then:
\[\lim_{x\to a^-}\frac{f(x)}{g(x)}=\lim_{x\to a^-}\frac{f'(x)}{g'(x)}.\]

Let $f(x)$ and $g(x)$ be functions differentiable on $(a,\infty)$. If
\bit
\item $\lim_{x\to\infty}f(x)=\lim_{x\to\infty}g(x)=0$, where $\lim_{x\to\infty}\frac{f(x)}{g(x)}$ is called indeterminate form of type $\frac{0}{0}$, or $\abs{\lim_{x\to\infty}f(x)}=\abs{\lim_{x\to\infty}g(x)}=\infty$, where $\lim_{x\to\infty}\frac{f(x)}{g(x)}$ is called indeterminate form of type $\frac{\infty}{\infty}$, and
\item $\lim_{x\to\infty}\frac{f'(x)}{g'(x)}\in\mathbb{R}\cup\{-\infty,\infty\}$,
\eit
then:
\[\lim_{x\to\infty}\frac{f(x)}{g(x)}=\lim_{x\to\infty}\frac{f'(x)}{g'(x)}.\]

Let $f(x)$ and $g(x)$ be functions differentiable on $(-\infty,a)$. If
\bit
\item $\lim_{x\to -\infty}f(x)=\lim_{x\to -\infty}g(x)=0$, where $\lim_{x\to -\infty}\frac{f(x)}{g(x)}$ is called indeterminate form of type $\frac{0}{0}$, or $\abs{\lim_{x\to -\infty}f(x)}=\abs{\lim_{x\to -\infty}g(x)}=\infty$, where $\lim_{x\to -\infty}\frac{f(x)}{g(x)}$ is called indeterminate form of type $\frac{\infty}{\infty}$, and
\item $\lim_{x\to -\infty}\frac{f'(x)}{g'(x)}\in\mathbb{R}\cup\{-\infty,\infty\}$,
\eit
then:
\[\lim_{x\to -\infty}\frac{f(x)}{g(x)}=\lim_{x\to -\infty}\frac{f'(x)}{g'(x)}.\]
\begin{proof}
Let $f(x)$ and $g(x)$ be functions differentiable on $(a,b)$ with $a<b$. We want to show that if $\lim_{x\to a^+}f(x)=\lim_{x\to a^+}g(x)=0$ and $\lim_{x\to a^+}\frac{f'(x)}{g'(x)}\in\mathbb{R}\cup\{-\infty,\infty\}$, then
\[\lim_{x\to a^+}\frac{f(x)}{g(x)}=\lim_{x\to a^+}\frac{f'(x)}{g'(x)}.\]
Because the existence of the limits, we know that there exists interval $(a,c)$ such that $g(x)$ and $g'(x)$ both $\neq 0$ on $(a,c)$. Let
\[F(x)=\begin{cases}f(x),\quad & x\in (a,c)\\
0,\quad & x=a\end{cases}\]
and
\[G(x)=\begin{cases}g(x),\quad & x\in (a,c)\\
0,\quad & x=a\end{cases}\]
By CMVT, for any $x\in (a,c)$, there exists $c\in (a,x)$ such that
\[\qty(F(x)-F(a))G'(c)=\qty(G(x)-G(a))F'(c).\]
Since $F(a)=G(a)=0$,
\[F(x)G'(c)=G(x)F'(c).\]
Thus,
\[\lim_{x\to a^+}\frac{f(x)}{g(x)}=\lim_{x\to a^+}\frac{F(x)}{G(x)}=\lim_{x\to a^+}\frac{F'(x)}{G'(x)}=\lim_{x\to a^+}\frac{f'(x)}{g'(x)}.\]

Let $f(x)$ and $g(x)$ be functions differentiable on $(a,b)$ with $a<b$. We want to show that if $\lim_{x\to a^+}f(x)=\lim_{x\to a^+}g(x)=\infty$ and $\lim_{x\to a^+}\frac{f'(x)}{g'(x)}\in\mathbb{R}\cup\{-\infty,\infty\}$, then
\[\lim_{x\to a^+}\frac{f(x)}{g(x)}=\lim_{x\to a^+}\frac{f'(x)}{g'(x)}.\]
Because $\lim_{x\to a^+}f(x)=\lim_{x\to a^+}g(x)=\infty$, by quotient law, we know that
\[\lim_{x\to a^+}\frac{1}{f(x)}=\lim_{x\to a^+}\frac{1}{g(x)}=0.\]
Because the existence of the limits, there exists interval $(a,c)$ such that $\frac{1}{f(x)}$ and $\frac{1}{g(x)}$ both $\neq 0$ on $(a,c)$. Let
\[F(x)=\begin{cases}\frac{1}{f(x)},\quad & x\in (a,b)\\
0,\quad & x=a\end{cases}\]
and
\[G(x)=\begin{cases}\frac{1}{g(x)},\quad & x\in (a,b)\\
0,\quad & x=a\end{cases}\]
\[F'(x)=\frac{-f'(x)}{\qty(f(x))^2}.\]
\[G'(x)=\frac{-g'(x)}{\qty(g(x))^2}.\]
By CMVT, for any $x\in (a,c)$, there exists $c\in (a,x)$ such that
\[\qty(F(x)-F(a))G'(c)=\qty(G(x)-G(a))F'(c).\]
Since $F(a)=G(a)=0$,
\[F(x)G'(c)=G(x)F'(c).\]
Thus,
\[\ba
\lim_{x\to a^+}\frac{f(x)}{g(x)}&=\lim_{x\to a^+}\frac{G(x)}{F(x)}=\lim_{x\to a^+}\frac{G'(x)}{F'(x)}\\
&=\lim_{x\to a^+}\frac{g'(x)}{f'(x)}\lim_{x\to a^+}\qty(\frac{f(x)}{g(x)})^2
\ea\]
\[\lim_{x\to a^+}\frac{f(x)}{g(x)}=\lim_{x\to a^+}\frac{f'(x)}{g'(x)}.\]

All other statements can be proven either similarly or by applying other statements.
\end{proof}
\sssc{Indeterminate produucts}
Let $I$ be an open interval and $a\in I$, and $f(x)$ and $g(x)$ be functions differentiable on $I\setminus\{a\}$. If $\lim_{x\to a}f(x)=0$ and $\abs{\lim_{x\to a}g(x)}=\infty$, where $\lim_{x\to a}f(x)g(x)$ is called indeterminate form of type $0\cdot\infty$, we may find $\lim_{x\to a}f(x)g(x)$ by writing the product $f(x)g(x)$ as a quotient:
\[fg=\frac{f}{1/g},\quad\tx{or\ }fg=\frac{g}{1/f}\]
and use general form of L'Hôpital's rule.

Let $f(x)$ and $g(x)$ be functions differentiable on $(a,b)$ with $a<b$. If $\lim_{x\to a^+}f(x)=0$ and $\abs{\lim_{x\to a^+}g(x)}=\infty$, where $\lim_{x\to a^+}f(x)g(x)$ is called indeterminate form of type $0\cdot\infty$, we may find $\lim_{x\to a^+}f(x)g(x)$ by writing the product $f(x)g(x)$ as a quotient:
\[fg=\frac{f}{1/g},\quad\tx{or\ }fg=\frac{g}{1/f}\]
and use general form of L'Hôpital's rule.

Let $f(x)$ and $g(x)$ be functions differentiable on $(b,a)$ with $a>b$. If $\lim_{x\to a^-}f(x)=0$ and $\abs{\lim_{x\to a^-}g(x)}=\infty$, where $\lim_{x\to a^-}f(x)g(x)$ is called indeterminate form of type $0\cdot\infty$, we may find $\lim_{x\to a^-}f(x)g(x)$ by writing the product $f(x)g(x)$ as a quotient:
\[fg=\frac{f}{1/g},\quad\tx{or\ }fg=\frac{g}{1/f}\]
and use general form of L'Hôpital's rule.

Let $f(x)$ and $g(x)$ be functions differentiable on $(a,\infty)$. If $\lim_{x\to\infty}f(x)=0$ and $\abs{\lim_{x\to\infty}g(x)}=\infty$, where $\lim_{x\to\infty}f(x)g(x)$ is called indeterminate form of type $0\cdot\infty$, we may find $\lim_{x\to\infty}f(x)g(x)$ by writing the product $f(x)g(x)$ as a quotient:
\[fg=\frac{f}{1/g},\quad\tx{or\ }fg=\frac{g}{1/f}\]
and use general form of L'Hôpital's rule.

Let $f(x)$ and $g(x)$ be functions differentiable on $(-\infty,a)$. If $\lim_{x\to -\infty}f(x)=0$ and $\abs{\lim_{x\to -\infty}g(x)}=\infty$, where $\lim_{x\to -\infty}f(x)g(x)$ is called indeterminate form of type $0\cdot\infty$, we may find $\lim_{x\to -\infty}f(x)g(x)$ by writing the product $f(x)g(x)$ as a quotient:
\[fg=\frac{f}{1/g},\quad\tx{or\ }fg=\frac{g}{1/f}\]
and use general form of L'Hôpital's rule.
\sssc{Indeterminate differences}
Let $I$ be an open interval and $a\in I$, and $f(x)$ and $g(x)$ be functions differentiable on $I\setminus\{a\}$. If $\lim_{x\to a}f(x)=\lim_{x\to a}g(x)\in\{\infty,-\infty\}$, where $\lim_{x\to a}\qty(f(x)-g(x))$ is called indeterminate form of type $\infty-\infty$, we may find $\lim_{x\to a}\qty(f(x)-g(x))$ by writing the difference $f(x)-g(x)$ as a quotient and use general form of L'Hôpital's rule.

Let $f(x)$ and $g(x)$ be functions differentiable on $(a,b)$ with $a<b$. If $\lim_{x\to a^+}f(x)=\lim_{x\to a^+}g(x)\in\{\infty,-\infty\}$, where $\lim_{x\to a^+}\qty(f(x)-g(x))$ is called indeterminate form of type $\infty-\infty$, we may find $\lim_{x\to a^+}\qty(f(x)-g(x))$ by writing the difference $f(x)-g(x)$ as a quotient and use general form of L'Hôpital's rule.

Let $f(x)$ and $g(x)$ be functions differentiable on $(b,a)$ with $a>b$. If $\lim_{x\to a^-}f(x)=\lim_{x\to a^-}g(x)\in\{\infty,-\infty\}$, where $\lim_{x\to a^-}\qty(f(x)-g(x))$ is called indeterminate form of type $\infty-\infty$, we may find $\lim_{x\to a^-}\qty(f(x)-g(x))$ by writing the difference $f(x)-g(x)$ as a quotient and use general form of L'Hôpital's rule.

Let $f(x)$ and $g(x)$ be functions differentiable on $(a,\infty)$. If $\lim_{x\to\infty}f(x)=\lim_{x\to\infty}g(x)\in\{\infty,-\infty\}$, where $\lim_{x\to\infty}\qty(f(x)-g(x))$ is called indeterminate form of type $\infty-\infty$, we may find $\lim_{x\to\infty}\qty(f(x)-g(x))$ by writing the difference $f(x)-g(x)$ as a quotient and use general form of L'Hôpital's rule.

Let $f(x)$ and $g(x)$ be functions differentiable on $(-\infty,a)$. If $\lim_{x\to -\infty}f(x)=\lim_{x\to -\infty}g(x)\in\{\infty,-\infty\}$, where $\lim_{x\to -\infty}\qty(f(x)-g(x))$ is called indeterminate form of type $\infty-\infty$, we may find $\lim_{x\to -\infty}\qty(f(x)-g(x))$ by writing the difference $f(x)-g(x)$ as a quotient and use general form of L'Hôpital's rule.
\sssc{Indeterminate powers/exponentials}
Let $I$ be an open interval and $a\in I$, and $f(x)$ and $g(x)$ be functions differentiable on $I\setminus\{a\}$. If $\lim_{x\to a}f(x)=\lim_{x\to a}g(x)=0$, where $\lim_{x\to a}f(x)^{g(x)}$ is called indeterminate form of type $0^0$, we may find $\lim_{x\to a}f(x)^{g(x)}$ by taking natural logarithm:
\[g(x)\ln\qty(f(x)),\]
which is in indeterminate form of $0\cdot\infty$ because $\lim_{x\to a}\ln\qty(f(x))=-\infty$.

Let $I$ be an open interval and $a\in I$, and $f(x)$ and $g(x)$ be functions differentiable on $I\setminus\{a\}$. If $\lim_{x\to a}f(x)=\infty$ and $\lim_{x\to a}g(x)=0$, where $\lim_{x\to a}f(x)^{g(x)}$ is called indeterminate form of type $\infty^0$, we may find $\lim_{x\to a}f(x)^{g(x)}$ by taking natural logarithm:
\[g(x)\ln\qty(f(x)),\]
which is in indeterminate form of $0\cdot\infty$ because $\lim_{x\to a}\ln\qty(f(x))=\infty$.

Let $I$ be an open interval and $a\in I$, and $f(x)$ and $g(x)$ be functions differentiable on $I\setminus\{a\}$. If $\lim_{x\to a}f(x)=1$ and $\lim_{x\to a}g(x)=\infty$, where $\lim_{x\to a}f(x)^{g(x)}$ is called indeterminate form of type $1^{\infty}$, we may find $\lim_{x\to a}f(x)^{g(x)}$ by taking natural logarithm:
\[g(x)\ln\qty(f(x)),\]
which is in indeterminate form of $0\cdot\infty$ because $\lim_{x\to a}\ln\qty(f(x))=0$.

Let $f(x)$ and $g(x)$ be functions differentiable on $(a,b)$ with $a<b$. If $\lim_{x\to a^+}f(x)=\lim_{x\to a^+}g(x)=0$, where $\lim_{x\to a^+}f(x)^{g(x)}$ is called indeterminate form of type $0^0$, we may find $\lim_{x\to a^+}f(x)^{g(x)}$ by taking natural logarithm:
\[g(x)\ln\qty(f(x)),\]
which is in indeterminate form of $0\cdot\infty$ because $\lim_{x\to a^+}\ln\qty(f(x))=-\infty$.

Let $f(x)$ and $g(x)$ be functions differentiable on $(a,b)$ with $a<b$. If $\lim_{x\to a^+}f(x)=\infty$ and $\lim_{x\to a^+}g(x)=0$, where $\lim_{x\to a^+}f(x)^{g(x)}$ is called indeterminate form of type $\infty^0$, we may find $\lim_{x\to a^+}f(x)^{g(x)}$ by taking natural logarithm:
\[g(x)\ln\qty(f(x)),\]
which is in indeterminate form of $0\cdot\infty$ because $\lim_{x\to a^+}\ln\qty(f(x))=\infty$.

Let $f(x)$ and $g(x)$ be functions differentiable on $(a,b)$ with $a<b$. If $\lim_{x\to a^+}f(x)=1$ and $\lim_{x\to a^+}g(x)=\infty$, where $\lim_{x\to a^+}f(x)^{g(x)}$ is called indeterminate form of type $1^{\infty}$, we may find $\lim_{x\to a^+}f(x)^{g(x)}$ by taking natural logarithm:
\[g(x)\ln\qty(f(x)),\]
which is in indeterminate form of $0\cdot\infty$ because $\lim_{x\to a^+}\ln\qty(f(x))=0$.

Let $f(x)$ and $g(x)$ be functions differentiable on $(b,a)$ with $a>b$. If $\lim_{x\to a^-}f(x)=\lim_{x\to a^-}g(x)=0$, where $\lim_{x\to a^-}f(x)^{g(x)}$ is called indeterminate form of type $0^0$, we may find $\lim_{x\to a^-}f(x)^{g(x)}$ by taking natural logarithm:
\[g(x)\ln\qty(f(x)),\]
which is in indeterminate form of $0\cdot\infty$ because $\lim_{x\to a^-}\ln\qty(f(x))=-\infty$.

Let $f(x)$ and $g(x)$ be functions differentiable on $(b,a)$ with $a>b$. If $\lim_{x\to a^-}f(x)=\infty$ and $\lim_{x\to a^-}g(x)=0$, where $\lim_{x\to a^-}f(x)^{g(x)}$ is called indeterminate form of type $\infty^0$, we may find $\lim_{x\to a^-}f(x)^{g(x)}$ by taking natural logarithm:
\[g(x)\ln\qty(f(x)),\]
which is in indeterminate form of $0\cdot\infty$ because $\lim_{x\to a^-}\ln\qty(f(x))=\infty$.

Let $f(x)$ and $g(x)$ be functions differentiable on $(b,a)$ with $a>b$. If $\lim_{x\to a^-}f(x)=1$ and $\lim_{x\to a^-}g(x)=\infty$, where $\lim_{x\to a^-}f(x)^{g(x)}$ is called indeterminate form of type $1^{\infty}$, we may find $\lim_{x\to a^-}f(x)^{g(x)}$ by taking natural logarithm:
\[g(x)\ln\qty(f(x)),\]
which is in indeterminate form of $0\cdot\infty$ because $\lim_{x\to a^-}\ln\qty(f(x))=0$.

Let $f(x)$ and $g(x)$ be functions differentiable on $(a,\infty)$. If $\lim_{x\to\infty}f(x)=\lim_{x\to\infty}g(x)=0$, where $\lim_{x\to\infty}f(x)^{g(x)}$ is called indeterminate form of type $0^0$, we may find $\lim_{x\to\infty}f(x)^{g(x)}$ by taking natural logarithm:
\[g(x)\ln\qty(f(x)),\]
which is in indeterminate form of $0\cdot\infty$ because $\lim_{x\to\infty}\ln\qty(f(x))=-\infty$.

Let $f(x)$ and $g(x)$ be functions differentiable on $(a,\infty)$. If $\lim_{x\to\infty}f(x)=\infty$ and $\lim_{x\to\infty}g(x)=0$, where $\lim_{x\to\infty}f(x)^{g(x)}$ is called indeterminate form of type $\infty^0$, we may find $\lim_{x\to\infty}f(x)^{g(x)}$ by taking natural logarithm:
\[g(x)\ln\qty(f(x)),\]
which is in indeterminate form of $0\cdot\infty$ because $\lim_{x\to\infty}\ln\qty(f(x))=\infty$.

Let $f(x)$ and $g(x)$ be functions differentiable on $(a,\infty)$. If $\lim_{x\to\infty}f(x)=1$ and $\lim_{x\to\infty}g(x)=\infty$, where $\lim_{x\to\infty}f(x)^{g(x)}$ is called indeterminate form of type $1^{\infty}$, we may find $\lim_{x\to\infty}f(x)^{g(x)}$ by taking natural logarithm:
\[g(x)\ln\qty(f(x)),\]
which is in indeterminate form of $0\cdot\infty$ because $\lim_{x\to\infty}\ln\qty(f(x))=0$.

Let $f(x)$ and $g(x)$ be functions differentiable on $(-\infty,a)$. If $\lim_{x\to -\infty}f(x)=\lim_{x\to -\infty}g(x)=0$, where $\lim_{x\to -\infty}f(x)^{g(x)}$ is called indeterminate form of type $0^0$, we may find $\lim_{x\to -\infty}f(x)^{g(x)}$ by taking natural logarithm:
\[g(x)\ln\qty(f(x)),\]
which is in indeterminate form of $0\cdot\infty$ because $\lim_{x\to -\infty}\ln\qty(f(x))=-\infty$.

Let $f(x)$ and $g(x)$ be functions differentiable on $(-\infty,a)$. If $\lim_{x\to -\infty}f(x)=\infty$ and $\lim_{x\to -\infty}g(x)=0$, where $\lim_{x\to -\infty}f(x)^{g(x)}$ is called indeterminate form of type $\infty^0$, we may find $\lim_{x\to -\infty}f(x)^{g(x)}$ by taking natural logarithm:
\[g(x)\ln\qty(f(x)),\]
which is in indeterminate form of $0\cdot\infty$ because $\lim_{x\to -\infty}\ln\qty(f(x))=\infty$.

Let $f(x)$ and $g(x)$ be functions differentiable on $(-\infty,a)$. If $\lim_{x\to -\infty}f(x)=1$ and $\lim_{x\to -\infty}g(x)=\infty$, where $\lim_{x\to -\infty}f(x)^{g(x)}$ is called indeterminate form of type $1^{\infty}$, we may find $\lim_{x\to -\infty}f(x)^{g(x)}$ by taking natural logarithm:
\[g(x)\ln\qty(f(x)),\]
which is in indeterminate form of $0\cdot\infty$ because $\lim_{x\to -\infty}\ln\qty(f(x))=0$.
\subsection{Lagrange multiplier (method) (拉格朗日乘數/乘子（法）)}
The Lagrange multiplier method is a method for finding the points where extremes occur of a differentiable function under constraints.
\sssc{Univariate form}
Let $f:\,\mathbb{R} \rightarrow \mathbb{R}$ and $g:\,\mathbb{R} \rightarrow \mathbb{R}$ . We want to find the points where extremes of \( f(x) \) under the constraint \( g(x) = \mathbf{0} \) occur. 

First, construct the Lagrangian function \( \mathcal{L}(x,\lambda) \):
\[\mathcal{L}(x,\lambda) = f(x) - \lambda \cdot g(x),\]
where \( \lambda\in\mathbb{R} \) is the Lagrange multiplier (拉格朗日乘數 or 乘子).

Find the derivative of $\mathcal{L}$ and set it to zero:
\[
\dv{\mathcal{L}}{x} = 0
\]
Solve the equation to find \( x \) and \( \lambda \) . 

\text{Statement:} Let $A$ be the set of all solutions for \( x \) such that $\dv{\mathcal{L}}{x} = 0$, and $B$ be the set of all points where extremes of \( f(x) \) under the constraint \( g(x) = \mathbf{0} \) occur. We claim that $B\subseteq A$.
\sssc{Multivariate form}
Let \( \mathbf{x} = (x_1, x_2, \dots, x_n) \) be the independent variable vector and $\mathbf{0}$ be the zero vector. Now we have $f:\,\mathbb{R}^n \rightarrow \mathbb{R}$ and $g:\,\mathbb{R}^n \rightarrow \mathbb{R}^c$. We want to find the points where extremes of \( f(\mathbf{x}) \) under the constraint \( g(\mathbf{x}) = \mathbf{0}\) occur. 

First, construct the Lagrangian function \( \mathcal{L}(\mathbf{x},\lambda) \):
\[
\mathcal{L}(\mathbf{x},\lambda) = f(\mathbf{x}) - \lambda \cdot g(\mathbf{x}),
\]
where \( \lambda\in\mathbb{R}^c \) is the Lagrange multiplier vector.

Find the gradient of $\mathcal{L}$ and set it to zero:
\[
\nabla \mathcal{L} = \mathbf{0}
\]
Solve the equation to find \( \mathbf{x} \) and \( \lambda \) . 

\text{Statement:} Let $A$ be the set of all solutions for \( \mathbf{x} \) such that $\nabla \mathcal{L} = \mathbf{0}$, and $B$ be the set of all points where extremes of \( f(\mathbf{x}) \) under the constraint \( g(\mathbf{x}) = \mathbf{0}\) occur. We claim that $B\subseteq A$.
\begin{proof}
Consider $\mathbf{x}^*\in B$. It must satisfy the constraint:
\[g(\mathbf{x}^*) = \mathbf{0}.\]
Any feasible point $\mathbf{x}$ near $\mathbf{x}^*$ must satisfy the constraint. We can represent $\mathbf{x}$ as:
\[\mathbf{x} = \mathbf{x}^* + \delta\mathbf{x},\]
where $\delta\mathbf{x}$ is a differential change tangent to the manifold defined by $g(\mathbf{x})$, that is, it lies in the kernel of $\nabla g(\mathbf{x}^*)$, that is,
\[g(\mathbf{x}^* + \delta\mathbf{x}) = \mathbf{0}.\]
Find the first-order approximation of $f$ at $\mathbf{x}^*$:
\[f(\mathbf{x}^*+ \delta\mathbf{x}) \approx f(\mathbf{x}^*) + \nabla f(\mathbf{x}^*) \cdot \delta\mathbf{x} + O(\|\delta\mathbf{x}\|^2)\]
Find the first-order approximation of $g$ at $\mathbf{x}^*$:
\[g(\mathbf{x}^*+ \delta\mathbf{x}) \approx g(\mathbf{x}^*) + \nabla g(\mathbf{x}^*) \cdot \delta\mathbf{x} + O(\|\delta\mathbf{x}\|^2)\]
Since $\mathbf{x}^*\in B$, for any feasible $\delta\mathbf{x}$ we must have:
\[\nabla f(\mathbf{x}^*) \cdot \delta\mathbf{x} = 0.\]
Since $g(\mathbf{x}^*) = \mathbf{0}$, we have
\[\nabla g(\mathbf{x}^*) \cdot \delta\mathbf{x} = O(\|\delta\mathbf{x}\|^2).\]
Because \( \delta\mathbf{x} \in \ker(\nabla g(\mathbf{x}^)) \), \( \nabla f(\mathbf{x}^) \) can be expressed as a linear combination of \( \nabla g(\mathbf{x}^) \) . This means that there exists a vector to $\lambda$ such that:
\[\nabla\mathcal{L} = \nabla \qty(f(\mathbf{x}) - \lambda \cdot g(\mathbf{x})) = \mathbf{0} \]
\end{proof}
\ssc{Generalized Schwarz's Theorem, Young's Theorem, or Clairaut's Theorem on equality or symmetry of mixed partial derivatives}
For a function $f\colon\Omega\subseteq\mathbb{R}^n\to\mathbb{R}^o$ and any $m$th derivative function $g$ of it given by
\[g=\frac{\partial^mf}{\prod_{k=1}^r\partial x_{i_k}^{\phantom{i_k}m_k}}\]
with $\sum_{k=1}^rm_k=m$ and $i_k\in\mathbb{N}\land i_k\leq n$ for all $k$. Define constants $t_q$ for all $q\in\mathbb{N}\land q\leq n$ as $t_q=\sum_{i_k=q}m_k$. 

If any such $g$ exists in a neighborhood of $\mathbf{p}\in\Omega$ and is continuous at $\mathbf{p}$, then all such $g$ with the same $t_q$ for all $q\in\mathbb{N}\land q\leq n$ are equal at $\mathbf{p}$.
\ssc{Numerical differentiation (數值微分)}
\sssc{Newton's Method (牛頓法) or Newton-Raphson Method (牛頓-拉普森法)}
Newton's method, also known as Newton-Raphson method, is an iterative technique used to approximate the roots of a real-valued function. Given a function $f(x)$ and an initial guess \( x_0 \), Newton's method refines this guess by repeatedly applying the formula:
\[
x_{n+1} = x_n - \frac{f(x_n)}{f'(x_n)},\quad n\in\mathbb{N}
\]
where:
\begin{itemize}
\item \( x_n \) is the current approximation,
\item \( f(x_n) \) is the value of the function at \( x_n \),
\item \( f'(x_n) \) is the derivative of \( f(x) \) evaluated at \( x_n \).
\end{itemize}
PLACEHOLDER: Newton's method converges quadratically or faster if and proof
PLACEHOLDER: Given convergence, suppose that we want the error to be less than a given value $v$, we can take $x_n$ such that $\abs{x_n-x_{n-1}}<v$?



\section{Integral theorems and methods}
\ssc{Linearity, or sum rule, difference rule, and constant multiple rule}
If functions $f$ and $g$ are integrable over an interval $I$ and $a,b\in\mathbb{R}$, then $af(x)+bg(x)$ is also integrable over $I$ and its integral is given by
\[\int_Iaf(x)+bg(x)\dd{x}=a\int_If(x)\dd{x}+b\int_Ig(x)\dd{x}.\]
\ssc{Fundamental Theorem of Calculus (FTC) (微積分基本定理)}
\sssc{Fundamental Theorem of Calculus Part First or First Fundamental Theorem of Calculus}
Let function $f\colon D\subseteq\bbR\to\bbR$ be continuous on a closed interval $[a,b]$, and $f$ be the function defined, for all $x\in[a,b]$ by
\[F(x)=\int_a^xf(t)\dd{t}.\]
Then $F$ uniformly continuous on $[a,b]$ and differentiable on $(a,b)$ and
\[F'(x)=f(x)\]
for all $x\in (a,b)$.
\begin{proof}
PLACEHOLDER
\end{proof}
\sssc{Fundamental Theorem of Calculus Part Second, Second Fundamental Theorem of Calculus, or Newton–Leibniz Theorem}
Let function $f\colon I\subseteq\bbR\to\bbR$ be Riemann integrable on a closed interval $[a,b]$, and $F\colon J\subseteq\bbR\to\bbR$ be a function continuous on $[a,b]$ and differentiable on $(a,b)$ such that
\[F'(x)=f(x)\]
for all $x\in (a,b)$.

Then
\[\int_a^bf(x)\dd{x}=F(b)-F(a).\]
\begin{proof}
PLACEHOLDER
\end{proof}
\ssc{Mean Value Theorem (MVT) for Integrals}
If a function $f\colon D\subseteq\bbR\to\bbR$ is continuous on a closed interval $[a,b]$ with $a<b$, then
\[\exists c\in (a, b)\text{\ s.t.\ }f(c)=\frac{1}{b-a}\int_a^bf(x)\dd{x}.\]
\begin{proof}
PLACEHOLDER
\end{proof}
\subsection{Integration by substitution (代換積分法), integration by change of variables (換元積分法), u-substitution (u-代換), reverse chain rule, substitution theroem (代換定理), change of variables theorem (換元定理), or transformation theorem (變換定理)}
\subsubsection{Univariate form}
Let $g:\,I\subseteq\mathbb{R}\to\mathbb{R}$ be injective and differentiable on $[a,b]\subseteq I$, with $g'$ being integrable on $[a,b]$, and $f:\,K\supseteq g([a,b])\to\mathbb{R}$ be integrable on $g([a,b])$. Then:
\[\int_a^bf\circ g(x)\cdot g'(x)\,\mathrm{d}x=\int_{g(a)}^{g(b)}f(u)\,\mathrm{d}u,\]
and for $K=g([a,b])$:
\[\int f\circ g(x)\cdot g'(x)\,\mathrm{d}x=\int f(u)\,\mathrm{d}u.\]
\begin{proof}
Consider the interval \([a, b]\) partitioned as
\[P = \{a = x_0 < x_1 < \dots  < x_n = b\}.\]
For each subinterval \([x_{i-1}, x_i]\), let \(\xi_i \in [x_{i-1}, x_i]\) be a sample point. The Riemann sum for the left-hand side of the integral is:
\[S_P = \sum_{i=1}^n f(g(\xi_i)) g'(\xi_i) (x_i - x_{i-1}).\]
Since \(g\) is injective and differentiable, it is either strictly increasing or strictly decreasing on \([a, b]\). Assume \(g\) is strictly increasing (the proof for \(g\) strictly decreasing follows similarly).

Under this assumption, \(g\) maps \([a, b]\) to \([g(a), g(b)]\) with \(g(a) < g(b)\). Let \([g(a), g(b)]\) be partitioned as
\[Q = \{g(a) = u_0 < u_1 < \dots  < u_m = g(b)\},\]
where \(u_i = g(x_i)\).

The Riemann sum for the right-hand side of the integral is:
\[T_Q = \sum_{i=1}^n f(u_i) (u_i - u_{i-1}).\]
Since \(u_i = g(x_i)\) and \(g'(\xi_i) \approx \frac{g(x_i) - g(x_{i-1})}{x_i - x_{i-1}}\), we rewrite:
\[ g'(\xi_i) (x_i - x_{i-1}) \approx g(x_i) - g(x_{i-1}) = u_i - u_{i-1}. \]
Thus, the left-hand Riemann sum \(S_P\) becomes:
\[ S_P = \sum_{i=1}^n f(g(\xi_i)) (u_i - u_{i-1}),\]
which matches the structure of the right-hand Riemann sum \(T_Q\) if we let \(\xi_i = g^{-1}(u_i)\).

As the partition \(P\) of \([a, b]\) gets finer, the Riemann sum \(S_P\) converges to \(\int_a^b f(g(x)) g'(x) \, \mathrm{d}x\). Similarly, as the partition \(Q\) of \([g(a), g(b)]\) gets finer, the Riemann sum \(T_Q\) converges to \(\int_{g(a)}^{g(b)} f(u) \, \mathrm{d}u\).
\end{proof}
\subsubsection{Multivariate form}
Let $\mathbf{T}:\,I\subseteq\mathbb{R}^n\to\mathbb{R}^n$ be injective and differentiable on $D\subseteq I$, with all elements of its gradient (i.e. Jacobian matrix) $\nabla\mathbf{T}$ being continuous on $D$, and $f:\,K\supseteq\mathbf{T}(D)\to\mathbb{R}$ be integrable on $\mathbf{T}(D)$. Then:
\[\int_{\mathbf{T}(D)}f(x_1\,x_2\,\dots\,x_n)\cdot\,\mathrm{d}x_1\,\mathrm{d}x_2\,\dots\,\mathrm{d}x_n=\int_D f(u_1\,u_2\,\dots\,u_n)\abs{\det\left(\nabla\mathbf{T}\right)}\,\mathrm{d}u_1\,\mathrm{d}u_2\,\dots\,\mathrm{d}u_n,\]
and for $K=\mathbf{T}(D)$:
\[\int f(x_1\,x_2\,\dots\,x_n)\cdot\,\mathrm{d}x_1\,\mathrm{d}x_2\,\dots\,\mathrm{d}x_n=\int f(u_1\,u_2\,\dots\,u_n)\abs{\det\left(\nabla\mathbf{T}\right)}\,\mathrm{d}u_1\,\mathrm{d}u_2\,\dots\,\mathrm{d}u_n.\]
\subsubsection{Measure theory form}
Let $X$ be a locally compact Hausdorff space equipped with a finite Radon measure $μ$, and let $Y$ be a \text{\textsigma}-compact Hausdorff space with a \text{\textsigma}-finite Radon measure $\rho$. Let $\phi:\,X\to Y$ be an absolutely continuous function, (which implies that $\mu(E)=0\implies\rho(\phi(E))=0$). Then there exists a real-valued Borel measurable function $w$ on $X$ such that for every Lebesgue integrable function $f:\,Y\to\mathbb{R}$, the function $(f\circ\phi)\cdot w$ is Lebesgue integrable on $X$, and for every open subset $U$ of $X$
\[\int_{\phi(U)}f(y)\,\mathrm{d}\rho(y)=\int_U(f\circ\phi)(x)\cdot w(x)\,\mathrm{d}\mu(x).\]
Furthermore, there exists some Borel measurable function $g$ such that 
\[w(x)=(g\circ\phi)(x).\]
\subsection{Integration by parts (IBP) (分部積分法) or partial integration (部分積分法)}
\subsubsection{Theorem}
\[\frac{\mathrm{d}}{\mathrm{d}x}\prod_{i=1}^nf_i(x)=\sum_{j=1}^n\left(\frac{\mathrm{d}f_j(x)}{\mathrm{d}x}\frac{\prod_{\substack{i=1\\i\neq j}}^n f_i(x)}{f_j(x)}\right)\]
For example,
\[\int_{\Omega} u\dd{v} = \qty(u v)\big\vert_{\Omega} - \int_{\Omega} v\dd{u}.\]
\subsubsection{Application}
Integration by parts is a heuristic rather than a purely mechanical process for solving integrals; given a single function to integrate, the typical strategy is to carefully separate this single function into a product of two functions such that the residual integral from the integration by parts formula is easier to evaluate than the single function.

The DETAIL rule or the LIATE rule is a rule of thumb for integration by parts. It involves choosing as u the function that comes first in the following list:
\begin{itemize}
\item L: Logarithmic function
\item I: Inverse trigonometric function
\item A: Algebraic function
\item T: Trigonometric function
\item E: Exponential function
\end{itemize}
\ssc{Leibniz integral rule (for differentiation under the integral sign) (（積分符號內取微分的）萊布尼茲積分法則)}
\sssc{Basic form for constant limits}
Let $a,b\in\mathbb{R}$, $f(x,t)$ be a function with domain $\mathbb{R}^2$, and the following integral exists. Then:
\[\begin{aligned}
&\dv{}{x}\qty(\int_a^bf(x,t)\dd{t})\\
=&\int_a^b\pdv{}{x}f(x,t)\dd{t}
\end{aligned}\]
\sssc{Basic form for variable limits}
Let $a(x),b(x)\in\mathbb{R}$, $f(x,t)$ be a function with domain $\mathbb{R}^2$, and the following integral exists. Then:
\[\begin{aligned}
&\dv{}{x}\qty(\int_{a(x)}^{b(x)}f(x,t)\dd{t})\\
=&f\qty(x,b(x))\cdot\dv{b(x)}{x}-f\qty(x,a(x))\cdot\dv{a(x)}{x}+\int_{a(x)}^{b(x)}\pdv{}{x}f(x,t)\dd{t}
\end{aligned}\]
\sssc{Measure theory form for const limits}
Let $X$ be an open subset of $\mathbb{R}$, and $\Omega$ be a measure space. Suppose $f\colon X\times\Omega\to\mathbb{R}$ satisfies the following conditions:
\ben
\item $f(x,\omega)$ is a Lebesgue-integrable function of $\omega$ for each $x\in X$.
\item For almost all $\omega\in\Omega$, the partial derivative $\pdv{f}{x}$ exists for all $x\in X$.
\item There is an integrable function $\theta\colon\Omega\to\mathbb{R}$ such that $\abs{\pdv{f}{x}\qty(x,\omega)}\leq\theta(\omega)$ for all $x\in X$ and almost all $\omega\in\Omega$.
\een
Then, for all $x\in X$,
\[\dv{}{x}\int_{\Omega}f(x,\omega)\dd{\omega}=\int_{\Omega}\pdv{f}{x}\qty(x,\omega)\dd{\omega}.\]
\ssc{Fubini's theorem (富比尼定理)}
If a function is Lebesgue integrable on $X\times Y$, then:
\[\iint_{X\times Y}f(x,y)\dd{(x,y)}=\int_X\qty(\int_Yf(x,y)\dd{y})\dd{x}=\int_Y\qty(\int_Xf(x,y)\dd{x})\dd{y}.\]
\ssc{(Lebesgue's) Dominated convergence theorem (DCT) (（勒貝格）控制/受制收斂定理)}
Let $\langle f_n\rangle_{n\in I}$ be a net of real or complex-valued measurable functions on a measure space $(S,\Sigma,\mu)$. If $f_n$ is almost everywhere pointwise convergent to a function $f$, and there is a Lebesgue-integrable function $g$ such that
\[\abs{f_n}\leq g\]
almost everywhere for all $n\in I$.

Then $f_n$ for all $n\in I$ and $f$ are in $L^1(\mu)$ and
\[\lim_n\int_Sf_n\dd{\mu}=\int_S\lim_nf_n\dd{\mu}=\int_Sf\dd{\mu},\]
and
\[\lim_n\int_S\abs{f_n-f}\dd{\mu}=0.\]
\ssc{Numerical integration (數值積分)}
\sssc{The trapezoidal rule (梯形法)}
Let $f$ be a continuous real-valued function on $[a,b]$, the trapezoidal rule gives the approximation
\[\int_a^bf(x)\,\mathrm{d}x\approx\frac{b-a}{2n}\left(2\left(\sum_{i=0}^nf(a+\frac{i}{n}(b-a))\right)-f(a)-f(b)\right).\]
The error is defined as
\[E_n=\int_a^bf(x)\,\mathrm{d}x-\frac{b-a}{2n}\left(2\left(\sum_{i=0}^nf(a+\frac{i}{n}(b-a))\right)-f(a)-f(b)\right).\]
When $\frac{\mathrm{d}^2f(x)}{\mathrm{d}x^2}$ is continuous on $[a,b]$, the error satisfies that
\[\abs{E_n}\leq\frac{(b-a)^3}{12n^2}\max_{a\leq x\leq b}\left(\abs{\frac{\mathrm{d}^2f(x)}{\mathrm{d}x^2}}\right).\]
\sssc{The Simpson's rule (辛普森法) or the Simpson's 1 or 3 rule}
Let $f$ be a continuous real-valued function on $[a,b]$, the Simpson's rule gives the approximation
\[\int_a^bf(x)\,\mathrm{d}x\approx\frac{b-a}{3n}\left(2\left(\sum_{i=0}^nf(a+\frac{i}{n}(b-a))\right)+2\left(\sum_{i=1}^{\frac{n}{2}}f(a+\frac{2i-1}{n}(b-a))\right)-f(a)-f(b)\right),\]
where $\frac{n}{2}\in\mathbb{N}$.

The error is defined as
\[E_n=\int_a^bf(x)\,\mathrm{d}x-\frac{b-a}{3n}\left(2\left(\sum_{i=0}^nf(a+\frac{i}{n}(b-a))\right)+2\left(\sum_{i=1}^{\frac{n}{2}}f(a+\frac{2i-1}{n}(b-a))\right)-f(a)-f(b)\right).\]
When $\frac{\mathrm{d}^4f(x)}{\mathrm{d}x^4}$ is continuous on $[a,b]$, the error satisfies that
\[\abs{E_n}\leq\frac{(b-a)^5}{180n^4}\max_{a\leq x\leq b}\left(\abs{\frac{\mathrm{d}^4f(x)}{\mathrm{d}x^4}}\right).\]
\ssc{Laplace transform (拉普拉斯變換)}
\sssc{(Unilateral or One-sided) Laplace transform (（單邊）拉普拉斯變換)}
The (unilateral or one-sided) Laplace transform is an integral transform that converts a function of a real variable (usually $t$, in the time domain) to a function of a complex variable (usually $s$, in the complex-valued frequency domain, also known as $s$-domain, or $s$-plane). The functions are often denoted in lowercase for the time-domain representation and uppercase for the frequency-domain.

The Laplace transform of a real function $f(t)$, denoted as $\mathcal{L}\{f(t)\}(s)$ or $F(s)$ is defined by the improper integral
\[\mathcal{L}\{f(t)\}(s) = F(s) = \int_0^{\infty} e^{-st} f(t)\dd{t}.\]

In the context of Laplace transform, convergence refers to absolute convergence, that is, we say $F(s)$ converges or $f(t)$ is Laplace-transformable if the improper integral
\[\int_0^{\infty} \abs{e^{-st} f(t)}\dd{t}\]
converges absolutely for some real $s$.

The set of values for which $F(s)$ converges, called region of convergence (ROC), is either of the form $\Re(s) > a$ or $\Re(s) \geq a$, where $a$ is an extended real constant, i.e. $-\infty\leq a\leq\infty$.
\sssc{Laplace transformability}
A function $f\colon\mathbb{R}\to\mathbb{R}$ is Laplace-transformable if it is piecewise continuous on $[0,\infty)$ and of exponential order.
\begin{proof}
Assume that a $f\colon D\subseteq [0,\infty)\to\mathbb{R}$ is piecewise continuous on $[0,\infty)$ and there exists $M>0$ and $T>0$ such that:
\[|f(t)|\leq Me^{\alpha t}\quad \forall t>T.\]

For $s>\alpha$, since $f(t)$ is piecewise continuous, there exist $t_0=0,t_1,\dots t_k\in\mathbb{R}$ such that:
\begin{itemize}
\item the integral
\[\int_{t_i}^{t_{i+1}} |f(t)| e^{-st}\dd{t}\]
exists and is finite for every $i\in\mathbb{Z}\land 0\leq i\leq k-1$, and that
\item the family $\{[t_0,t_1],[t_1,t_2],\dots [t_{k-1},t_k]$ is locally finite, and that
\item $f(t)$ is continuous on $[t_k,\infty)$.
\end{itemize}

Rewrite $F(s)$ as:
\[\int_0^\infty |f(t)| e^{-st}\dd{t} = \sum_{i=0}^{k-1} \int_{t_i}^{t_{i+1}} |f(t)| e^{-st}\dd{t} + \int_{t_k}^\infty |f(t)| e^{-st}\dd{t}\]

$\sum_{i=0}^{k-1} \int_{t_i}^{t_{i+1}} |f(t)| e^{-st}\dd{t}$ is finite.

For the second integral, since $|f(t)| \le M e^{\alpha t}$, we have
\[\int_{t_k}^\infty |f(t)| e^{-st}\dd{t}\le \int_{t_k}^\infty M e^{\alpha t} e^{-st}\dd{t} = M \int_{t_k}^\infty e^{-(s-\alpha)t}\dd{t}.\]
Since $s>\alpha$, the integral converges, so the second integral is finite.
\end{proof}
\sssc{Linearity}
\[\mathcal{L}\{af(t)+bg(t)\}=a\mathcal{L}\{f(t)\}+b\mathcal{L}\{g(t)\}.\]
\sssc{First shifting theorem or Time shift}
\[\mathcal{L}\{f(t-a)u(t-a)\}=e^{-as}F(s), \quad a > 0,\]
where $u(t)$ is the unit step function.
\sssc{Second shifting theorem or Frequency shift}
\[\mathcal{L}\{e^{at} f(t)\}(s) = F(s-a).\]
\sssc{Differentiation in Time Domain}
\[\mathcal{L}\{f'(t)\}(s) = sF(s) - f\qty(0^+).\]
\[\mathcal{L}\{f^{(n)}(t)\}(s) = s^n F(s) - \sum_{i=0}^{n-1}s^{n-1-i}f^{(i)}\qty(0^+).\]
\begin{proof}
\[\mathcal{L}\{f'(t)\}(s) = \int_0^\infty e^{-st} f'(t)\dd{t}.\]
\[u = e^{-st} \quad \Rightarrow \quad \dd{u} = -s e^{-st} \dd{t}.\]
\[dv = f'(t)\dd{t} \quad \Rightarrow \quad v = f(t).\]
Integral by parts.
\[\int_0^\infty u\dd{v} = (u v)\big\vert_0^\infty - \int_0^\infty v\dd{u}.\]
\bma
\int_0^\infty e^{-st} f'(t)\dd{t}&=\qty(e^{-st}f(t))\big\vert_0^\infty+\int_0^\infty f(t)se^{-st}\dd{t}\\
&=-f\qty(0^+)+\int_0^\infty f(t)se^{-st}\dd{t}\\
&=s\mathcal{L}\{f(t)\}(s)-f\qty(0^+)
\ea\]
\end{proof}
\sssc{Integration in Time Domain}
\[\mathcal{L}\left\{\int_0^t f(\tau)\dd{\tau}\right\}(s) = \frac{1}{s} F(s).\]
\begin{proof}
\bma
\mathcal{L}\{\int_0^t f(\tau)\dd{\tau}\}(s) &= \int_0^{\infty} e^{-st} \int_0^t f(\tau)\dd{\tau}\dd{t}\\
&=\qty(-\frac{1}{s}e^{-st}\int_0^t f(\tau)\dd{\tau})\big\vert_0^\infty+\int_0^{\infty} \frac{1}{s}e^{-st}f(t)\dd{t}\\
&=\frac{1}{s}\int_0^{\infty} e^{-st}f(t)\dd{t}.
\eam
\end{proof}
\sssc{Differentiation in Frequency Domain}
\[\mathcal{L}\{t f(t)\}(s) = -\dv{}{s} F(s).\]
\[\mathcal{L}\{t^n f(t)\}(s) = (-1)^n \dv[n]{}{s} F(s).\]
\begin{proof}
By Leibniz integral rule,
\bma
-\dv{}{s} F(s)&=-\dv{}{s}\int_0^{\infty} e^{-st} f(t)\,\mathrm{d}t\\
&=-\int_0^{\infty} \pdv{}{s}\qty(e^{-st} f(t))\,\mathrm{d}t\\
&=-\int_0^{\infty} -te^{-st} f(t)\,\mathrm{d}t\\
&=\int_0^{\infty} te^{-st} f(t)\,\mathrm{d}t\\
&=\mathcal{L}\{t f(t)\}(s)
\eam
\end{proof}
\sssc{Scaling in Time Domain}
\[\mathcal{L}\{f(at)\}(s) = \frac{1}{a} F\left(\frac{s}{a}\right), \quad a>0.\]
\sssc{Convolution Theorem}
If $h(t) = (f * g)(t) = \int_0^t f(\tau)g(t-\tau)\dd{\tau}$, then
\[\mathcal{L}\{h(t)\}(s) = F(s)G(s).\]
\begin{proof}
\[\mathcal{L}\{h(t)\}(s)=\int_0^{\infty} e^{-st} \qty(\int_0^t f(\tau)g(t-\tau)\dd{\tau})\dd{t}\]
By Fubini's theorem,
\[\mathcal{L}\{h(t)\}(s)=\int_0^\infty\int_\tau^\infty e^{-st} f(\tau)g(t-\tau)\dd{t}\dd{\tau}\]
Let $u=t-\tau$. $\dd{t}=\dd{u}$.
\[\begin{aligned}
\mathcal{L}\{h(t)\}(s)&=\int_0^\infty f(\tau)\int_0^\infty e^{-s(u+\tau)} g(u)\dd{u}\dd{\tau}\\
&=\int_0^\infty f(\tau)e^{-s\tau}\int_0^\infty e^{-su} g(u)\dd{u}\dd{\tau}\\
&=F(s)G(s)
\end{aligned}\]
\end{proof}
\sssc{Initial Value Theorem}
If $f(t)$ and $f'(t)$ are Laplace-transformable:
\[\lim_{t \to 0^+} f(t) = \lim_{s \to \infty} s F(s).\]
\begin{proof}
\[s F(s) = \int_0^\infty s f(t) e^{-st} \dd{t}.\]
Let $u = st$, $\dd{t} = \frac{\dd{u}}{s}$.
\[s F(s) = \int_0^\infty f\qty(\frac{u}{s}) e^{-u} \dd{u}.\]
\[\lim_{s \to \infty}s F(s)=\int_0^\infty f\qty(\frac{u}{s}) e^{-u} \dd{u}.\]

We define a net of functions $\langle f_s\rangle_{s\in\mathbb{R}_{>s_0}}$, where $s_0$ is such that $F(s)$ converges for all $\Re(s)>s_0$.

For fixed $u \in\mathbb{R}_{>0}$, $\lim_{s\to\infty}\frac{u}{s} \to 0^+$, so $f_n\qty(\frac{u}{s})$ pointwise converges to $f(0^+)$.

For dominated convergence theorem, we require an integrable function $g(u)$ such that
\[\abs{f\qty(\frac{u}{s}) e^{-u}} \le g(u), \quad \forall s > 0.\]

Since $f(t)$ is Laplace-transformable, it is of exponential order, that is, there exists $\alpha>0$, $M>0$, and $T>0$ such that:
\[|f(t)|\leq Me^{\alpha t}\quad \forall t>T.\]
\[M e^{\alpha \frac{u}{s}} e^{-u} = M e^{-u\qty(1 - \frac{\alpha}{s})}\le M e^{-\frac{u}{2}}, \quad \forall s > 2\alpha.\]

By dominated convergence theorem, we obtain:
\[\lim_{s \to \infty}s F(s)=\lim_{n\to\infty}
\int_0^\infty f(0^+) e^{-u} \dd{u}.\]

Evaluate the integral:
\[\int_0^\infty f(0^+) e^{-u} \dd{u} = f(0^+) \int_0^\infty e^{-u}\dd{u} = f(0^+).\]
\end{proof}
\sssc{Mellin's inverse formula, Bromwich integral, or Fourier–Mellin integral of Inverse Laplace transform (反拉普拉斯變換)}
The inverse Laplace transform of a complex function $F(s)$, denoted as $\mathcal{L}^{-1}\{F(s)\}(t)$ or $f(t)$, is defined as a real function such that
\[\mathcal{L}\{f(t)\}(s) = F(s),\]
where $\mathcal {L}$ denotes the Laplace transform.

Mellin's inverse formula, Bromwich integral, or Fourier–Mellin integral states that, the inverse Laplace transform of a complex function $F(s)$ is given by the line integral:
\[\mathcal{L}^{-1}\{F(s)\}(t)=f(t)=\frac{1}{2\pi i}\lim_{T\to\infty}\int_{\gamma-iT}^{\gamma+iT}e^{st}F(s)\dd{s},\]
where $\gamma$ is a real number such that it is greater than the real part of all singularities of $F$ and that $F$ is bounded on the line $s=\gamma$.
\sssc{(Bilateral or Two-sided) Laplace Transform (（雙邊）拉普拉斯變換)}
The (bilateral or two-sided) Laplace transform is an integral transform that converts a function of a real variable (usually $t$, in the time domain) to a function of a complex variable (usually $s$, in the complex-valued angular frequency domain, also known as $s$-domain or $s$-plane). The functions are often denoted in lowercase for the time-domain representation and uppercase for the frequency-domain.

The Laplace transform of a real function $f(t)$, denoted as $\mathcal{B}\{f(t)\}(s)$, is defined by the improper integral
\[\mathcal{B}\{f(t)\}(s) = \int_{-\infty}^{\infty} e^{-st} f(t)\,\mathrm{d}t.\]
\ssc{Fourier Transform (FT) (傅立葉變換)}
\sssc{Fourier Transform (FT)}
Fourier transform is an integral transform that converts a function of a real variable (usually $t$, in the time domain) to a function of another real variable (usually $\omega$, in the real-valued angular frequency domain). The functions are often denoted in lowercase for the time-domain representation and uppercase for the frequency-domain.

The Fourier transform, denoted as $\mathcal{F}\{f(t)\}(\omega)$ or $F(\omega)$, is defined by the improper integral
\[\mathcal{F}\{f(t)\}(\omega) = F(\omega) = \int_{-\infty}^{\infty} e^{-i\omega t} f(t)\,\mathrm{d}t.\]
\sssc{Inverse Fourier Transform (反傅立葉變換)}
The inverse Fourier transform of a complex function $F(s)$, denoted as $\mathcal{F}^{-1}\{F(s)\}(t)$ or $f(t)$, is defined as a real function such that
\[\mathcal{F}\{f(t)\}(s) = F(s),\]
where $\mathcal {F}$ denotes the Fourier transform.

The inverse Laplace transform of a complex function $F(s)$ is given by the line integral:
\[f(t) = \frac{1}{2\pi}\int_{-\infty}^{\infty} F(\omega) e^{i\omega t}\dd{\omega} .\]



\sct{Lists}
\ssc{List of Limits of Real Functions}
\sssc{Limit of sine function over independent variable at zero}
\[\lim_{h\to 0}\frac{\sin h}{h}=1.\]
\begin{proof}
On a unit circle with center $O$, consider the arc $\arc{AB}=h$ subtended by acute angle $h$ (i.e. $\frac{\pi}{2}>h>0$), the chord length $\ol{BC}=\sin(h)$ with $C$ be the intersection of the chord and $\ora{OA}$, and the tangent length $\ol{AD}=\tan(h)$ with $D$ be the intersection of the tangent and $\ora{OB}$.

We can observe that
\[\ol{BC}<\arc{AB}.\]

Take tangent of the circle at $B$. Let it intersects $\ol{AD}$ at $E$. We can observe that the adjacent $\ol{BE}$ is less than the hypothenuse $\ol{DE}$. Thus
\[\arc{AB}<\ol{BE}+\ol{AE}<\ol{DE}+\ol{AE}=\ol{AD}.\]

Thus,
\[\sin h<h<\tan h.\]
\[1<\frac{h}{\sin h}<\frac{1}{\cos h}.\]
\[\cos h<\frac{\sin h}{h}<1.\]
\[\lim_{h\to 0}\cos h=1.\]

By the squeeze theorem:
\[\lim_{h\to 0}\frac{\sin h}{h}=1.\]
\end{proof}
\ssc{List of Derivatives of Real Functions and Real Functions as Definite Integrals}
\sssc{Power function}
\[\dv{x^n}{x}=nx^{n-1}.\]
\[x^n=n\int_0^xt^{n-1}\dd{t}.\]
\begin{proof}
\[\dv{x^n}{x}=\lim_{h\to 0}\frac{(x+h)^n-x^n}{h}=\lim_{h\to 0}\frac{\sum_{k=1}^{\infty}\binom{n}{k}x^{n-k}h^k}{h}=nx^{n-1}.\]
\end{proof}
\sssc{Absolute value function}
\[\dv{|x|}{x}=\operatorname{sgn}(x),\quad x\neq 0.\]
\sssc{Logarithmic function}
\[\dv{\ln(x)}{x}=\frac{1}{x}.\]
\[\ln(x)=\int_1^x\frac{1}{t}\dd{t},\quad x>0.\]
\sssc{Exponential function}
\[\dv{a^x}{x}=\ln(a)a^x.\]
\[a^x=1+\int_0^x\ln(a)a^t\dd{t}.\]
\begin{proof}
\[\ba
\dv{a^x}{x}&=\lim_{h\to 0}\frac{a^{x+h}-a^x}{h}\\
&=a^x\lim_{h\to 0}\frac{a^h-1}{h}\\
&=a^x\lim_{h\to 0}\frac{e^{\ln(a)h}-1}{h}\\
&=a^x\lim_{h\to 0}\frac{\lim_{n\to\infty}\sum_{k=0}^n\frac{\qty(\ln(a)h)^k}{k!}-1}{h}\\
&=a^x\lim_{h\to 0}\frac{\lim_{n\to\infty}\sum_{k=1}^n\frac{\qty(\ln(a)h)^k}{k!}}{h}\\
&=\ln(a)a^x
\ea\]
\end{proof}
\sssc{Sine function}
\[\dv{\sin(x)}{x}=\cos(x).\]
\[\sin(x)=\int_0^x\cos(t)\dd{t}.\]
\begin{proof}
\[\begin{aligned}
\dv{\sin\theta}{\theta}&=\lim_{h\to 0}\frac{\sin(\theta+h)-\sin(\theta)}{h}\\
&=\lim_{h\to 0}\frac{\sin(\theta)(\cos h-1)+\cos\theta\sin h}{h}
\end{aligned}\]
\[\cos h-1=-2\sin^2\qty(\frac{h}{2}),\]
\[\frac{\sin^2\qty(\frac{h}{2})}{h}=\frac{\sin\qty(\frac{h}{2})}{\frac{h}{2}}\cdot\frac{\sin\qty(\frac{h}{2})}{2}.\]
By
\[\lim_{h\to 0}\frac{\sin h}{h}=1.\]
we have:
\[\lim_{h\to 0}\frac{\cos h-1}{h}=0.\]
\[\begin{aligned}
&=\lim_{h\to 0}\sin\theta\cdot 0+\cos\theta\cdot 1\\
&=\cos\theta.
\end{aligned}\]
\end{proof}
\sssc{Cosine function}
\[\dv{\cos(x)}{x}=-\sin(x).\]
\[\cos(x)=1-\int_0^x\sin(t)\dd{t}.\]
\sssc{Tangent function}
\[\dv{\tan(x)}{x}=\sec^2(x).\]
\[\tan(x)=\int_{\operatorname{round}\qty(\frac{x}{\pi})\pi}^x\sec^2(t)\dd{t},\quad x\in\bbR\setminus\{\frac{\pi}{2}+k\pi\mid k\in\bbZ\},\]
in which $\operatorname{round}(x)=\left\lfloor x+\frac{1}{2}\right\rfloor$.
\begin{proof}
\[\begin{aligned}
\dv{\tan(x)}{x}&=\dv{}{x}\frac{\sin(x)}{\cos(x)}\\
&=\frac{\cos^2(x)+\sin^2(x)}{\cos^2(x)}\\
&=\sec^2(x).
\end{aligned}\]
\end{proof}
\sssc{Cotangent function}
\[\dv{\cot(x)}{x}=-\csc^2(x).\]
\[\cot(x)=-\int_{\frac{\pi}{2}+\operatorname{round}\qty(\frac{x}{\pi}-\frac{1}{2})\pi}^x\csc^2(t)\dd{t},\quad x\in\bbR\setminus\{k\pi\mid k\in\bbZ\},\]
in which $\operatorname{round}(x)=\left\lfloor x+\frac{1}{2}\right\rfloor$.
\begin{proof}
\[\cot(x)=\frac{\cos(x)}{\sin(x)}.\]
\[\begin{aligned}
\dv{\cot(x)}{x}&=\dv{}{x}\frac{\cos(x)}{\sin(x)}\\
&=\frac{-\sin^2(x)-\cos^2(x)}{\sin^2(x)}\\
&=-\csc^2(x).
\end{aligned}\]
\end{proof}
\sssc{Secant function}
\[\dv{\sec(x)}{x}=\sec(x)\tan(x).\]
\[\sec(x)=\int_{\operatorname{round}\qty(\frac{x}{\pi})\pi}^x\sec(t)\tan(t)\dd{t},\quad x\in\bbR\setminus\{\frac{\pi}{2}+k\pi\mid k\in\bbZ\},\]
in which $\operatorname{round}(x)=\left\lfloor x+\frac{1}{2}\right\rfloor$.
\begin{proof}
\[\ba
\dv{\sec(x)}{x}&=\dv{}{x}\frac{1}{\cos(x)}\\
&=\frac{\sin(x)}{\cos^2(x)}\\
&=\sec(x)\tan(x).
\ea\]
\end{proof}
\sssc{Cosecant function}
\[\dv{\csc(x)}{x}=-\csc(x)\cot(x).\]
\[\csc(x)=-\int_{\frac{\pi}{2}+\operatorname{round}\qty(\frac{x}{\pi}-\frac{1}{2})\pi}^x\csc(t)\cot(t)\dd{t},\]
in which $\operatorname{round}(x)=\left\lfloor x+\frac{1}{2}\right\rfloor$.
\begin{proof}
\[\ba
\dv{\csc(x)}{x}&=\dv{}{x}\frac{1}{\sin(x)}\\
&=\frac{-\cos(x)}{\sin^2(x)}\\
&=-\csc(x)\cot(x).
\ea\]
\end{proof}
\sssc{Inverse sine function}
\[\dv{\arcsin(x)}{x}=\frac{1}{\sqrt{1-x^2}}.\]
\[\arcsin(x)=\int_0^x\frac{1}{\sqrt{1-t^2}}\dd{t},\quad\abs{x}\leq 1.\]
\begin{proof}
Let $y=\arcsin(x)$. $\sin(y)=x$.
\[\dv{}{x}\sin(y)=\dv{}{x}(x).\]
Using the chain rule on the left:
\[\cos(y)\dv{y}{x}=1.\]
\[\dv{y}{x}=\frac{1}{\cos(y)}=\frac{1}{\sqrt{1-x^2}}.\]
\end{proof}
\sssc{Inverse cosine function}
\[\dv{\arccos(x)}{x}=-\frac{1}{\sqrt{1-x^2}}.\]
\[\arccos(x)=\int_x^1\frac{1}{\sqrt{1-t^2}}\dd{t},\quad\abs{x}\leq 1.\]
\begin{proof}
Let $y=\arccos(x)$. $\cos(y)=x$.
\[\dv{}{x}\cos(y)=\dv{}{x}(x).\]
Using the chain rule on the left:
\[-\sin(y)\dv{y}{x}=1.\]
\[\dv{y}{x}=-\frac{1}{\cos(y)}=-\frac{1}{\sqrt{1-x^2}}.\]
\end{proof}
\sssc{Inverse tangent function}
\[\dv{\arctan(x)}{x}=\frac{1}{1+x^2}.\]
\[\arctan(x)=\int_0^x\frac{1}{1+t^2}\dd{t}.\]
\begin{proof}
Let $y=\arctan(x)$. $\tan(y)=x$.
\[\dv{}{x}\tan(y)=\dv{}{x}(x).\]
Using the chain rule on the left:
\[\sec^2(y)\dv{y}{x}=1.\]
\[\dv{y}{x}=\cos^2(y)=\frac{1}{1+x^2}.\]
\end{proof}
\sssc{Inverse cotangent function}
\[\dv{\arccot(x)}{x}=-\frac{1}{1+x^2}.\]
\[\arccot(x)=\int_x^{\infty}\frac{1}{1+t^2}\dd{t}.\]
\begin{proof}
Let $y=\arccot(x)$. $\cot(y)=x$.
\[\dv{}{x}\cot(y)=\dv{}{x}(x).\]
Using the chain rule on the left:
\[-\csc^2(y)\dv{y}{x}=1.\]
\[\dv{y}{x}=-\sin^2(y)=-\frac{1}{1+x^2}.\]
\end{proof}
\sssc{Inverse secant function}
\[\dv{\arcsec(x)}{x}=\frac{1}{|x|\sqrt{x^2-1}}.\]
\[\arcsec(x)=\operatorname{sgn}(x)\int_1^{\abs{x}}\frac{1}{t\sqrt{t^2-1}}\dd{t}+(1-\operatorname{sgn}(x))\frac{\pi}{2},\quad\abs{x}\geq 1.\]
\begin{proof}
\[\ba
\dv{\arcsec(x)}{x}&=\dv{\arccos(\frac{1}{x})}{x}\\
&=-\frac{1}{\sqrt{1-x^{-2}}}\qty(-x^{-2})\\
&=\frac{1}{|x|\sqrt{x^2-1}}
\ea\]
\end{proof}
\sssc{Inverse cosecant function}
\[\dv{\arccsc(x)}{x}=-\frac{1}{|x|\sqrt{x^2-1}}.\]
\[\arccsc(x)=\operatorname{sgn}(x)\int_{\abs{x}}^{\infty}\frac{1}{t\sqrt{t^2-1}}\dd{t},\quad\abs{x}\geq 1.\]
\begin{proof}
\[\ba
\dv{\arccsc(x)}{x}&=\dv{\arcsin(\frac{1}{x})}{x}\\
&=\frac{1}{\sqrt{1-x^{-2}}}\qty(-x^{-2})\\
&=-\frac{1}{|x|\sqrt{x^2-1}}
\ea\]
\end{proof}
\sssc{Hyperbolic sine function}
\[\dv{\sinh(x)}{x}=\cosh(x).\]
\sssc{Hyperbolic cosine function}
\[\dv{\cosh(x)}{x}=\sinh(x).\]
\sssc{Hyperbolic tangent function}
\[\dv{\tanh(x)}{x}=\sech^2(x).\]
\sssc{Hyperbolic cotangent function}
\[\dv{\coth(x)}{x}=-\csch^2(x),\quad x\neq 0.\]
\sssc{Hyperbolic secant function}
\[\dv{\sech(x)}{x}=-\tanh(x)\sech(x).\]
\sssc{Hyperbolic cosecant function}
\[\dv{\csch(x)}{x}=-\coth(x)\csch(x),\quad x\neq 0.\]
\sssc{Inverse hyperbolic sine function}
\[\dv{\arcsinh(x)}{x}=\frac{1}{\sqrt{x^2+1}}.\]
\sssc{Inverse hyperbolic cosine function}
\[\dv{\arccosh(x)}{x}=\frac{1}{\sqrt{x^2-1}},\quad x>1.\]
\sssc{Inverse hyperbolic tangent function}
\[\dv{\arctanh(x)}{x}=\frac{1}{1-x^2},\quad|x|<1.\]
\sssc{Inverse hyperbolic cotangent function}
\[\dv{\operatorname{arccoth}(x)}{x}=\frac{1}{1-x^2},\quad|x|>1.\]
\sssc{Inverse hyperbolic secant function}
\[\dv{\arcsech(x)}{x}=-\frac{1}{x\sqrt{1-x^2}},\quad 0<x<1.\]
\sssc{Inverse hyperbolic cosecant function}
\[\dv{\arcsinh(x)}{x}=-\frac{1}{|x|\sqrt{1+x^2}},\quad x\neq 0.\]
\ssc{List of Taylor Expansions of Real Functions}
\sssc{Power function}
For any $n\in\mathbb{R}$ and $x\in\mathbb{R}$ in the maximum possible domain:
\[x^n=\sum_{k=0}^{\infty}\binom{n}{k}a^{n-k}(x-a)^k,|x-a|<|a|.\]
\sssc{Logarithmic function}
For any $x\in\mathbb{R}_{>0}$:
\[\ln(x)=\ln(a)+\sum_{n=1}^{\infty}(-1)^{n-1}\frac{(x-a)^n}{na^n},\quad 0<x\leq a.\]
\begin{proof}
\[\dv{\ln(x)}{x}=\frac{1}{x}.\]
Thus for $k\in\mathbb{N}$:
\[\ba
\dv[k]{\ln(x)}{x}&=\qty(\prod_{j=0}^{k}(-1-j))x^{-k}\\
&=(-1)^k\frac{1}{(k+1)!x^k}
\ea\]
$\sum_{n=1}^{\infty}(-1)^{n-1}\frac{(x-a)^n}{na^n}$ converge if and only if $-1<\frac{(x-a)^n}{a^n}\leq 1$ (alternating harmonic series converges) if and only if $0<x\leq a$.
\end{proof}
\sssc{Exponential function}
For any $x\in\mathbb{R}$ and $a\in\mathbb{R}_{>0}$:
\[a^x=\sum_{n=0}^{\infty}\frac{a^b\qty(\ln(a))^n}{n!}(x-b)^n.\]
\sssc{Sine function}
\[\sin(x)=\sum_{n=0}^{\infty}\frac{\sin\qty(a+\frac{n\pi}{2})}{n!}(x-a)^n.\]
\sssc{Cosine function}
\[\cos(x)=\sum_{n=0}^{\infty}\frac{\cos\qty(a+\frac{n\pi}{2})}{n!}(x-a)^n.\]
\sssc{Tangent function}
Define the derivative polynomials $P_n$ such that $P_n\qty(\tan(x))=\tan^{(n)}(x)$ with recursion as:
\[\begin{cases}
&P_0(y)=y,\\
&P_n(y)=(1+y^2)P_{n-1}'(y),\quad n\in\mathbb{N}.
\end{cases}\]

The Taylor expansion of the tangent function at $a$ is:
\[\tan(x)=\sum_{n=0}^{\infty}\frac{P_n\qty(\tan(x))(a)}{n!}(x-a)^n.\]
\begin{proof}
Let $y=\tan(x)$.
\[y'=1+y^2.\]
Prove by mathematical induction. When $n=0$,
\[P_0(y)=y.\]
Assume:
\[P_{n-1}(y)=y^{(n-1)}.\]
Differentiate both side:
\[y'P_{n-1}'(y)=y^{(n)}=(1+y^2)P_{n-1}'(y)=P_n(y).\]
\end{proof}
\sssc{Cotangent function}
Define the derivative polynomials $P_n$ such that $P_n\qty(\cot(x))=\cot^{(n)}(x)$ with recursion as:
\[\begin{cases}
&P_0(y)=y,\\
&P_n(y)=-(1+y^2)P_{n-1}'(y),\quad n\in\mathbb{N}.
\end{cases}\]

The Taylor expansion of the cotangent function at $a$ is:
\[\cot(x)=\sum_{n=0}^{\infty}\frac{P_n\qty(\cot(x))(a)}{n!}(x-a)^n.\]
\begin{proof}
Let $y=\cot(x)$.
\[y'=-(1+y^2).\]
Prove by mathematical induction. When $n=0$,
\[P_0(y)=y.\]
Assume:
\[P_{n-1}(y)=y^{(n-1)}.\]
Differentiate both side:
\[y'P_{n-1}'(y)=y^{(n)}=-(1+y^2)P_{n-1}'(y)=P_n(y).\]
\end{proof}
\ssc{List of Integrals of Real Functions}
\sssc{Power function}
\[\int x^n\dd{x}=\begin{cases}\frac{1}{n+1}x^{n+1}+C,\quad n\neq -1\\\ln|x|+C,\quad n=-1\end{cases}.\]
\sssc{Logarithmic function}
\[\int\ln(x)\dd{x}=x\ln(x)-x+C.\]
\begin{proof}
    \[\int\ln(x)\dd{x}=x\ln(x)-\int x\frac{1}{x}\dd{x}=x\ln(x)-x+C.\]
\end{proof}
\[\int\qty(\ln(x))^n\dd{x}=x\qty(\ln(x))^n-n\int\qty(\ln(x))^{n-1}\dd{x},\quad n\in\bbN.\]
\begin{proof}
    Differentiate RHS:
    \[\dv{}{x}\qty(x\qty(\ln(x))^n-n\int\qty(\ln(x))^{n-1}\dd{x})=\qty(\ln(x))^n+n\qty(\ln(x))^{n-1}-n\qty(\ln(x))^{n-1}=\qty(\ln(x))^n.\]
\end{proof}
\sssc{Exponential function}
\[\int e^{nx}\dd{x}=\frac{1}{n}e^{nx}+C,\quad n\neq 0.\]
\sssc{Sine function}
\[\int\sin(x)\dd{x}=-\cos(x)+C.\]
\[\int\sin^2(x)\dd{x}=\frac{x}{2}-\frac{\sin(2x)}{4}+C.\]
\begin{proof}
    \[\sin^2(x)=\frac{1}{2}\qty(1-\cos(2x)).\]
    \[\int\sin^2(x)\dd{x}=\frac{x}{2}-\frac{1}{2}\int\cos(2x)\dd{x}=\frac{x}{2}-\frac{\sin(2x)}{4}+C.\]
\end{proof}
\[\int\sin^n(x)\dd{x}=-\frac{1}{n}\sin^{n-1}(x)\cos(x)+\frac{n-1}{n}\int\sin^{n-2}(x)\dd{x},\quad n\in\bbN_{>2}.\]
\begin{proof}
    Differentiate RHS:
    \[\dv{}{x}\qty(-\frac{1}{n}\sin^{n-1}(x)\cos(x)+\frac{n-1}{n}\int\sin^{n-2}(x)\dd{x})=-\frac{n-1}{n}\sin^{n-2}(x)\cos^2(x)+\frac{n-1}{n}\sin^{n-2}(x)=\sin^n(x).\]
\end{proof}
\sssc{Cosine function}
\[\int\cos(x)\dd{x}=\sin(x)+C.\]
\[\int\cos^2(x)\dd{x}=\frac{x}{2}+\frac{\sin(2x)}{4}+C.\]
\begin{proof}
    \[\sin^2(x)=\frac{1}{2}\qty(1+\cos(2x)).\]
    \[\int\sin^2(x)\dd{x}=\frac{x}{2}+\frac{1}{2}\int\cos(2x)\dd{x}=\frac{x}{2}+\frac{\sin(2x)}{4}+C.\]
\end{proof}
\[\int\cos^n(x)\dd{x}=\frac{1}{n}\cos^{n-1}(x)\sin(x)+\frac{n-1}{n}\int\cos^{n-2}(x)\dd{x},\quad n\in\bbN_{>2}.\]
\begin{proof}
    Differentiate RHS:
    \[\dv{}{x}\qty(\frac{1}{n}\cos^{n-1}(x)\cos(x)+\frac{n-1}{n}\int\cos^{n-2}(x)\dd{x})=-\frac{n-1}{n}\cos^{n-2}(x)\sin^2(x)+\frac{n-1}{n}\cos^{n-2}(x)=\cos^n(x).\]
\end{proof}
\sssc{Tangent function}
\[\int\tan(x)\dd{x}=-\ln|\cos(x)|+C=\ln|\sec(x)|+C.\]
\begin{proof}
    Differentiate RHS:
    \[\dv{-\ln|\cos(x)|}{x}=-\frac{1}{|\cos(x)|}(-\sin(x))\frac{|\cos(x)|}{\cos(x)}=\tan(x).\]
\end{proof}
\[\int\tan^2(x)\dd{x}=\tan(x)-x+C.\]
\begin{proof}
    Differentiate RHS:
    \[\dv{}{x}\qty(\tan(x)-x)=\sec^2(x)-1=\tan^2(x).\]
\end{proof}
\[\int\tan^n(x)\dd{x}=\frac{1}{n-1}\tan^{n-1}(x)-\int\tan^{n-2}(x)\dd{x},\quad n\in\bbN_{>2}.\]
\begin{proof}
    Differentiate RHS:
    \[\dv{}{x}\qty(\frac{1}{n-1}\tan^{n-1}(x)-\int\tan^{n-2}(x)\dd{x})=\tan^{n-2}(x)+\tan^n(x)-\tan^{n-2}(x)=\tan^n(x).\]
\end{proof}
\sssc{Cotangent function}
\[\int\cot(x)\dd{x}=\ln|\sin(x)|+C.\]
\begin{proof}
    Differentiate RHS:
    \[\dv{\ln|\sin(x)|}{x}=\frac{1}{|\sin(x)|}(\cos(x))\frac{|\sin(x)|}{\sin(x)}=\cot(x).\]
\end{proof}
\[\int\cot^2(x)\dd{x}=-\cot(x)-x+C.\]
\begin{proof}
    Differentiate RHS:
    \[\dv{}{x}\qty(-\cot(x)-x)=\csc^2(x)-1=\cot^2(x).\]
\end{proof}
\[\int\cot^n(x)\dd{x}=-\frac{1}{n-1}\cot^{n-1}(x)-\int\cot^{n-2}(x)\dd{x},\quad n\in\bbN_{>2}.\]
\begin{proof}
    Differentiate RHS:
    \[\dv{}{x}\qty(-\frac{1}{n-1}\cot^{n-1}(x)-\int\cot^{n-2}(x)\dd{x})=\cot^{n-2}(x)+\cot^n(x)-\cot^{n-2}(x)=\cot^n(x).\]
\end{proof}
\sssc{Secant function}
\[\int\sec(x)\dd{x}=\ln|\sec(x)+\tan(x)|+C.\]
\begin{proof}
    Differentiate RHS:
    \[\dv{}{x}\qty(\ln|\sec(x)+\tan(x)|)=\frac{1}{|\sec(x)+\tan(x)|}\qty(\sec(x)\tan(x)+\sec^2(x))\frac{|\sec(x)+\tan(x)|}{\sec(x)+\tan(x)}=\sec(x).\]
\end{proof}
\[\int\sec^2(x)\dd{x}=\tan(x)+C.\]
\[\int\sec^n(x)\dd{x}=\frac{1}{n-1}\sec^{n-2}(x)\tan(x)+\frac{n-2}{n-1}\int\sec^{n-2}(x)\dd{x},\quad n\in\bbN_{>2}.\]
\begin{proof}
    Differentiate RHS:
    \[\dv{}{x}\qty(\frac{1}{n-1}\sec^{n-2}(x)\tan(x)+\frac{n-2}{n-1}\int\sec^{n-2}(x)\dd{x})=\frac{1}{n-1}\sec^n(x)+\frac{n-2}{n-1}\sec^{n-2}(x)\tan^2(x)+\frac{n-2}{n-1}\sec^{n-2}(x)=\sec^n(x).\]
\end{proof}
\sssc{Cosecant function}
\[\int\csc(x)\dd{x}=-\ln|\csc(x)+\cot(x)|+C.\]
\begin{proof}
    Differentiate RHS:
    \[\dv{}{x}\qty(-\ln|\csc(x)+\cot(x)|)=-\frac{1}{|\csc(x)+\cot(x)|}\qty(-\csc(x)\cot(x)-\csc^2(x))\frac{|\csc(x)+\cot(x)|}{\csc(x)+\cot(x)}=\csc(x).\]
\end{proof}
\[\int\csc^2(x)\dd{x}=-\cot(x)+C.\]
\[\int\csc^n(x)\dd{x}=-\frac{1}{n-1}\csc^{n-2}(x)\cot(x)+\frac{n-2}{n-1}\int\csc^{n-2}(x)\dd{x},\quad n\in\bbN_{>2}.\]
\begin{proof}
    Differentiate RHS:
    \[\dv{}{x}\qty(-\frac{1}{n-1}\csc^{n-2}(x)\cot(x)+\frac{n-2}{n-1}\int\csc^{n-2}(x)\dd{x})=\frac{1}{n-1}\csc^n(x)-\frac{n-2}{n-1}\csc^{n-2}(x)\cot^2(x)+\frac{n-2}{n-1}\csc^{n-2}(x)=\csc^n(x).\]
\end{proof}
\sssc{Inverse sine function}
\[\int\arcsin(x)\dd{x}=x\arcsin(x)+\sqrt{1-x^2}+C.\]
\begin{proof}
    Differentiate RHS:
    \[\dv{}{x}\qty(x\arcsin(x)+\sqrt{1-x^2})=\frac{x}{\sqrt{1-x^2}}+\arcsin(x)+\frac{-x}{\sqrt{1-x^2}}=\arcsin(x).\]
\end{proof}
\sssc{Inverse cosine function}
\[\int\arccos(x)\dd{x}=x\arccos(x)-\sqrt{1-x^2}+C.\]
\begin{proof}
    Differentiate RHS:
    \[\dv{}{x}\qty(x\arccos(x)-\sqrt{1-x^2})=-\frac{x}{\sqrt{1-x^2}}+\arccos(x)-\frac{-x}{\sqrt{1-x^2}}=\arccos(x).\]
\end{proof}
\sssc{Inverse tangent function}
\[\int\arctan(x)\dd{x}=x\arctan(x)-\frac{1}{2}\ln(1+x^2)+C.\]
\begin{proof}
    Differentiate RHS:
    \[\dv{}{x}\qty(x\arctan(x)-\frac{1}{2}\ln(1+x^2))=\frac{x}{1+x^2}+\arctan(x)-\frac{2x}{2\qty(1+x^2)}=\arctan(x).\]
\end{proof}
\sssc{Inverse cotangent function}
\[\int\arccot(x)\dd{x}=x\arccot(x)+\frac{1}{2}\ln(1+x^2)+C.\]
\begin{proof}
    Differentiate RHS:
    \[\dv{}{x}\qty(x\arccot(x)+\frac{1}{2}\ln(1+x^2))=-\frac{x}{1+x^2}+\arccot(x)+\frac{2x}{2\qty(1+x^2)}=\arccot(x).\]
\end{proof}
\sssc{Inverse secant function}
\[\int\arcsec(x)\dd{x}=x\arcsec(x)-\operatorname{sgn}(x)\ln\abs{x+\sqrt{x^2-1}}+C,\quad|x|\geq 1.\]
\begin{proof}
Differentiate RHS:
\[\ba
\dv{}{x}\qty(x\arcsec(x)-\operatorname{sgn}(x)\ln\abs{x+\sqrt{x^2-1}})&=\frac{x}{|x|\sqrt{x^2-1}}+\arcsec(x)-\frac{\operatorname{sgn}(x)}{\abs{x+\sqrt{x^2-1}}}\operatorname{sgn}(x)\qty(1+\frac{x}{\sqrt{x^2-1}})\\
&=\frac{\operatorname{sgn}(x)}{\sqrt{x^2-1}}+\arcsec(x)-\frac{1}{\abs{x+\sqrt{x^2-1}}}\frac{x+\sqrt{x^2-1}}{\sqrt{x^2-1}}\\
&=\frac{\operatorname{sgn}(x)}{\sqrt{x^2-1}}+\arcsec(x)-\frac{\operatorname{sgn}(x)}{\sqrt{x^2-1}}\\
&=\arcsec(x)
\ea\]
\end{proof}
\sssc{Inverse cosecant function}
\[\int\arccsc(x)\dd{x}=x\arccsc(x)+\operatorname{sgn}(x)\ln\abs{x+\sqrt{x^2-1}}+C,\quad|x|\geq 1.\]
\begin{proof}
Differentiate RHS:
\[\ba
\dv{}{x}\qty(x\arccsc(x)+\operatorname{sgn}(x)\ln\abs{x+\sqrt{x^2-1}})&=-\frac{x}{|x|\sqrt{x^2-1}}+\arccsc(x)+\frac{\operatorname{sgn}(x)}{\abs{x+\sqrt{x^2-1}}}\operatorname{sgn}(x)\qty(1+\frac{x}{\sqrt{x^2-1}})\\
&=-\frac{\operatorname{sgn}(x)}{\sqrt{x^2-1}}+\arccsc(x)+\frac{1}{\abs{x+\sqrt{x^2-1}}}\frac{x+\sqrt{x^2-1}}{\sqrt{x^2-1}}\\
&=-\frac{\operatorname{sgn}(x)}{\sqrt{x^2-1}}+\arccsc(x)+\frac{\operatorname{sgn}(x)}{\sqrt{x^2-1}}\\
&=\arccsc(x)
\ea\]
\end{proof}
\sssc{Hyperbolic sine function}
\[\int\sinh(x)\dd{x}=\cosh(x)+C.\]
\[\int\sinh^2(x)\dd{x}=\frac{\sinh(2x)}{4}-\frac{x}{2}+C.\]
\[\int\sinh^n(x)\dd{x}=\frac{1}{n}\sinh^{n-1}(x)\cosh(x)-\frac{n-1}{n}\int\sinh^{n-2}(x)\dd{x},\quad n\in\bbN_{>2}.\]
\sssc{Hyperbolic cosine function}
\[\int\cosh(x)\dd{x}=\sinh(x)+C.\]
\[\int\cosh^2(x)\dd{x}\frac{\sinh(2x)}{4}+\frac{x}{2}+C.\]
\[\int\cosh^n(x)\dd{x}=\frac{1}{n}\cosh^{n-1}(x)\sinh(x)+\frac{n-1}{n}\int\cosh^{n-2}(x)\dd{x},\quad n\in\bbN_{>2}.\]
\sssc{Hyperbolic tangent function}
\[\int\tanh(x)\dd{x}=\ln\cosh(x)+C.\]
\[\int\tanh^2(x)\dd{x}=x-\tanh(x)+C.\]
\[\int\tanh^n(x)\dd{x}=\frac{1}{n-1}\tanh^{n-1}(x)-\int\tanh^{n-2}(x)\dd{x},\quad n\in\bbN_{>2}.\]
\sssc{Hyperbolic cotangent function}
\[\int\coth(x)\dd{x}=\ln|\sinh(x)|+C.\]
\[\int\coth^2(x)\dd{x}=x-\coth(x)+C.\]
\[\int\coth^n(x)\dd{x}=-\frac{1}{n-1}\coth^{n-1}(x)-\int\coth^{n-2}(x)\dd{x},\quad n\in\bbN_{>2}.\]
\sssc{Hyperbolic secant function}
\[\int\sech(x)\dd{x}=\arctanh\sinh(x)+C.\]
\[\int\sech^2(x)\dd{x}=\tanh(x)+C.\]
\[\int\sech^n(x)\dd{x}=\frac{1}{n-1}\sech^{n-1}(x)\tanh(x)+\frac{n-2}{n-1}\int\sech^{n-2}(x)\dd{x},\quad n\in\bbN_{>2}.\]
\sssc{Hyperbolic cosecant function}
\[\int\csch(x)\dd{x}=\ln\abs{\tanh\frac{x}{2}}+C.\]
\[\int\csch^2(x)\dd{x}=-\coth(x)+C.\]
\[\int\csch^n(x)\dd{x}=-\frac{1}{n-1}\csch^{n-1}(x)\coth(x)+\frac{n-2}{n-1}\int\csch^{n-2}(x)\dd{x},\quad n\in\bbN_{>2}.\]
\sssc{Inverse hyperbolic sine function}
\[\int\arcsinh(x)\dd{x}=x\arcsinh(x)-\sqrt{x^2+1}+C.\]
\sssc{Inverse hyperbolic cosine function}
\[\int\arccosh(x)\dd{x}=x\arccosh(x)-\sqrt{x^2-1}+C,\quad(x)\geq 1.\]
\sssc{Inverse hyperbolic tangent function}
\[\int\arctanh(x)\dd{x}=x\arctanh(x)+\frac{1}{2}\ln(1-x^2)+C,\quad|x|<1.\]
\sssc{Inverse hyperbolic cotangent function}
\[\int\operatorname{arccoth}(x)\dd{x}=x\operatorname{arccoth}(x)+\frac{1}{2}\ln(x^2-1)+C,\quad|x|>1.\]
\sssc{Inverse hyperbolic secant function}
\[\int\arcsech(x)\dd{x}=x\arcsech(x)+\arctan\qty(\frac{\sqrt{1-x^2}}{x})+C,\quad 0<x\leq 1.\]
\sssc{Inverse hyperbolic cosecant function}
\[\int\arccsch(x)\dd{x}=x\arccsch(x)+\ln\abs{x+\sqrt{x^2+1}}+C,\quad x\neq 0.\]
\sssc{Rational functions}
$a\neq 0$.
\[\int\frac{f'(x)}{f(x)}\,\mathrm{d}x=\ln|f(x)|+C.\]
\[\int\frac{1}{x^2+a^2}\,\mathrm{d}x=\frac{1}{a}\arctan\frac{x}{a}+C.\]
\[\int\frac{1}{x^2-a^2}\,\mathrm{d}x=\frac{1}{2a}\ln\frac{x-a}{x+a}+C.\]
\[\int\frac{1}{a^2-x^2}\,\mathrm{d}x=\frac{1}{2a}\ln\frac{a+x}{a+x}+C.\]
\[\int\frac{1}{ax+b}\,\mathrm{d}x=\frac{1}{a}\ln|ax+b|+C.\]
\[\int(ax+b)^n\,\mathrm{d}x=\frac{(ax+b)^{n-1}}{a(n+1)}+C,\quad n\neq -1.\]
\[\int\frac{x}{ax+b}\,\mathrm{d}x=\frac{x}{a}-\frac{b}{a^2}\ln|ax+b|+C.\]
\[\int\frac{x}{(ax+b)^2}\,\mathrm{d}x=\frac{b}{a^2(ax+b)}+\frac{1}{a^2}\ln|ax+b|+C.\]
\[\int x(ax+b)^n\,\mathrm{d}x=\frac{a(n+1)x-b}{a^2(n+1)(n+2)}(ax+b)^{n+1}+C,\quad n\notin\{-1,-2\}.\]
PLACEHOLDER
\sssc{Irrational functions}
\[\int\sqrt{a^2+x^2}\,\mathrm{d}x=\frac{x}{2}\sqrt{a^2+x^2}+\frac{a^2}{2}\ln\qty(x+\sqrt{a^2+x^2})+C.\]
\[\int\sqrt{x^2-a^2}\,\mathrm{d}x=\frac{x}{2}\sqrt{x^2-a^2}-\frac{a^2}{2}\ln\qty(x+\sqrt{x^2-a^2})+C,\quad x^2>a^2.\]
\[\int\sqrt{a^2-x^2}\,\mathrm{d}x=\frac{x}{2}\sqrt{a^2-x^2}+\frac{a^2}{2}\arcsin\frac{x}{|a|}+C,\quad|a|\geq|x|.\]
\sssc{Gaussian integral (高斯積分) or Euler–Poisson integral}
\[\int_{-\infty}^{\infty}e^{-t^2}\dd{t}=\sqrt{\pi}.\]
\[\int_0^{\infty}e^{-t^2}\dd{t}=\frac{\sqrt{\pi}}{2}.\]
PLACEHOLDER
\ssc{Lists of Laplace Transform}
PLACEHOLDER
\ssc{Lists Fourier Transform}
PLACEHOLDER
\ssc{List of Counterexamples}
\sssc{Dirichlet function (狄利克雷函數)}
The Dirichlet function is a function $f\colon\mathbb{R}\to\mathbb{R};$
\[f(x)=\begin{cases}1,\quad & x\in\mathbb{Q}\\
0,\quad & x\in\mathbb{R}\setminus\mathbb{Q}
\end{cases},\]
which is discontinuous at everywhere.
\sssc{Weierstrass function (魏爾施特拉斯函數)}
The Weierstrass function is a function $f\colon\mathbb{R}\to\mathbb{R};$
\[f(x)=\sum_{n=0}^{\infty}a^n\cos\left(b^n\pi x\right),\]
in which $0<a<1$, $b$ is a positive odd integer, and $ab>1+\frac{3}{2}\pi$, which is continuous but differentiable nowhere.
\sssc{Differentiable but derivative not continuous}
The function $f\colon\mathbb{R}\to\mathbb{R};$
\[f(x)=\begin{cases}x^2\sin\qty(\frac{1}{x}),\quad&x\neq 0\\0,\quad&x=0\end{cases},\]
is differentiable but its derivative is not continuous at $0$. (Note that because $\{0\}$ has Lebesgue measure $0$, $f'\colon\mathbb{R}\to\mathbb{R}$ is continuous almost everywhere, that is, $f'$ is Riemann integrable on any closed interval.)
\begin{proof}
\[f'(x)=2x\sin\qty(\frac{1}{x})-x^2\cos\qty(\frac{1}{x}),\quad x\neq 0\]
\bma
f'(0)&=\lim_{h\to 0}\frac{f(h)}{h}\\
&=\lim_{h\to 0}h\sin\qty(\frac{1}{h})
\eam
For any $|h|>0$, we have $-|h|\leq h\sin\qty(\frac{1}{h})\leq |h|$. By the squeeze theorem, $\lim_{h\to 0}-|h|=\lim_{h\to 0}|h|=0$ implies $\lim_{h\to 0}h\sin\qty(\frac{1}{h})=0$.
\bma
\lim_{x\to 0}f'(x)&=2\lim_{x\to 0}x\sin\qty(\frac{1}{x})-\lim_{x\to 0}\cos\qty(\frac{1}{x})\\
&=-\lim_{x\to 0}\cos\qty(\frac{1}{x})\\
\eam
$\lim_{x\to 0}\cos\qty(\frac{1}{x})$ doesn't exist, so $\lim_{x\to 0}f'(x)$ doesn't exist.
\end{proof}
\sssc{Differentiable at one point and discontinuous everywhere else}
The function $f\colon\mathbb{R}\to\mathbb{R};$
\[f(x)=\begin{cases}1,\quad & x^2\in\mathbb{Q}\\
0,\quad & x\in\mathbb{R}\setminus\mathbb{Q}
\end{cases}\]
which is differentiable at $0$ with $f'(0)=0$ and discontinuous everywhere else.
\sssc{Derivative being zero at a point where no local extreme occurs}
\[f(x)=x^3,\quad f'(x)=3x^2.\]
\[f'(0)=0\]
$f(x)$ has no local extreme at $0$.



\sct{Ordinary Differential Equations (常微分方程) and Dynamical Systems (動態系統 or 動力系統)}
\ssc{Ordinary Differential Equations}
\sssc{Differential form of first-order ODEs}
For a first-order ODE of a dependent variable $y$ with respect to an independent variable $x$,
\[M(x,y)=-N(x,y)\dv{y}{x},\]
where $M$ and $N$ are functions of two variables, $x$ and $y$, the differential form of it is
\[M(x,y)\dd{x}+N(x,y)\dd{y}=0.\]
\sssc{Normal form of ODEs}
The normal form of an $n$th-order ODE of a dependent variable $y$ in codomain $Y$ with respect to an independent variable $x$ in domain $X$ is
\[\dv[n]{y}{x}=F(x,y,y',y'',\dots y^{(n-1)}),\]
in which $F$ is a function of $(n+1)$ variables, $x,y,y',y'',\dots y^{(n-1)}$, with domain being a subset of $X\times Y^n$ and codomain being $Y$.
\sssc{Linear ODE}
An $n$th order ODE of a dependent variable $y$ with respect to an independent variable $x$ is linear if and only if it can be written of the form
\[\sum_{i=0}^na_n(x)y^{(i)}=g(x),\]
where $a_0(x),\dots a_n(x)$, called the coefficients, and $g(x)$, called the forcing term, are given functions of one variable, $x$, and $a_n(x)\neq 0$ on the domain of interest.

An ODE is nonlinear if it is not linear.

A linear ODE is called a homgeneous linear ODE if $g(x)=0$; otherwise, it is called a nonhomogeneous linear ODE.

Let $L[y]=\sum_{i=0}^na_n(x)y^{(i)}$ denotes the left-hand side, then $L[\alpha y_1+\beta y_2]=\alpha L[y_1]+\beta L[y_2]$. That's why it's called to be linear.
\sssc{Solution or flow of an ODE}
Any function $f$ that is defined on an interval $I$, called interval of definition, the interval of existence, the interval of validity, or the domain of the solution, and of class $C^n$ on $I$, and when substituted into an $n$th-order ordinary differential equation reduces the equation to an identity, is said to be a solution (aka flow) of the equation on $I$.

An ODE does not necessarily have to possess a solution.
\sssc{Solution curve or trajectory}
The graph of a solution $f$ on its interval of definition of an ODE is called a solution curve or a trajectory.
\sssc{Explicit solution}
An explicit solution of an ODE of a dependent variable $y$ with respect to an independent variable $x$ is of the form $y=f(x)$, where $f(x)$ is a function of $x$.
\sssc{Implicit solution}
A relation $G(x, y) = 0$ is said to be an implicit solution of an ODE on an interval $I$ if there exists at least one function $f$ that satisfes the relation $G(x, y) = 0$ as well as the ODE on $I$.
\sssc{Type of solutions}
An $n$-parameter family of solutions of an $(\geq n)$th-order ODE of a dependent variable $y$ with respect to an independent variable $x$ is of the form $y=f(x,c_1,c_2,\dots c_n)$ (explicit) or $G(x,y,c_1,c_2,\dots c_n)=0$ (implicit), in which $c_1,c_2,\dots c_n$ are parameters that are arbitrary given that the solution obtained is a solution of the ODE.

If every solution of an $(\geq n)$th-order ODE on an interval $I$ can be obtained from an $n$-parameter family of solutions by appropriate choices of the parameters, we then say that that family of solutions is the general solution (通解) of the ODE.

A solution of an ODE that is free of parameters is called a particular solution (特解).

Sometimes a differential equation possesses a solution that is not a member of a family of solutions of the equation, that is, a solution that cannot be obtained by specializing any of the parameters in the family of solutions. Such an extra solution is called a singular solution (奇異解).
\sssc{Solutions by substitutions or solutions by change of variables}
When solving an ODE, we often replace dependent variables with functions of the independent variable and new dependent variables where the new dependent variables are functions of the independent variable and old dependent variables to transform it into another ODE of the new dependent variables that is easier to solve.
\sssc{System of ODEs (常微分方程組)}
A system of ODEs is two or more equations involving the derivatives of two or more unknown functions or dependent variables of a single independent variable.

A solution of a system of ODEs involving the derivatives of $n$ unknown functions or dependent variables, $y_1,y_2,\dots y_n$ of a single independent variable $x$ is a $n$-tuple of sufficiently smooth functions of $x$ defined on a common interval of definition that satisfy all equations in the system.

A system of ODEs does not necessarily have to possess a solution.
\sssc{Slope function or rate function}
In a first-order ODE of the form $\dv{y}{x}=f(x,y)$, $f$ is called the slope function or rate function.
\sssc{Direction field or slope field (斜率場)}
In a first-order ODE of the form $\dv{y}{x}=f(x,y)$, if we evaluate $f$ over a rectangle grid of points and draw a lineal element at each point $(x,y)$ of the grid with slope $f(x,y)$, then the collection of all these lineal elements is called a direction field or a slope field of the ODE.
\sssc{Isocline}
In a first-order ODE of the form $\dv{y}{x}=f(x,y)$, the isocline for slope $m$ is defined as the family of points $\{(x,y)\mid f(x,y)=m\}$. In the method of isocline, we draw a lineal element at each point in the isocline for slope $m$ of $f$ with slope $m$.
\sssc{Autonomous system (自治系統) (of ODEs)}
A system of first-order ODE is called autonomous if it can be written of the form
\[\dv{x}{t}=f(x),\quad x\in D\subseteq\mathbb{R}^n\land f\colon D\to\mathbb{R}^n.\]
\sssc{Critical point (臨界點), equilibrium, equilibrium point (平衡點), or stationary point (駐點) of autonomous systems of ODEs}
For an autonomous systems of ODEs $\dv{y}{x}=f(y)$, we say that a point $c$ in the domain of $f$ is a critical point, equilibrium, equilibrium point, or stationary point if $f(c)=0$.

If $c$ is a critical point, then $y(x)=c$ is a constant solution of the system of autonomous first-order ODEs, also called an equilibrium solution (平衡解).

If $c$ is a critical point and $f$ is locally Lipschitz at $c$, then $y(x)=c$ is the unique solution of the system of autonomous first-order ODEs through $c$.
\sssc{Translation property or shift property of solutions of autonomous systems of ODEs}
Let function $\Phi(t,y)$ denotes the solution of the autonomous system of ODEs
\[\dv{x}{t}=f(x),\quad x\in D\subseteq\mathbb{R}^n\land f\colon D\to\mathbb{R}^n,\]
with IC $x(0)=y$.

Then the translation property or shift property states that
\[\forall s\in\mathbb{R}\colon\Phi(t+s,y)=\Phi(t,\Phi(s,y)).\]
\sssc{Initial-value problem (IVP) (初值問題)}
An $n$th-order initial-value problem is an $n$th-order ODE with $n$ conditions for $y$ and $(<n)$th derivative functions of $y$ at the same point, that is, $\{y^{(k)}(x_0)=y_k\}_{k\in[0,n-1]\cap\mathbb{Z}}$, called initial conditions (IC) (初始條件).

A solution of an IVP is a solution of the ODE that complies with all the ICs.

The interval of existence of an IVP is the largest open interval containing $x_0$ where a solution of the IVP exists; the interval of existence of an IVP is the largest open interval containing $x_0$ where a unique solution of the IVP exists.
\sssc{Rectangle}
A rectangle in $\mathbb{R}\times\mathbb{R}^n$ is a subset $D$ of it of the form:
\[D=\{(t,y)\mid|t-T|\leq a\land\forall\tx{\ integer\ }1\leq i\leq n\colon |y_i-Y_i|\leq b_i\}\]
where $t,T\in\mathbb{R}$, $y=(y_1, y_2,\dots  y_n)\in\mathbb{R}^n$, $Y=(Y_1, Y_2,\dots  Y_n)\in\mathbb{R}^n$, and constant $a,b_i\in\mathbb{R}_{>0}$.
\sssc{Boundary-value problem (BVP) (邊值問題)}
An $n$th-order boundary-value problem is an $n$th-order ODE with some conditions called boundary conditions (BC) (邊界條件).

A solution of an BVP is a solution of the ODE that complies with all the BCs.
\ssc{Classical theory}
\sssc{(Golbal/regular) Lipschitz continuity (利普希茨連續) of functions between metric spaces}
Let $(X,d_1)$ and $(Y,d_2)$ be metric spaces, and $f\colon D\subseteq X\to Y$ be a function. $f$ is called to be Lipschitz continuous if there exists a real constant $K$ such that $\forall x,y\in D$, $d_{2}(f(x),f(y))\leq Kd_{1}(x,y)$.

Let $(X,d_1)$ and $(Y,d_2)$ be metric spaces, and $f\colon O\subseteq X\to Y$ be a function. $f$ is called to be Lipschitz continuous on $D\subseteq O$ if there exists a real constant $K$ such that $\forall x,y\in D$, $d_{2}(f(x),f(y))\leq Kd_{1}(x,y)$.

Lipschitz continuity implies absolute continuity.

If a function $f\colon D\subseteq X\to Y$ between metric spaces $(X,d_1)$ and $(Y,d_2)$ is such that there exists a real constant $K$ such that $\forall x,y\in D$, $d_{2}(f(x),f(y))\leq Kd_{1}(x,y)$, then, any such $K$ is sometimes called a Lipschitz constant (利普希茨常數) of the function $f$, $f$ is sometimes called to be $K$-Lipschitz, and the smallest such $K$ is sometimes called the best Lipschitz constant, the Lipschitz constant, or the dilation of $f$.
\sssc{Short map}
A short map is a function between metric spaces that is $1$-Lipschitz.
\sssc{Contraction mapping, contraction map, contraction, contractive mapping, contractive map, or contractor (壓縮映射)}
A contraction mapping is a function $f$ from a metric space to itself such that there exists a real number $0\leq K<1$ such that $f$ is $K$-Lipschitz, in which $K$ is sometimes called a contraction constant and the smallest such $K$ is sometimes called the best contraction constant or the contraction constant.
\sssc{Banach fixed-point theorem (巴拿赫不動點定理), contraction mapping theorem (壓縮映射定理), contraction principle, or Banach–Caccioppoli theorem}
For any contraction mapping $T$ over a non-empty complete metric space $X$ with best contraction constant $K$, there must exist a unique fixed-point $x^*$ of $T$ in $X$, and for any $x\in X$, for any sequence $\langle x_n\rangle_{n\in \mathbb {N}_0}$ defined as
\[\begin{cases}
&x_0=x\\
&x_n=T\qty(x_{n-1}),n\in\mathbb{N}
\end{cases},\]
\[\lim_{n\to \infty }x_{n}=x^{*}\]
and
\[d(x_n,x^*)\leq\frac{K^n}{1-K}d(x_1,x_0).\]
\sssc{Locally Lipschitz of functions between metric spaces}
Let $(X,d_1)$ and $(Y,d_2)$ be metric spaces, and $f\colon D\subseteq X\to Y$ be a function. $f$ is called to be locally Lipschitz if $\forall x\in D$, there exists a real constant $K$ and an open set $V\subseteq D$ with $x\in V$ such that $d_{2}(f(y),f(z))\leq Kd_{1}(y,z)$ for all $y,z\in V$.

Let $(X,d_1)$ and $(Y,d_2)$ be metric spaces, and $f\colon O\subseteq X\to Y$ be a function. $f$ is called to be locally Lipschitz on $D\subseteq O$ if $\forall x\in D$, there exists a real constant $K$ and an open set $V\subseteq D$ with $x\in V$ such that $d_{2}(f(y),f(z))\leq Kd_{1}(y,z)$ for all $y,z\in V$.
PLACEHOLDER: below waiting for change to "Ordinary Differential Equations and Dynamical Systems" version
\sssc{Peano existence theorem (皮亞諾存在性定理), Peano theorem (皮亞諾定理), or Cauchy–Peano theorem (柯西-皮亞諾定理)}
Let $D$ be an open subset of $\mathbb{R}\times\mathbb{R}^n$ and $f\colon D\to \mathbb {R}$ be a continuous function, then every initial value problem given by explicit first-order ODE $y'(t)=f\left(t,y(t)\right)$ defined on $D$ and initial condition $y\left(t_{0}\right)=y_{0}$ with $(t_{0},y_{0})\in D$ has a local solution $z\colon I\to \mathbb {R}^n$ where $I$ is a neighbourhood of $t_0$ in $\mathbb {R} $.
\sssc{Carathéodory's existence theorem}
Let
\[D=\{(t,y)\mid|t-T|\leq a\land\forall\tx{\ integer\ }1\leq i\leq n\colon |y_i-Y_i|\leq b_i\}\]
be a rectangle in $\mathbb{R}\times\mathbb{R}^n$ where $t,T\in\mathbb{R}$, $y=(y_1, y_2,\dots  y_n)\in\mathbb{R}^n$, $Y=(Y_1, Y_2,\dots  Y_n)\in\mathbb{R}^n$, and constant $a,b_i\in\mathbb{R}_{>0}$, and $f(t,y)\colon D\to \mathbb {R}$ be a function that is:
\bit
\item continuous in $y$ for each fixed $t$,
\item Lebesgue-measurable in $t$ for each fixed $y$, and
\item such that there is a Lebesgue-integrable function $m\colon [T-a,T+a]\to [0,\infty )$ such that $|f(t,y)|\leq m(t)$ for all $(t,y)\in D$,
\end{itemize}
then every initial value problem given by explicit first-order ODE $y'(t)=f\left(t,y(t)\right)$ defined on $D$ and initial condition $y\left(t_{0}\right)=y_{0}$ with $(t_{0},y_{0})\in D$ has a local solution $z\colon I\to \mathbb {R}^n$ where $I$ is a neighbourhood of $t_0$ in $\mathbb{R}$.

Let $I$ be an open interval of $\mathbb{R}$ and $f(t,y)\colon I\times\mathbb{R}^n\to\mathbb{R}$ where $t\in\mathbb{R}$, $y=(y_1, y_2,\dots  y_n)\in\mathbb{R}^n$ be a function that is:
\bit
\item continuous in $y$ for each fixed $t$,
\item Lebesgue-measurable in $t$ for each fixed $y$, and
\item such that there is a Lebesgue-integrable function $m\colon I\to [0,\infty )$ such that $|f(t,y)|\leq m(t)$ for all $(t,y)\in I\times\mathbb{R}^n$,
\end{itemize}
then every initial value problem given by explicit first-order ODE $y'(t)=f\left(t,y(t)\right)$ defined on $I\times\mathbb{R}^n$ and initial condition $y\left(t_{0}\right)=y_{0}$ with $(t_{0},y_{0})\in I\times\mathbb{R}^n$ has a local solution $z\colon I\to \mathbb {R}^n$ where $I$ is a neighbourhood of $t_0$ in $\mathbb{R}$.
\sssc{Picard–Lindelöf theorem (皮卡-林德勒夫定理) or Cauchy–Lipschitz theorem (柯西-利普希茨定理) local version}
Let
\[D=\{(t,y)\mid|t-T|\leq a\land\forall\tx{\ integer\ }1\leq i\leq n\colon |y_i-Y_i|\leq b_i\}\]
be a rectangle in $\mathbb{R}\times\mathbb{R}^n$ where $t,T\in\mathbb{R}$, $y=(y_1, y_2,\dots  y_n)\in\mathbb{R}^n$, $Y=(Y_1, Y_2,\dots  Y_n)\in\mathbb{R}^n$, and constant $a,b_i\in\mathbb{R}_{>0}$, and $f(t,y)\colon D\to \mathbb {R}$ be a function that is continuous in $t$ andi locally Lipschitz on $D$ in all $y_i$, then every initial value problem given by explicit first-order ODE $y'(t)=f\left(t,y(t)\right)$ defined on $D$ and initial condition $y\left(t_{0}\right)=y_{0}$ with $(t_{0},y_{0})\in D$ has a unique local solution $z\colon I\to \mathbb {R}^n$ where $I$ is a neighbourhood of $t_0$ in $\mathbb {R} $, that is, let $I_1,I_2$ be two neighbourhoods of $t_0$ in $\mathbb{R}$ and $z_i\colon I_i\to\mathbb{R}^n$ for $i\in\{1,2\}$ be two differentiable functions such that $z_i'(t)=f\left(t,z_i(t)\right)$ for all $t\in I_i$, for $i\in\{1,2\}$, then $\exists t_0\in I_1\cap I_2\text{\ s.t.\ }z_1(t_0)=z_2(t_0)\implies\forall t\in I_1\cap I_2\colon z_1(t)=z_2(t)$.

Let $I$ be an open interval of $\mathbb{R}$ and $f(t,y)\colon I\times\mathbb{R}^n\to\mathbb{R}$ where $t\in\mathbb{R}$, $y=(y_1, y_2,\dots  y_n)\in\mathbb{R}^n$ be a function that is continuous in $t$ and locally Lipschitz in all $y_i$, then every initial value problem given by explicit first-order ODE $y'(t)=f\left(t,y(t)\right)$ defined on $I\times\mathbb{R}^n$ and initial condition $y\left(t_{0}\right)=y_{0}$ with $(t_{0},y_{0})\in I\times\mathbb{R}^n$ has a unique solution $z\colon I\to\mathbb {R}^n$, that is, let $I_1,I_2\subseteq\mathbb{R}$ and $z_i\colon I_i\to\mathbb{R}^n$ for $i\in\{1,2\}$ be two differentiable functions such that $z_i'(t)=f\left(t,z_i(t)\right)$ for all $t\in I_i$, for $i\in\{1,2\}$, then $\exists t_0\in I_1\cap I_2\text{\ s.t.\ }z_1(t_0)=z_2(t_0)\implies\forall t\in I_1\cap I_2\colon z_1(t)=z_2(t)$.
PLACEHOLDER: end
\sct{Solving first-order ODEs}
Unless otherwise specified, the unknown functions or dependent variables are in $\mathbb{R}^n$ with $n\in\mathbb{N}$, and the independent variables are in $\mathbb{R}$.
\ssc{First-order separable ODEs}
\sssc{Definition}
A first-order ODE of the form
\[\dv{y}{x}=g(x)h(y)\]
is said to be separable or have separable variables.
\sssc{Solutions by direct integration}
The solutions of
\[\dv{y}{x}=g(x)h(y)\]
are the family of solutions
\[\int\frac{1}{h(y)}\dd{y}=\int g(x)\dd{x}+C\]
and the constant solution
\[h(y)=0.\]
\ssc{First-order linear ODEs}
\sssc{Definition}
A first-order ODE of the form
\[\dv{y}{x}+P(x)y=Q(x)\]
is said to be linear.
\sssc{Solutions by integrating factor}
Use the integrating factor:
\[\mu(x)=e^{\int P(x)\dd{x}}.\]
Multiply both sides with $\mu(x)$:
\[\mu(x)\dv{y}{x}+\mu(x)P(x)y=\mu(x)Q(x).\]
The left-hand side is exactly a derivative:
\[\dv{}{x}\qty(\mu(x)y(x))=\mu(x)Q(x).\]
Integrate:
\[\mu(x)y(x)=\int\mu(x)Q(x)\dd{x}+C.\]
\[y(x)=\frac{1}{\mu(x)}\qty(\int\mu(x)Q(x)\dd{x}+C).\]
\sssc{Remarks}
Occasionally, a first-order differential equation is not linear in one variable
but is linear in the other variable. If a first-order ODE that can be wriiten of the form
\[\dv{y}{x}=\frac{1}{P(y)x+Q(y)}.\]
We can take its reciprocal as
\[\dv{x}{y}-P(y)x=Q(y),\]
which is recognized as linear in the variable $x$.
\ssc{Exact ODEs}
\sssc{Definition}
A first-order ODE of the form
\[M(x,y)\dd{x}+N(x,y)\dd{y}=0\]
with
\[\pdv{M}{y}=\pdv{N}{x}\]
in a certian rectangle $R\subseteq\bbR^2$ is said to be exact.

By Schwarz's Theorem, Young's Theorem, or Clairaut's Theorem, the criterion
\[\pdv{M}{y}=\pdv{N}{x}\]
holds if and only if there exists a function $F\colon R\to\bbR$ such that
\[\pdv{F}{x}=M,\quad\pdv{F}{y}=N,\]
and $\frac{\partial^2F}{\partial x\partial y}$.

Such $F$ is called the potential function.
\sssc{Solutions by integration and then differentiation with respect to different variables}
First, integrate $M(x,y)$ with respect to $x$:
\[F(x,y)=\int M(x,y)\dd{x}+\phi(y),\]
where $\phi(y)$ is an arbitrary constant of integration that may depend on $y$.

Second, differentiate with respect to $y$:
\[\pdv{F}{y}(x,y)=N(x,y)=\pdv{}{y}\qty(\int M(x,y)\dd{x})+\phi'(y).\]

Third, integrate
\[\phi'(y)=N(x,y)-\pdv{}{y}\qty(\int M(x,y)\dd{x})\]
with respect to $y$ to find $\phi(y)$.

Fourth, substitute $\phi(y)$ back to find the potential function $F(x,y)$, the implicit solutions is given by
\[F(x,y)=C,\]
where $C$ is an arbitrary constant.
\sssc{Integration factors to make non-exact first-order ODEs exact}
Given a first-order ODE of the form
\[M(x,y)\dd{x}+N(x,y)\dd{y}=0,\]
we define the integration factor $\mu(x,y)$ such that
\[\mu(x,y)M(x,y)\dd{x}+\mu(x,y)N(x,y)\dd{y}=0\]
is exact and with potential function $F$ such that
\[\pdv{F}{x}=\mu(x,y)M(x,y),\quad\pdv{F}{y}=\mu(x,y)N(x,y).\]

If
\[\frac{\pdv{M}{y}-\pdv{N}{x}}{N}\]
depends only on $x$, we can construct integration factor $\mu(x)$ by letting
\[\frac{\mu'(x)}{\mu(x)}=\frac{\pdv{M}{y}-\pdv{N}{x}}{N},\]
that is,
\[\mu(x)=e^{\int\frac{\pdv{M}{y}-\pdv{N}{x}}{N}\dd{x}}.\]
\begin{proof}
This is a separable ODE of $\mu$ with repect to $x$.

Let
\[P(x)=\frac{\pdv{M}{y}-\pdv{N}{x}}{N}.\]
\[\mu^{-1}\dd{\mu}=P(x)\dd{x}.\]
\[\ln|\mu|=\int P(x)\dd{x}.\]
\[\mu=e^{\int P(x)\dd{x}}.\]
\end{proof}

Similarly, if
\[\frac{\pdv{N}{x}-\pdv{M}{y}}{M}\]
depends only on $y$, we can construct integration factor $\mu(y)$ by letting
\[\frac{\mu'(y)}{\mu(y)}=\frac{\pdv{N}{x}-\pdv{M}{y}}{M},\]
that is,
\[\mu(y)=e^{\int\frac{\pdv{N}{x}-\pdv{M}{y}}{M}\dd{x}}.\]
\ssc{Homogeneous first-order ODEs}
\sssc{Definition}
A first-order ODE of the form
\[\dv{y}{x}=F(x,y)\]
with $F(x,y)$ be a homogeneous function of degree $0$ is said to be homogeneous. 

For first-order ODE of the form
\[M(x,y)\dd{x}+N(x,y)\dd{y}=0,\]
if $M(x,y)$ and $N(x,y)$ are homogeneous functions of the same degree, $F(x,y)=-\frac{M(x,y)}{N(x,y)}$ is a homogeneous function of degree $0$.
\sssc{Solutions by substitutions}
For any homogeneous function $F(x,y)$ of $2$ variables, there exists a function $f$ of $1$ variable such that $F(x,y)=f\qty(\frac{y}{x})$ for all $x\neq 0$ such that $(x,y)$ is in the domain of $F$.

Introduce the substitution $v=\frac{y}{x}$.
\[\dv{y}{x}=v+x\dv{v}{x}=f(v),\]
which is a separable ODE.
\[\frac{\dd{v}}{f(v)-v}=\frac{\dd{x}}{x}.\]
\[\int\frac{\dd{v}}{f(v)-v}=\ln|x|+C.\]
Substituting $v=\frac{y}{x}$ back gives the solutions.

For the singular solution when $x=0$, we define function $g(y)=F(0,y)$.
\[\dv{y}{x}=g(y),\]
which is a separable ODE that has the family of solutions
\[\int\frac{1}{g(y)}\dd{y}=x\]
and the constant solution
\[g(y)=0.\]
\ssc{Bernoulli differential equations}
A first-order ODE of the form
\[\dv{y}{x}+P(x)y=Q(x)y^n,\quad n\in\mathbb{R}\setminus\{0,1\}\]
is called a Bernoulli differential equation.
\sssc{Solutions by substitutions}
Divide both sides by $y^n$:
\[y^{-n}\dv{y}{x}+P(x)y^{1-n}=Q(x).\]
Introduce the substitution $v=y^{1-n}$.
\[\dv{v}{x}=(1-n)y^{-n}\dv{y}{x}.\]
Substitute into the equation:
\[\frac{1}{1-n}\dv{v}{x}+P(x)v=Q(x).\]
\[\dv{v}{x}+(1-n)P(x)v=(1-n)Q(x),\]
which is linear.

Use the integrating factor:
\[\mu(x)=e^{\int (1-n)P(x)\dd{x}}.\]
Multiply both sides with $\mu(x)$:
\[\mu(x)\dv{v}{x}+\mu(x)(1-n)P(x)v=\mu(x)(1-n)Q(x).\]
The left-hand side is exactly a derivative:
\[\dv{}{x}\qty(\mu(x)v(x))=\mu(x)(1-n)Q(x).\]
Integrate:
\[\mu(x)v(x)=\int\mu(x)(1-n)Q(x)\dd{x}+C.\]
\[v(x)=\frac{1}{\mu(x)}\qty(\int\mu(x)(1-n)Q(x)\dd{x}+C).\]
Recall $v=y^{1-n}$, we obtain the family of general solutions.

PLACEHOLDER: For F(x,y)=f(ax+by+c) solutions by substitution 



\ssc{General method for reducing first-order ODE to separable ODE with substitution}
Consider a first-order ODE of the form
\[\dv{y}{x}=F(x,y).\]
Take functions $u(x,y)$ and $g(u)$ that satisfy
\begin{itemize}
    \item there exists functions $p(x,u)=y$ and $r(u)s(x)=\pdv{p}{u}\neq 0$,
    \item there exists a function $q$ such that $q(u)=\pdv{p}{x}$, and
    \item $g(u)=F(x,y)$
\end{itemize}
almost everywhere in the domain of $F$.

Substitute:
\[q(u)+r(u)s(x)\dv{u}{x}=g(u).\]
\[\frac{r(u)}{g(u)-q(u)}\dd{u}=\frac{1}{s(x)}\dd{x}.\]
This is a separable first-order ODE. By integration, we obtain a family of solutions:
\[\int\frac{r(u)}{g(u)-q(u)}\dd{u}=\int\frac{1}{s(x)}\dd{x}+C.\]
And $g(u)-q(u)=0$ is a constant solution.

Substituting $u(x,y)$ back gives solutions for $y$.

Finally, consider the singular solutions at points where the conditions fail.
\ssc{Dynamical system (動態系統 or 動力系統)}
\sssc{Dynamical system}
A dynamical system is a tuple $(T, X, \Phi)$ where $T$ is a monoid, written additively and with the independent variable $t$ in it called evolution parameter (演化參數) or time, $X$ is a non-empty set, called the phase space (相空間) or state space (狀態空間) with the independent variable $x$ in it called (system) phase (相) or state (狀態), and $\Phi$ is a function, called the evolution function (演化函數) or flow,
\[\Phi(t,x)\colon U\subseteq(T\times X)\to X\]
where $U$ is such that the second projection map $\pi_2$ for $T\times X$ satisfies
\[\pi_2(U)=X,\]
and for any $x \in X$, it satisfies
\bit
\item identity:
\[\Phi (0,x)=x,\]
\item semigroup property:
\[\Phi (t_{2},\Phi (t_{1},x))=\Phi (t_{2}+t_{1},x)\]
for any $t_{1},t_{2}+t_{1}\in I(x)$ and $t_{2}\in I(\Phi (t_{1},x))$i, in which
\[I(x)\coloneq\{t\in T\mid(t,x)\in U\}\]
for any $x\in X$.
\eit
\sssc{Orbit (軌道)}
Given a dynamical system $(T, X, \Phi)$ with
\[\Phi(t,x)\colon U\subseteq(T\times X)\to X\]
and
\[I(x)\coloneq\{t\in T\mid(t,x)\in U\}.\]
The set
\[\gamma _{x}\coloneq\{\Phi (t,x)\colon t\in I(x)\}\subset X\]
is called the orbit through $x$.

An orbit which consists of a single point is called constant orbit; a non-constant orbit is called closed or periodic if there exists $t\neq 0\in I(x)$ such that $\Phi (t,x)=x$.
\sssc{Phase portrait (相圖)}
Given a dynamical system $(T, X, \Phi)$, the phase portrait is a geometric representation of the collection of all orbits $\gamma$ through any $x\in X$.
\sssc{Attractor (吸引子)}
For a dynamical system $(\mathbb{R}, X, \Phi)$ with $D\subseteq X$ and $\Phi(t,x)\colon \mathbb{R}_{\geq 0}\times D\to X$, a non-empty subset $A$ of $D$ is called an attractor if and only if it satisfies the following three conditions:
\bit
\item Forward invariance:
    \[\forall a\in A\colon\forall t>0\colon\Phi(t,a)\in A.\]
\item Attraction: There exists a neighborhood of $A$, called the basin of attraction for $A$ and denoted $B(A)$, such that for any $b\in B(A)$ and any open neighborhood $N$ of $A$, there is a positive constant $T$ such that $\forall t>T\colon\Phi(t,b)\in N$.
\item Minimality: There is no non-empty proper subset of $A$ that satisfies the above two conditions.
\eit

A point $a$ in $D$ is called an attractor if and only if $\{a\}$ is an attractor.
\sssc{Repeller}
For a dynamical system $(\mathbb{R}, X, \Phi)$ with $D\subseteq X$ and $\Phi(t,x)\colon \mathbb{R}_{\geq 0}\times D\to X$, a non-empty subset $R$ of $X$ is called a repeller if and only if it satisfies the following three conditions:
\bit
\item Forward invariance:
    \[\forall r\in R\colon\forall t>0\colon\Phi(t,r)\in R.\]
\item Repulsion: There exists a neighborhood of $R$, called the basin of repulsion for $R$ and denoted $B(R)$, such that for any $b\in B(R)\setminus R$ and any open neighborhood $N$ of $R$, there is a positive constant $T$ such that $\forall t>T\colon\Phi(t,b)\notin N$.
\item Minimality: There is no non-empty proper subset of $R$ that satisfies the above two conditions.
\eit

A point $r$ in $D$ is called a repeller if and only if $\{r\}$ is a repeller.
\sssc{Continuous-time dynamical system}
A dynamical system $(\mathbb{R}, \mathbb{R}^n, \Phi)$ with $A=\mathbb{R}_{\geq 0}$ (or $\mathbb{R}$), $B\subseteq\mathbb{R}^n$, and $\Phi(t,x)\colon A\times B\to\mathbb{R}^n$ is called a continuous-time dynamical system if $\pdv{\Phi}{t}\big\vert_{t=0}$ exists for all $X\in B$.
\sssc{Autonomous or time-invariant continuous-time dynamical system}
A continuous-time dynamical system $(\mathbb{R}, \mathbb{R}^n, \Phi)$ is called autonomous, aka time-invariant, if $\pdv{\Phi(t,x)}{t}\big\vert_{t=0}$ is only dependent on $x$.
\sssc{Linear (autonomous) continuous-time dynamical system}
An autonomous continuous-time dynamical system $(\mathbb{R}, \mathbb{R}^n, \Phi(t,x))$ with evolution rule $\dv{x}{t}=f(x)$ is called linear if $f(x)=Ax$ where $A$ is a constant matrix in $\mathbb{R}^{n\times n}$.

The eigenvalues and eigen vectors of $A$ are called the eigenvalues and eigen vectors of the linear autonomous continuous-time dynamical system.
\sssc{Phase portrait}
For an autonomous continuous-time dynamical system, one usually plots the space $X$, marks equilibria, and draw arrows at each point $x$ in a grid indicating $f(x)$ at it.

Specifically, a phase portrait for an autonomous continuous-time dynamical system in which $X=\mathbb{R}$ is also called a phase line.
\sssc{Equilibria, fixed points, or fixpoints of autonomous continuous-time dynamical systems}
An equilibrium, fixed point, or fixpoint of an autonomous continuous-time dynamical system $(T, X, \Phi)$ is a point $x^*\in X$ such that the vector field $f(x)=\pdv{\Phi}{t}\big\vert_{t=0}$ vanishes there, that is, $f(x^*)=0$.
\sssc{Systems of first-order ODEs as continuous-time dynamical systems}
Consider a system of first-order ODEs:
\[\dv{x}{t}=f(t,x), \quad x\in\mathbb{R}^n\land f\colon\mathbb{R}\times\mathbb{R}^n\to\mathbb{R}^n.\]

Let $U\subseteq\mathbb{R}\times\mathbb{R}^n$ be the set such that for any $(t_0,x_0)$ in $U$, $t_0$ is in the interval of definition of the solution of it with the initial condition $x(0)=x_0$ and that the solution is unique, and $x(t;x_0)$ be the solution of it with the initial condition $x(0)=x_0$.

Then the dynamical system represented by it has phase space $\mathbb{R}^n$, evolution parameter $t$, phase or state $x$, and evolution function $\Phi\colon U\to\mathbb{R}^n;\Phi(t,x_0)=x(t;x_0)$. The system of ODEs is called the evolution rule of the dynamical system, and the dynamical system is called to be defined by the system of ODEs.
\sssc{Autonomous system of ODEs as autonomous continuous-time dynamical systems}
Consider an autonomous system of ODEs:
\[\dv{x}{t}=f(x), \quad x\in\mathbb{R}^n\land f\colon\mathbb{R}^n\to\mathbb{R}^n.\]

Let $U\subseteq\mathbb{R}\times\mathbb{R}^n$ be the set such that for any $(t_0,x_0)$ in $U$, $t_0$ is in the interval of definition of the solution of it with the initial condition $x(0)=x_0$ and that the solution is unique, and $x(t;x_0)$ be the solution of it with the initial condition $x(0)=x_0$.

Then the dynamical system represented by it has phase space $\mathbb{R}^n$, evolution parameter $t$, phase or state $x$, and evolution function $\Phi\colon U\to\mathbb{R}^n;\Phi(t,x_0)=x(t;x_0)$. The system of ODEs is called the evolution rule of the dynamical system, and the dynamical system is called to be defined by the system of ODEs. The equilibria of the dynamical system are the critical points of the system of ODEs.
\sssc{Stability of equilibria}
Let $x^*$ be an equilibrium of an autonomous continuous-time dynamical system defined by the evolution rule $\dv{x}{t}=f(x)$ and $A=\mathbb{R}_{\geq 0}$ or $\mathbb{R}$ be the domain of the evolution function. Then
\bit
\item $x^*$ is called to be (Lyapunov) stable (（李亞普諾夫）穩定) if and only if
    \[\forall\varepsilon>0\colon\exists\delta>0\text{\ s.t.\ }\|x(0)-x^*\|<\delta\implies\forall t\in A\colon\|x(t)-x^*\|<\varepsilon.\]
\item $x^*$ is called to be asymptotically stable if and only if
    \[\exists\delta>0\text{\ s.t.\ }\|x(0)-x^*\|<\delta\implies\lim_{t\to\infty}x(t)=x^*.\]
An asymptotically stable (漸近穩定) equilibrium is necessarily (Lyapunov) stable and an attractor.
\item $x^*$ is called to be exponentially (指數穩定) stable if and only if
    \[\exists M>0,\alpha>0,\delta>0\text{\ s.t.\ }\|x(0)-x^*\|<\delta\implies\forall t\in A\|x(t)-x^*\|\leq\|x(0)-x^*\|e^{-\alpha t}.\]
    An exponentially stable equilibrium is necessarily asymptotically stable.
\item for $n=1$, $x^*$ is called to be semi-stable or semistable if and only if it is (Lyapunov) stable and
    \[(\exists\delta>0\text{\ s.t.\ }x(0)-x^*<\delta\implies\lim_{t\to\infty}x(t)=x^*)\lor (\exists\delta>0\text{\ s.t.\ }x^*-x(0)<\delta\implies\lim_{t\to\infty}x(t)=x^*).\]
\item for linear autonomous continuous-time dynamical system, $x^*$ is called to be marginally stable (臨界穩定) if it is (Lyapunov) stable but not asymptotically stable,
\item $x^*$ is called to be unstable (不穩定) if and only if it is not (Lyapunov) stable.

An equilibrium that is in a repeller is necessarily unstable.
\eit
\end{document}

