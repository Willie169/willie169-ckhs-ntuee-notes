\documentclass[a4paper,12pt]{report}
\setcounter{secnumdepth}{5}
\setcounter{tocdepth}{3}
\input{/usr/share/latex-toolkit/template.tex}
\begin{document}
\title{Discrete-Time Markov Chain (DTMC) and Discrete-Time Markov Decision Process (DTMDP)}
\author{沈威宇}
\date{\temtoday}
\titletocdoc
\section{Discrete-Time Markov Chain (DTMC) (離散時間馬可夫鏈) and Discrete-Time Markov Decision Process (DTMDP) (離散時間馬可夫決定過程)}
\subsection{Stochastic matrix (隨機矩陣), transition matrix (轉移矩陣), probability matrix (機率矩陣), transition probability matrix (轉移機率矩陣), substitution matrix (替代矩陣), or Markov matrix (馬可夫矩陣)}
\begin{itemize}
\item (Left) stochastic matrix, (left) transition matrix, (left) probability matrix, (left) transition probability matrix, (left) substitution matrix, or (left) Markov matrix: A square matrix in which the sum of each row is 1 and each element is greater than or equal to zero.
\item Right stochastic matrix, right transition matrix, right probability matrix, right transition probability matrix, right substitution matrix, or right Markov matrix: A square matrix in which the sum of each column is 1 and each element is greater than or equal to zero.
\item Doubly stochastic matrix, doubly transition matrix, doubly probability matrix, doubly transition probability matrix, doubly substitution matrix, or doubly Markov matrix: A matrix that is both a left stochastic matrix and a right stochastic matrix.
\item Closed property (封閉性) of left stochastic matrix: The matrix product of several left stochastic matrices, the arithmetic mean of several left stochastic matrices, and the positive integer power of a left stochastic matrix are all stochastic matrices.
\item Closed property of right stochastic matrix: The matrix product of several right stochastic matrices, the arithmetic mean of several right stochastic matrices, and the positive integer power of a right stochastic matrix are all stochastic matrices.
\item Regularity: A left or right stochastic matrix $\mb{A}$ is regular if there exists at least one $n\in\mathbb{N}_0$ such that every entries in $\mb{A}^n$ is positive.
\item Singularity: A left or right stochastic matrix is singular if it is not regular.
\end{itemize}
\subsection{Discrete-time Markov Chain (DTMC)}
\begin{itemize}
\item State $s$: The state of the agent with respect to the environment.
\item State space $S$: The set of all possible states.
\item State transition: The transition from a state to next state.
\item State transition probability $p(s'|s)$: A probability mass function that defines the likelihood of an agent transitioning to $s'$ from $s$.
\item (State) transition (probability) matrix $P$: Suppose the states could be indexed as $s_i\left(_{i=1}^n\right)$. The state transition matrix is defined to be:
\[P\in[0,1]^{n\times n}\land P_{ij}:=p(s_j|s_i).\]
It satisfies:
\[\forall i\in\mathbb{N}\leq n\colon\sum_{j=1}^nP_{ij}=1\]
\item Trajectory: A finite or infinite state chain that an agent can take. In a trajectory, the $i$th state is called $s_{i-1}$, making the trajectory $s_0s_1\ldots s_{i-1}s_i\ldots$.
\item System model, Transition model, or Model: the state transition probability of each state of a DTMC.
\item Discrete-time Markov chain (DTMC) $(S,p(s'|s))$: A stochastic process describing a sequence of possible events in which the probability of each event depends only on the current state in the previous event. A DTMC is given by a two-tuple of the state space $S$ and state transition probability $p(s'|s)$, that the following property, called Markov property (馬可夫性) or memoryless property (無記憶性或無後效性), holds:
\[p(s_{t+1}|s_t,s_{t-1},\ldots,s_0)=p(s_{t+1}|s_t).\]
\item Irreducibility (不可約性): a DTMC is irreducible if for any two states $s_i$ and $s_j$, there exists at least one positive integer $k$ such that $(P^k)_{ij}>0$. A DTMC is irreducible if and only if its state transition matrix $P$ is a regular stochastic matrix.
\item Period (週期): the period of a state $s_i$ in $S$ is the greatest common divisor of all positive integer $k$ such that $(P^k)_{ii}>0$.
\item Aperiodicity (非週期性): a state is aperiodic if its period is 1; a DTMC is aperiodic if all states in its state space is aperiodic.
\item Steady state (穩定狀態或穩態): A probability distribution $\pi$ over $S$ such that $\pi P = \pi$. If a DTMC is irreducible and aperiodic, then it has a unique steady-state distribution $\pi$, and for any probability distribution $\pi'$ over $S$:
\[\lim_{k\to\infty}\pi'P^k=\pi\]
\end{itemize}
\subsection{Discrete-time Markov decision process (DTMDP)}
\begin{itemize}
\item State $s$: The state of the agent with respect to the environment.
\item State space $S$: The set of all possible states.
\item State transition: The transition from a state to next state.
\item Action: A choice the agent can make to interact with the environment, changing its state.
\item Action space of a state $A_s$: The set of all possible actions of a state $s$.
\item (State) transition (probability) matrix (of an action) $p(s'|s,a)$: A probability mass function that defines the likelihood of an agent transitioning to $s'$ given that the agent takes an action $a$ in a state $s$.
\item Policy $\pi(a|s)$: A policy is a probability function from the state space to the action spaces an agent follows to select actions based on its current state. It defines the conditional probability of the agent taking action $a$ when in state $s$.
\item Deterministic policy: A policy is deterministic if, for each state $s$, there exists exactly one action $a$ such that $\pi(a|s)=1$ and $\pi(a'|s)=0$ for all other actions $a'\neq a$.
\item Stochastic policy: A policy that is not deterministic.
\item State transition probability (given a policy) $p_{\pi}(s'|s)$:
\[p_{\pi}(s'|s):=\sum_a\pi(a|s)p(s'|s,a).\]
\item State transition matrix (given a policy) $P_{\pi}$: Suppose the states could be indexed as $s_i\left(_{i=1}^n\right)$. The state transition matrix of a policy $\pi$ is defined to be:
\[P_{\pi}\in[0,1]^{n\times n}\land (P_{\pi})_{ij}:=p_{\pi}(s_j|s_i).\]
It satisfies:
\[\forall i\in\mathbb{N}\leq n\colon\sum_{j=1}^nP_{ij}=1\]
\item Reward $r$: A real number the agent gets after taking a action.
\item Reward transition probability $p(r|s,a)$: A probability mass function that defines the likelihood of an agent receiving reward $r$ given that the agent takes an action $a$ in a state $s$.
\item Reward (given a policy) $r_{\pi}(s)$:
\[r_{\pi}(s):= E[r|S_t=s,A_t=a]=\sum_a\pi(a|s)\sum_rp(r|s,a)r.\]
\item Reward vector (given a policy) $r_{\pi}$: Suppose the states could be indexed as $s_i\left(_{i=1}^n\right)$. The reward vector of a policy $\pi$ is defined to be:
\[r_{\pi}:=[r_{\pi}(s_i)\left(_{i=1}^n\right)]^{\top}\in\mathbb{R}^n.\]
\item Trajectory: A finite or infinite state-action-reward chain that an agent can take by taking a chain of actions in the action space of the state it's in, moving along a chain of states in the state space, and receiving rewards along the way. In a trajectory, the $i$th state is called $s_{i-1}$, the $i$th action taken is called $a_{i-1}$, the $i$th reward received is called $r_i$, making the trajectory $s_0\xrightarrow[r_1]{a_0}s_1\ldots s_{i-1}\xrightarrow[r_i]{a_{i-1}}s_i\ldots$.
\item Return: The sum of all rewards the agent receives along a trajectory.
\item Discounted return $G_t$: The discounted reward $G_t$ at step $t$ given that the discount rate is $\gamma\in[0,1)$, the reward at step $i$ is $r_i$, and the final step in the trajectory is the step $t+n$ ($n=\infty$ for infinite trajectory), is defined to be:
\[G_t:=\sum_{i=0}^n\gamma^ir_{t+i}.\]
\item Terminal state: The state that the agent is in after its last action in a finite trajectory.
\item Episode or Trial: A trajectory with a terminal state.
\item Episodic task: A task with a terminal state.
\item Continuing task: A task without a terminal state.
\item Target state: The terminal state in a finite trajectory, or the state that the agent stays in since a specific action is taken and that the agent takes a same action that doesn't change its state afterwards in an infinite trajectory. Not all infinite trajectories have a target state.
\item Absorbing state: A target state in an infinite trajectory that any action of the agent after it yields zero reward. Not all infinite trajectories have an absorbing state.
\item System model, Transition model, or Model: the state transition probability of each action in the action space of each state, and the reward transition probability of each action in the action space of each state of a DTMDP.
\item Discrete-time Markov decision process (DTMDP) $(S,A_s,p(s'|s,a),p(r|s,a))$: A DTMDP is given by a four-tuple of the state space $S$, the action spaces $A_s$ of each state $s$, the state transition probability $p(s'|s,a)$ of each action $a$ in the action space of each state $s$, and the reward transition probability $p(r|s,a)$ of each action $a$ in the action space of each state $s$, that the following property, called Markov property or memoryless property, holds:
\[\begin{aligned}
& p(S_{t+1}|A_t,S_t,A_{t-1},S_{t-1},\ldots,A_0,S_0)=p(S_{t+1}|A_t,S_t),\\
& p(R_{t+1}|A_t,S_t,A_{t-1},S_{t-1},\ldots,A_0,S_0)=p(R_{t+1}|A_t,S_t).
\end{aligned}\]
If a policy is given, a DTMDP becomes a DTMC.
\item State value $v_{\pi}(s)$: The state value, a function of state $s$ given the policy $\pi$, is the expected value of the discounted return $G_t$ given $S_t=s$, that is,
\[v_{\pi}(s):= E[G_t|S_t=s].\]
\item State value vector $v_{\pi}$: Suppose the states could be indexed as $s_i\left(_{i=1}^n\right)$. The state value vector of a policy $\pi$ is defined to be:
\[v_{\pi}:=[v_{\pi}(s_i)\left(_{i=1}^n\right)]^{\top}\in\mathbb{R}^n.\]
\item Policy evaluation: Given a policy, finding out the corresponding state values of all states is called policy evaluation.
\item Action value or Q-value $q_{\pi}(s,a)$ or $Q^{\pi}(s,a)$: The action value, a function of state-action pair $(s,a)$ given the policy $\pi$, is the expected value of the discounted return $G_t$ given $S_t=s$ and $A_t=a$, that is,
\[q_{\pi}(s,a):= E[G_t|S_t=s,A_t=a]= E[R_{t+1}+\gamma v_{\pi}(S_{t+1})|S_t=s,A_t=a].\]
\item Optimal Policy $\pi^*$: Given two policies $\pi_1$ and $\pi_2$, if
\[v_{\pi_1}(s)\geq v_{\pi_2}(s),\quad\forall s\in S,\]
then we say $\pi_1$ is "better" than $\pi_2$.\\
A policy $\pi^*$ is optimal if for any other policy $\pi$
\[v_{\pi^*}(s)\geq v_{\pi}(s),\quad\forall s\in S.\]
Given a DTMDP, there must exist an optimal policy, but it is not necessarily unique.
\end{itemize}
\subsection{Bellman Equation}
\subsubsection{Bellman Equation Elementwise Form}
The elementwise form of the Bellman equation of a given policy $\pi$ is
\[\begin{aligned}
v_{\pi}(s)=& E[G_t|S_t=s]\\
=& E[R_{t+1}|S_t=s]+\gamma E[G_{t+1}|S_t=s]\\
=&\sum_a\pi(a|s)\sum_rp(r|s,a)r+\gamma\sum_a\pi(a|s)\sum_{s'}p(s'|s,a)v_{\pi}(s')\\
=&\sum_a\pi(a|s)\left(\sum_rp(r|s,a)r+\gamma\sum_{s'}p(s'|s,a)v_{\pi}(s')\right)\\
=&r_{\pi}(s)+\gamma\sum_{s'}p_{\pi}(s'|s)v_{\pi}(s'),\quad\forall s\in S.
\end{aligned}\]
\subsubsection{Bellman Equation Matrix-Vector Form}
The matrix-vector form of the Bellman equation of a given policy $\pi$ is
\[v_{\pi}=r_{\pi}+\gamma P_{\pi}v_{\pi}.\]
\subsubsection{Bellman Equation Closed Form Solution}
\[v_{\pi}=(I-\gamma P_{\pi})^{-1}r_{\pi}.\]
\subsubsection{Bellman Equation Iterative Solution Matrix-Vector Form}
Consider a sequence $\{v_k\}$ where $v_0$ is any arbitrary vector $\in\mathbb{R}^{|S|}$, and
\[v_k=r_{\pi}+\gamma P_{\pi}v_{k-1},\quad k\in\mathbb{N},\]
then
\[v_{\pi}=\lim_{k\to\infty}v_k.\]
In practice, we usually stop when $\|v_k-v_{k-1}\|$ is sufficiently small or when $k$ is sufficiently large.
\subsubsection{Bellman Equation Iterative Solution Elementwise Form}
Consider a sequence $\{v_k(s)\}$ where $v_0(s)$ is any arbitrary value, and
\[v_k=\sum_a\pi(a|s)\left(\sum_rp(r|s,a)r+\gamma\sum_{s'}p(s'|s,a)v_{k-1}(s)\right),\quad k\in\mathbb{N},\]
then
\[v_{\pi}(s)=\lim_{k\to\infty}v_k.\]
In practice, we usually stop when $|v_k(s)-v_{k-1}(s)|$ is sufficiently small or when $k$ is sufficiently large.
\subsubsection{Value Improvement Theorem}
Consider a sequence $\{v_k\}$ where $v_0$ is any arbitrary vector $\in\mathbb{R}^{|S|}$, and
\[v_k=r_{\pi}+\gamma P_{\pi}v_{k-1},\quad k\in\mathbb{N},\]
then
\[v_{k+1}\geq v_k.\] 
\subsection{State-Action Value Function or Value Function (VF)}
Compare
\[v_{\pi}(s)=\sum_a\pi(a|s)q_{\pi}(s,a)\]
and the Bellman equation, we have the action-value function:
\[q_{\pi}(s,a)=\sum_rp(r|s,a)r+\gamma\sum_{s'}p(s'|s,a)v_{\pi}(s').\]
\subsection{Contraction Mapping Theorem}
\subsubsection{Fixed Point}
Given $f\colon X\to X$, $x\in X$ is a fixed point if
\[f(x)=x.\]
\subsubsection{Contraction Mapping or Contractive Function}
$f\colon X\to X$ is a contraction mapping if
\[\exists\gamma\in[0,1)\colon\|f(x_1)-f(x_2)\|\leq\gamma\|x_1-x_2\|,\quad\forall x_1,x_2\in X,\]
where $\|\cdot\|$ can be any vector norm.
\subsubsection{Contraction Mapping Theorem}
For any contraction mapping,
\begin{itemize}
\item Existence: there exists a fixed point $x^*$ satisfying $f(x^*)=x^*$.
\item Uniqueness: the fixed point $x^*$ is unique.
\item Algorithm: Consider a sequence $\{x_k\}$ where $x_0$ is any arbitrary value and $x_{k+1}=f(x_k),\quad k\in\mathbb{N}$, then
\[\lim_{k\to\infty}x_k=x^*.\]
Moreover, the convergence rate is exponential and determined by $\gamma$.
\end{itemize}
\subsection{Bellman Optimality Equation (BOE)}
\subsubsection{Bellman Optimality Equation Elementwise Form}
\[v(s)=\max_{\pi}\left(\sum_a\pi(a|s)q(s,a)\right),\quad s\in S.\]
\subsubsection{Bellman Optimality Equation Matrix-Vector Form}
\[v=\max_{\pi}\left(r_{\pi}+\gamma P_{\pi}v_{\pi}\right).\]
\subsubsection{Value Iteration (VI) Matrix-Vector Form}
Let
\[f(v)=\max_{\pi}\left(r_{\pi}+\gamma P_{\pi}v\right),\]
where $v$ is a state value given policy $\pi$.\\
Because $f(v)$ is a contraction mapping, it satisfies the contraction mapping theorem, that is,
\begin{itemize}
\item Existence and uniqueness:
\[\exists!v^*\text{\ such that\ }v^*=f(v^*),\]
\item Iterative algorithm: Consider a sequence $\{v_k\}$ where $v_0$ is any arbitrary value, and $v_k=f(v_{k-1}),\quad k\in\mathbb{N}$. It converges to $v^*$ in an exponential rate determined by $\gamma$ as $k$ approaching $\infty$.
\end{itemize}
One iteration in the value iteration algorithm,
\[v_{k+1}=f(v_k)=\max_{\pi}\left(r_{\pi}+\gamma P_{\pi}v_k\right),\quad k+1\in\mathbb{N},\]
can be decomposed into two steps,
\begin{enumerate}
\item \textbf{Policy update (PU)}: Solve 
\[\pi_{k+1}=\arg\max_{\pi}(r_{\pi}+\gamma P_{\pi}v_k)\]
for $\pi_{k+1}$ given $v_k$.
\item \textbf{Value update (VU)}: Solve 
\[v_{k+1}=r_{\pi_{k+1}}+\gamma P_{\pi_{k+1}}v_k),\]
for $v_{k+1}$ given $\pi_{k+1}$ and $v_k$.
\end{enumerate}
In practice, we usually stop when $\|v_k-v_{k-1}\|$ is sufficiently small or when $k$ is sufficiently large.
\subsubsection{Value Iteration (VI) Elementwise Form}
In elementwise form, the two steps of value iteration can be written as,
\begin{enumerate}
\item \textbf{Policy update (PU)}: Solve
\[\pi_{k+1}(s)=\arg\max_{\pi}\sum_a\pi(a|s)q_{\pi_k}(s,a),\quad s\in S\]
for $\pi_{k+1}(s)$ given $v_k(s')$ for all $s'\in S$.\\
Let $a^*_k(s)=\arg\max_aq_{\pi_k}(s,a)$. We select
\[\pi_{k+1}(a|s)=\begin{cases}1,\quad a=a^*_k(s)\\0,\quad a\neq a^*_k(s)\end{cases},\]
called "greedy policy" because it simply selects the greatest policy value.
\item \textbf{Value update (VU)}: Solve
\[v_{k+1}(s)=\sum_a\pi_{k+1}(a|s)q_{\pi_k}(s,a),\quad s\in S\]
for $v_{k+1}(s)$ given $\pi_{k+1}(a|s)$ for all $a\in A_s$ for all $s\in S$, and $v_k(s')$ for all $s'\in S$.\\
Since $\pi_{k+1}(a|s)$ is greedy,
\[v_{k+1}(s)=\max_aq_{\pi_k}(s,a).\]
\end{enumerate}
In practice, we usually stop when $|v_k(s)-v_{k-1}(s)|$ is sufficiently small or when $k$ is sufficiently large.
\subsubsection{Optimality Theorem}
Suppose $v^*$ is the solution to a Bellman optimality equation, that is, 
\[v^*=\max_{\pi}\left(r_{\pi}+\gamma P_{\pi}v^*\right).\]
Suppose
\[\pi^*=\arg\max_{\pi}\left(r_{\pi}+\gamma P_{\pi}v^*\right).\]
Then 
\[v^*=r_{\pi^*}+\gamma P_{\pi^*}v^*.\]
$v^*$ is the optimal state value, and $\pi^*$ is the optimal policy.
\subsubsection{Optimal Policy Invariance Theorem}
Consider a Markov decision process with $v^*\in\mathbb{R}^{|S|}$ as the optimal state value satisfying $v^*=\max_{\pi}(r_{\pi}+\gamma P_{\pi}v^*$. If every reward $r$ is changed by an affine transformation to $ar+b$, where $a,b\in\mathbb{R}$ and $a\neq 0$, then the corresponding optimal state value $v'$ is also an affine transformation of $v^*$:
\[v'=av^*+\frac{b}{1-\gamma}\mathbf{1},\]
where $\gamma\in[0,1)$ is the discount rate and $\mathbf{1}=[1,\ldots,1]^{\top}\in\mathbb{R}^{|S|}$.\\
Consequently, the optimal policies are invariant to any affine transformation of the reward signals.
\subsubsection{Policy Improvement Theorem}
If 
\[\pi_{k+1}=\arg\max_{\pi}(r_{\pi}+\gamma P_{\pi}v_{\pi_k}),\]
then $\|v_{\pi_{k+1}}\|\geq\|v_{\pi_k}\|$ for any $k$.
\subsubsection{Convergence of Policy Iteration Theorem}
The state value sequence $\{v_{\pi_k}\}_{k=0}^{\infty}$ generated by the policy iteration algorithm converges to the optimal state value $v^*$. Consequently, the policy sequc $\{\pi_k\}_{k=0}^{\infty}$ coverages to an optimal policy.
\subsubsection{Policy Iteration (PI) Matrix-Vector Form}
An arbitrary initial policy $\pi_0$ is given. One iteration in the policy iteration algorithm can be decomposed into two steps,
\begin{enumerate}
\item \textbf{Policy evaluation (PE)}: Solve the Bellman equation
\[v_{\pi_k}=r_{\pi_k}+\gamma P_{\pi_k}v_{\pi_k}\]
for $v_{\pi_k}$ given $\pi_k$.
\item \textbf{Policy improvement (PI)}: Solve
\[\pi_{k+1}=\arg\max_{\pi}(r_{\pi}+\gamma P_{\pi}v_{\pi_k})\]
for $\pi_{k+1}$ given $v_{\pi_k}$.
\end{enumerate}
In practice, we usually stop when $\|v_{\pi_k}-v_{\pi_{k-1}}\|$ is sufficiently small or when $k$ is sufficiently large.
\subsubsection{Policy Iteration (PI) Elementwise Form}
In elementwise form, the two steps of policy iteration can be written as,
\begin{enumerate}
\item \textbf{Policy evaluation (PE)}: Solve the Bellman equation
\[v_{\pi_k}(s)=\sun_a\pi(a|s)q_{\pi_k}(s,a),\quad s\in S\]
for $v_{\pi_k}$ given $\pi_k$.
\item \textbf{Policy improvement (PI)}:\\
Solve
\[\pi_{k+1}(s)=\arg\max_{\pi}\sum_a\pi(a|s)q_{\pi_k}(s,a),\quad s\in S\]
for $\pi_{k+1}(s)$ given $v_{\pi_k}(s)$.\\
Let $a^*_k(s)=\arg\max_aq_{\pi_k}(s,a)$. We select
\[\pi_{k+1}(a|s)=\begin{cases}1,\quad a=a^*_k(s)\\0,\quad a\neq a^*_k(s)\end{cases},\]
called "greedy policy" because it simply selects the greatest policy value.
\end{enumerate}
In practice, we usually stop when $|v_{\pi_k}(s)-v_{\pi_{k-1}}(s)|$ is sufficiently small or when $k$ is sufficiently large.
\subsubsection{Truncated Policy Iteration}
The truncated policy iteration is the same as policy iteration with the policy evaluation step using the iterative solution but stopped when $\|v_k-v_{k-1}\|$ is sufficiently small or when $k$ is sufficiently large. If stopping when $k=1$, the truncated policy iteration becomes value iteration except that the first iteration lacks value update and is initialized with an arbitrary policy; if stopping when $k=\infty$, the truncated policy iteration becomes policy iteration.
\end{document}
