\documentclass[a4paper,12pt]{report}
\setcounter{secnumdepth}{5}
\setcounter{tocdepth}{3}
\input{/usr/share/latex-toolkit/template.tex}
\begin{document}
\title{Linear Algebra}
\author{沈威宇}
\date{\temtoday}
\titletocdoc
\chapter{Linear Algebra (線性代數)}
\section{Matrix (矩陣)}
\subsection{Matrix}
A matrix is a rectangular array of numbers (or other mathematical objects), called the entries (元或元素) of the matrix. Most commonly, a matrix over a field $F$ is a rectangular array of elements of $F$.
\sssc{Size}
The size of a matrix is defined by the number of rows (zh-hant: 列/zh-hans: 行) and columns (zh-hant: 行/zh-hans: 列) it contains. A matrix with $m$ rows and $n$ columns is called an $m\times n$ matrix, or $m$-by-$n$ matrix, where $m$ and $n$ are called its dimensions (階). A matrix with the same number of rows and columns $n$ is called a $n$ square matrix (方陣). A matrix with an infinite number of rows or/and columns is called an infinite matrix.

The set of all $m\times n$ matrices over $\mathbb{F}$ is commonly denoted as $\mathbb{F}^{m\times n}$.
\sssc{Notation}
The entry in the $i$-th row and $j$-th column, called the $i,j$th entry (第$i,j$元) or $ij$th entry (第$ij$元), is commonly denoted as $a_{i,j}$ or $a_{ij}$.

An $m\times n$ matrix $\mathbf{A}$ is commonly denoted as:
\[\mathbf{A}=\begin{bmatrix}a_{11} & a_{12} & \ldots & a_{1n}\\a_{21} & a_{22} & \ldots & a_{2n}\\\vdots & \vdots & \ddots & \vdots\\a_{m1} & a_{m2} & \ldots & a_{mn}\end{bmatrix},\]
\[\mathbf{A}=\begin{pmatrix}a_{11} & a_{12} & \ldots & a_{1n}\\a_{21} & a_{22} & \ldots & a_{2n}\\\vdots & \vdots & \ddots & \vdots\\a_{m1} & a_{m2} & \ldots & a_{mn}\end{pmatrix},\]
\[\mathbf{A}=[a_{ij}]_{1\leq i\leq m,1\leq j\leq n},\]
\[\mathbf{A}=(a_{ij})_{1\leq i\leq m,1\leq j\leq n},\]
\[\mathbf{A}=[a_{ij}]_{m\times n},\]
or
\[\mathbf{A}=(a_{ij})_{m\times n}.\]

The $i,i$th entries are called diagonal entries (對角線元).

A matrix with only one row is called a row matrix or row vector, and a matrix with only one column is called a column matrix or column vector.

The $i$th row of the matrix $\mathbf{A}$ is commonly denoted as $\mathbf{A}_{i,:}$, and the $j$th column is commonly denoted as $\mathbf{A}_{:,j}$.

The matrix formed by merging two matrices $\mathbf{A}$ and $\mathbf{B}$ with the same number of rows left and right is denoted as:
\[\begin{bmatrix}\mathbf{A} & \mathbf{B}\end{bmatrix}\]
or
\[\begin{pmatrix}\mathbf{A} & \mathbf{B}\end{pmatrix}\]

The matrix formed by merging two matrices $\mathbf{A}$ and $\mathbf{B}$ with the same number of columns up and down is denoted as:
\[\begin{bmatrix}\mathbf{A} \\ \mathbf{B}\end{bmatrix}\]
or
\[\begin{pmatrix}\mathbf{A} \\ \mathbf{B}\end{pmatrix}\]
\ssc{Matrix equality}
We say that two matrices $\mathbf{A}=[a_{ij}]_{m\times n}$ and $\mathbf{B}=[b_{ij}]_{m\times n}$ are equal if 
\[\forall 1\leq i\leq m,1\leq j\leq n\colon a_{ij}=b_{ij}.\]
\subsection{Basic operations}
\subsubsection{Matrix addition}
The sum of two $m\times n$ matrices $\mathbf{A}$ and $\mathbf{B}$ is calculated entrywise.
\subsubsection{Scalar multiplication}
The product $c\mathbf{A}$ of a scalar $c$ is calculated by multiplying every entry of $\mathbf{A}$ by $c$.
\subsubsection{Substraction}
The substraction of two $m\times n$ matrices $\mathbf{A}$ and $\mathbf{B}$ is defined as $\mathbf{A}-\mathbf{B}=\mathbf{A}+(-1)\mathbf{B}$.
\sssc{Matrix multiplication (矩陣乘法)}
Multiplication of two matrices is defined only if the number of columns of the left matrix is the same as the number of rows of the right matrix. The matrix product $\mathbf{A}\mathbf{B}$ of an $m\times n$ matrix $\mathbf{A}$ and $n\times p$ matrix $\mathbf{B}$ is the $m\times p$ matrix whose $ij$th entry is the dot product of the $i$th row of $\mathbf{A}$ and the $j$th column of $\mathbf{B}$.
\sssc{Positive integer exponent}
Given a square matrix $\mathbf{A}$ and a positive integer $n$, $\mathbf{A}^n$ is defined as $\prod_{i=1}^n\mathbf{A}$.
\subsubsection{Zero matrix (零矩陣)}
A zero matrix $\mathbf{O}$ over a field $F$ is a matrix of which all entries are the additive identity of $F$, that is, let $m\times n$ zero matrix over $F$ be $\mathbf{O}_{m\times n}$, for any matrices $\mathbf{A}_{m\times n}$ and $\mathbf{B}_{n\times p}$ over $F$, it satisfies:
\[\begin{aligned}
&\mathbf{A}_{m\times n}+\mathbf{O}_{m\times n}=\mathbf{A}_{m\times n}\\
\mathbf{A}_{m\times n}-\mathbf{A}_{m\times n}=\mathbf{O}_{m\times n}\\
&\mathbf{O}_{m\times n}\mathbf{B}_{n\times p}=\mathbf{O}_{m\times p}\\
&\mathbf{A}_{m\times n}\mathbf{O}_{n\times p}=\mathbf{O}_{m\times p}
\end{aligned}\]
\subsubsection{Identity matrix (單位矩陣)}
An identity matrix $\mathbf{I}$ over a field $F$ is a square matrix of which all diagonal entries are the multiplicative identity of $F$ and all off-diagonal entries are the multiplicative absorbing element $0_F$ of $F$ (i.e. $\forall x\in F\colon x\times 0_F=0_F$), that is, let $n\times n$ identity matrix over $F$ be $\mathbf{I}_n$, for any $m\times n$ matrices $\mathbf{A}$ over $F$, it satisfies:
\[\mathbf{A}\mathbf{I}_n=\mathbf{I}_m\mathbf{A}=\mathbf{A}\]
\sssc{Kronecker product (克羅內克積)}
If $\mathbf{A}$ is an $m\times n$ matrix and $\mathbf{B}$ is a $p\times q$ matrix, then the Kronecker product $\mathbf{A}\otimes\mathbf{B}$ is the $pm\times qn$ block matrix:
\[(\mathbf{A}\otimes\mathbf{B})_{i,j}=a_{\lceil i/p\rceil ,\lceil j/q\rceil }b_{(i-1)\mod p+1,(j-1)\mod q+1}\]
\sssc{Hadamard product (哈達瑪乘積), Schur product (舒爾乘積), elementwise product, or entrywise product (逐項乘積)}
For two $m\times n$ matrices $\mb{A}$ and $\mb{B}$, the Hadamard product $ \mb{A}\odot \mb{B}$ or $\mb{A}\circ \mb{B}$ is an $m\times n$ matrix defined as:
\[(\mb{A}\odot \mb{B})_{ij}=(\mb{A})_{ij}(\mb{B})_{ij}.\]
\sssc{Property}
For scalars $r$ and $s$, matrices $A$, $B$, $C$, $D$, zero matrix $O$, and identity matrix $I$, assuming that all of the following operations hold, $\neq$ means not necessarily equals:
\[A+B=B+A\]
\[A+(B+C)=(A+B)+C\]
\[r(A+B)=rA+rB\]
\[rA+sA=(r+s)A\]
\[(rs)A=r(sA)=s(rA)\]
\[(AB)C=A(BC)\]
\[r(AB)=(rA)B=A(rB)\]
\[A(B+C)=AB+AC\]
\[(A+B)C=AC+BC\]
\[(A+B)(C+D)=AC+AD+BC+BD\]
\[AB\neq BA\]
\[(AB=AC\land A\neq O)\nRightarrow B=C\]
\[A^2=O\nRightarrow A=O\]
\[AB=O\nRightarrow(A=O\lor B=O)\]
\[A^2=I\iff A=A^{-1}\nRightarrow A=\pm I\]
\[AB=I\nRightarrow(\exists r\in\mathbb{R}\colon A=rI\land B=\frac{I}{r})\]
\[AB=B\nRightarrow A=I\]
\[ACB=ADB\nRightarrow C=D\]
\[A\odot B=B\odot A\]
\[A\odot(B\odot C)=(A\odot B)\odot C\]
\[A\odot(B+C)=A\odot B+A\odot C\]
\sssc{Zero exponent}
Given an $n\times n$ matrix $\mathbf{A}$ and $n\times n$ identity matrix $\mathbf{I}$, $\mathbf{A}^0$ is defined as $\mathbf{I}$.
\subsection{Matrix row and column operations}
\sssc{Row operations (zh-hant: 列/zh-hans: 行運算)}
Including:
\bit
\item Row addition (zh-hant: 列/zh-hans: 行相加): Adding a row to another.
\item Row multiplication (zh-hant: 列/zh-hans: 行加倍): Multiplying all entries of a row by a non-zero constant.
\item Row switching (zh-hant: 列/zh-hans: 行交換): Interchanging two rows of a matrix.
\eit
If one matrix can be transformed to another through a sequence of row operations, the two matrices are called row-equivalent.
\sssc{(General) row echelon form (（廣義）zh-hant: 列/zh-hans: 行階梯形式)}
A matrix is in row echelon form if:
\bit
\item All rows having only zero entries are at the bottom.
\item The leading entry, that is, the left-most non-zero entry, of every non-zero row is on the right of the leading entry of every row above.
\item Some texts add the condition that the leading coefficient must be 1.
\eit
\sssc{Column operations (zh-hant: 行/zh-hans: 列運算)}
Including:
\bit
\item Column addition (zh-hant: 行/zh-hans: 列相加): Adding a column to another.
\item Column multiplication (zh-hant: 行/zh-hans: 列加倍): Multiplying all entries of a column by a non-zero constant.
\item Column switching (zh-hant: 行/zh-hans: 列交換): Interchanging two columns of a matrix.
\eit
If one matrix can be transformed to another through a sequence of column operations, the two matrices are called column-equivalent.
\sssc{Transpose (轉置)}
The transpose of an $m\times n$ matrix $\mathbf{A}$ is the $n\times m$ matrix $\mathbf{A}^\top$:
\[(\mathbf{A}^\top)_{i,j}=a_{j,i}\]
\sssc{Symmetric matrix (對稱矩陣)}
$\mathbf{A}$ is called a symmetric matrix if $\mathbf{A}=\mathbf{A}^\top$.
\sssc{Skew-symmetric matrix/antisymmetric matrix (反對稱矩陣)}
$\mathbf{A}$ is called a skew-symmetric matrix or an antisymmetric matrix if $\mathbf{A}=-\mathbf{A}^\top$.
\sssc{Transpose identities}
\[\qty(\sum_{i=1}^n\mathbf{A}_i)^\top=\sum_{i=1}^n\mathbf{A}_i^{\phantom{i}\top}\]
\[\qty(\prod_{i=1}^n\mathbf{A}_i)^\top=\prod_{i=1}^n\mathbf{A}_{n-i+1}^{\phantom{n-i+1}\top}\]
\ssc{Trace (跡)}
The trace of a matrix $\mb{A}$ is the sum of all diagonal entries of $\mb{A}$, denoted as $\tr(\mb{A})$.
\sssc{Conjugate transpose (共軛轉置)/Hermitian conjugate (埃爾米特共軛)/Hermitian transpose (埃爾米特轉置)}
The conjugate transpose of an $m\times n$ matrix $\mathbf{A}$ is the $n\times m$ matrix $\mathbf{A}*$:
\[(\mathbf{A}^*)_{i,j}=\ol{a_{j,i}}\]
\ssc{Cofactor matrix, adjugate matrix, determinant, and inverse}
Let $\mathbf{M}_{ij}$ of $\mathbf{A}$ be the determinant of the submatrix of $\mathbf{A}$ without the $i$th row and $j$th column.
\sssc{Determinant (行列式或判別式)}
The determinant of a $1\times 1$ matrix is the only element of it; the determinant of an $n\times n$ matrix $\mathbf{A}$, denoted $\det(\mathbf{A})$ or 
\[\begin{vmatrix}a_{11} & a_{12} & \ldots & a_{1n}\\a_{21} & a_{22} & \ldots & a_{2n}\\\vdots & \vdots & \ddots & \vdots\\a_{n1} & a_{n2} & \ldots & a_{nn}\end{vmatrix},\]
is
\[\det(\mathbf{A})=\sum_{i=1}^na_{ij}(-1)^{i+j}\mathbf{M}_{ij}\quad j\leq n\in\mathbb{N}\]
\[\det(\mathbf{A})=\sum_{j=1}^na_{ij}(-1)^{i+j}\mathbf{M}_{ij}\quad i\leq n\in\mathbb{N}\]
\sssc{Determinant identities}
Let $\mathbf{A}$ and $\mathbf{A}_i$ be square matrices.
\[\det\qty(\sum_{i=1}^n\mathbf{A}_i)=\sum_{i=1}^n\det\qty(\mathbf{A}_i)\]
\[\det\qty(\prod_{i=1}^n\mathbf{A}_i)=\prod_{i=1}^n\det\qty(\mathbf{A}_i)\]
\[\det(\mathbf{A})\neq 0\implies\det(\mathbf{A}^n)\neq 0\]
\sssc{Cofactor matrix (餘因子矩陣)}
The cofactor matrix of an $n\times n$ matrix $\mathbf{A}$ is the $n\times n$ matrix $\mathrm{cof}(\mathbf{A})$:
\[[\mathrm{cof}(\mathbf{A})]_{ij}=\left((-1)^{i+j}\mathbf{M}_{ij}\right)\]
\sssc{Adjugate matrix (伴隨矩陣)/classical adjoint (經典伴隨矩陣)}
The adjugate matrix or classical adjoint matrix of an $n\times n$ matrix $\mathbf{A}$ is the $n\times n$ matrix $\mathrm{adj}(\mathbf{A})$:
\[\mathrm{adj}(\mathbf{A})=\mathrm{cof}(\mathbf{A})^\top\]
\sssc{Inverse (反方陣、逆方陣、反矩陣或逆矩陣), multiplicative inverse (乘法反方陣), or reciprocal}
The inverse of an $n\times n$ matrix $\mathbf{A}$ is an $n\times n$ matrix $\mathbf{A}^{-1}$ such that:
\[\mathbf{A}\mathbf{A}^{-1}=\mathbf{A}^{-1}\mathbf{A}=\mathbf{I}\]
\sssc{Negative integer exponent}
Given an invertible matrix $\mathbf{A}$ and a negative integer $n$, $\mathbf{A}^n$ is defined as $(\mathbf{A}^{-1})^n$.
\sssc{Invertible matrix (可逆矩陣), orthogonal matrix (正交矩陣), orthonormal matrix (正則矩陣), non-singular matrix (非奇異矩陣), or non-degenerate matrix (非退化矩陣)}
An invertible matrix, orthogonal matrix, orthonormal matrix, non-singular matrix, or non-degenerate matrix is a matrix such that its inverse exists.

Let $\mathbf{A}$ be an $n\times n$ matrix, the following prepositions are equivalent to "$\mathbf{A}$ is an invertible matrix":
\[\begin{aligned}
&\exists\mathbf{A}^{-1}\\
&\exists\qty(\mathbf{A}^\top)^{-1}\\
&\exists\qty(\mathbf{A}^\top\mathbf{A})^{-1}\\
&\operatorname{rank}(\mathbf{A})=n\\
&\mathbf{A}\text{存在非零的特徵值}
\end{aligned}\]
\sssc{Non-invertible matrix (不可逆矩陣), non-orthogonal matrix (非正交矩陣), non-orthonormal matrix (非正則矩陣), singular matrix (奇異矩陣), or degenerate matrix (退化矩陣)}
An non-invertible matrix, non-orthogonal matrix, non-orthonormal matrix, singular matrix, or degenerate matrix is a matrix that is not an invertible matrix.
\sssc{Inverse of a two-by-two matrix}
For an invertible $2\times 2$ matrix $\mathbf{A}$:
\[\mathbf{A}=\begin{bmatrix}a & b \\c & d\end{bmatrix}\implies\mathbf{A}^{-1}=
\frac{1}{\det\qty(\mathbf{A})}\begin{bmatrix}d & -b \\-c & a\end{bmatrix}\]
\sssc{Computing inverse with row operations}
Let $\mathbf{A}$ be an $n\times n$ matrix and $\mathbf{I}$ be the $n\times n$ identity matrix. If $\begin{pmatrix}\mathbf{A} & \mathbf{I}\end{pmatrix}$ can become $\begin{pmatrix}\mathbf{I} & \mathbf{B}\end{pmatrix}$ after row operations, then
\[\mathbf{B}=\mathbf{A}^{-1}\]
\sssc{Unitary matrix (么正矩陣或酉矩陣)}
An unitary matrix is a square matrix whose inverse equals to its conjugate transpose.
\sssc{Determinant property}
\bit
\item When a row (or column) is added to another, the determinant remains unchanged.
\item When all entries of a row (or column) is multiplied by a real number $n$, the determinant is multiplied by $n$.
\item When two rows (or columns) are swapped, the determinant is multiplied by $-1$.
\item Given matrix $\mb{A}$ and its inverse $\mb{A}^{-1}$, $\det(\mb{A}^{-1})=\frac{1}{\det(\mb{A})}$.
\eit
\sssc{Inverse identities}
Let $\mathbf{A}$ and $\mb{A}_i$ be invertible matrices, $\mathbf{B}$ be a matrix of the same size of $\mathbf{A}$.
\[\mathbf{A}^{-1}=\frac{\mathrm{adj}\qty(\mathbf{A})}{\det\qty(\mathbf{A})}\]
\[\qty(\mathbf{A}^\top)^{-1}=\qty(\mathbf{A}^{-1})^\top\]
\[\qty(\prod_{i=1}^n\mathbf{A}_i)^{-1}=\prod_{i=1}^n\qty(\mathbf{A}_{n-i+1})^{-1}\]
\[(\mathbf{A}^{-1}\mathbf{B}\mathbf{A})^n=\mathbf{A}^{-1}\mathbf{B}^n\mathbf{A},\quad n\in\mathbb{N}\]
\sssc{Vandermonde matrix (范德蒙矩陣)}
A Vandermonde matrix $\mathbf{V}$ is:
\[V_{i,j}=\alpha_i^{\pht{i}j-1}\]
An $n\times n$ Vandermonde matrix $\mathbf{V}$ satisfies:
\[\det(V)=\prod _{1\leq i<j\leq n}(\alpha _{j}-\alpha _{i})\]
\subsection{Diagonal matrix (對角矩陣)}
\sssc{Diagonal matrix}
A diagonal matrix over a field $F$ is a square matrix over $F$ of which all non-diagonal entries are the additive identity of $F$.
\sssc{Diagonal matrix identities}
If:
\[\mathbf{D} = \begin{bmatrix}
d_1 & 0 & \cdots & 0 \\
0 & d_2 & \cdots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \cdots & d_n
\end{bmatrix}\]
where all $d_i\neq 0$, then:
\[\mathbf{D}^{-1} = \begin{bmatrix}
\frac{1}{d_1} & 0 & \cdots & 0 \\
0 & \frac{1}{d_2} & \cdots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \cdots & \frac{1}{d_n}\end{bmatrix}\]

Let $\mathbf{E}$, $\mathbf{F}$ be two $n\times n$ diagonal matrices:
\[\mathbf{E}\mathbf{F}=[e_{ij}f_{ij}]_{1\leq i,j\leq n}\]
\ssc{System of Linear Equations (線性方程組)}
\sssc{System of equations (方程組)}
\begin{itemize}
\item\tb{Consistent system (相容方程組)}: A system of equations is considered consistent if it has at least one solution.
\item\tb{Inconsistent/contradictory system (相容方程組)}: A system of equations is considered inconsistent or contradictory if it has no solution.
\item\tb{Dependent system (相依方程組)}: A consistent system of equations is considered dependent if it has infinite solutions.
\item\tb{Independent system (獨立方程組)}: A consistent system of equations is considered independent if it has finite solutions.
\end{itemize}
\sssc{Linear Equation (線性方程)}
A linear equation is an equation that may be put in the from:
\[\sum_{k=1}^na_kx_k=c,\]
where $a_k$ and $c$ are called coefficients.

If $c=0$, the linear equation is called a homogeneous linear equation; otherwise, it is called a heterogeneous linear equation.
\sssc{System of Linear Equations (線性方程組)}
A system of linear equations is a collection of two or more linear equations involving the same tuple of variables.
\sssc{Coefficient matrix (係數矩陣)}
The coefficients of a system of linear equations written as a matrix, with one row for each equation.
\sssc{Augmented matrix (增廣矩陣)}
Coefficient matrix with the constants on the right side of the equal signs of a system of linear equations augmented to the last column of the matrix.
\sssc{Gaussian Elimination (高斯消去法) or row reduction (zh-hant: 列/zh-hans: 行簡化)}
An algorithm to solve a system of linear equations by using row operations to modify its argumented matrix to row echelon form. Two systems of linear equations have the same solution if and only if their augmented matrices are row-equivalent.
\sssc{Cramer Rule or Cramer formula (克拉瑪公式)}
Given a system of linear equations that can be written in the form:
\[\mathbf{A}\mathbf{X}=\mathbf{C}\]
where $\mathbf{A}$ is an $n\times n$ coefficient matrix, $\mathbf{X}$ is a variable matrix:
\[\mathbf{X}=\begin{pmatrix}x_1, x_2, x_3, \ldots, x_n\end{pmatrix}^\top,\]
and $\mathbf{C}$ is a coefficient matrix:
\[\mathbf{C}=\begin{pmatrix}c_1, c_2, c_3, \ldots, c_n\end{pmatrix}^\top.\]
Let \(\mathbf{A}_i\) be $\mathbf{A}$ with the \(i\)th column replaced by \(\mathbf{C}\). The solution to the system of linear equations is given by:
\[\begin{cases}
x_i=\frac{\det\qty(\mathbf{A}_i)}{\det\qty(\mathbf{A})},\quad\det\qty(\mathbf{A})\neq 0\\
\text{inconsistent},\quad\det\qty(\mathbf{A})=0\land\bigvee_{i=1}^n\det\qty(\mathbf{A}_i)\neq 0\\
\text{dependent},\quad\det\qty(\mathbf{A})=0\land\bigvee_{i=1}^n\det\qty(\mathbf{A}_i)=0
\end{cases}\]
where $x_i=\frac{\det\qty(\mathbf{A}_i)}{\det\qty(\mathbf{A})}$ can be geometrically viewed as the volume of the parallelogram formed by the column vectors of $\mathbf{A}_i$ divided by the volume of the parallelogram formed by the column vectors of $\mathbf{A}$.

Particularly, when $\mathbf{C}$ is a zero matrix, that is,
\[\mathbf{A}\mathbf{X}=\mathbf{0},\]
the solution to the system of linear equations is given by:
\[\begin{cases}
x_i=0,\quad\det\qty(\mathbf{A})\neq 0\\
\tx{dependent},\quad\det\qty(\mathbf{A})=0
\end{cases}\]
\section{Linear Subspace (線性子空間)}
\ssc{Linear subspace}
A linear subspace of a vector space $V$ over a field $\mathbb{K}$ is a nonempty subset $W$ of $V$ such that, 
\[\forall w_1,w_2\in W,a,b\in\mathbb{K}:\,aw_1+bw_2\in W.\]
\ssc{Four fundamental subspaces of a matrix (矩陣的四大子空間)}
The four fundamental subspaces of a matrix are the row space, column space, null space, and left null space.
\ssc{Column space (zh-hant: 行/zh-hans: 列空間)}
The column space of $\mathbf{A}$, denoted as \(\operatorname{Col}(\mathbf{A})\) or $\mathrm{\mathbf{C}}(\mathbf{A})$, is defined as the set of all linear combinations of the column vectors of $\mathbf{A}$.
\ssc{Row space (zh-hant: 列/zh-hans: 行空間)}
The row space of $\mathbf{A}$ is defined as the set of all linear combinations of the row vectors of $\mathbf{A}$, that is, $\operatorname{Col}(\mathbf{A}^{\top})$.
\ssc{Null space (零空間), kernel (核), or kernel space (核空間)}
The kernel of a linear map $L:\,V\to W$, denoted as $\ker(L)$ or $\mathrm{Null}(L)$, is defined as:
\[\ker(L)=\{v\in V:\,L(v)=0\}.\]

$\dim(\ker(\mathbf{A}))$ is called nullity (零化度).
\ssc{Left null space (左零空間), left kernel (左核), or left kernel space (左核空間)}
The left null space of a linear map $L:\,V\to W$ is defined as $\ker(L^{-1})$.
\subsection{Image space (像空間) or range (值域)}
The image space of a linear map $T\colon V\to W$, denoted as $\operatorname{im}(T)$
\[\operatorname{im}(T)=\{T(v):\,v\in V\}.\]
\ssc{Cokernel (餘核)}
The cokernel of a linear map $T\colon V\to W$, denoted as $\operatorname{coker}(T)$
\[\operatorname{coker}(T)=W\setminus\operatorname{im}(T).\]
\subsection{Constant rank theorem (常秩定理)}
For any matrix $\mathbf{A}$:  \[\dim(\operatorname{Col}(\mathbf{A}))=\dim(\operatorname{Col}(\mathbf{A}^\top))\]
\begin{proof}\mbox{}\\
Let $\mathbf{A}$ be an $m\times n$ matrix, $r=\dim\qty(\operatorname{Col}\qty(\mathbf{A}))$, $ c_{1},c_{2},\ldots ,c_{r}$ be a basis of $\operatorname{Col}(\mathbf{A})$, matrix $\mathbf{C}=[c_{1},c_{2},\ldots ,c_{r}]$, and $r\times n$ matrix $\mathbf{R}$ such that $\mathbf{A}=\mathbf{C}\mathbf{R}$. 

Since:
\[\operatorname{Col}(\mathbf{A}^\top)\subseteq\operatorname{Col}(\mathbf{R}^\top),\]
\[\dim(\operatorname{Col}(\mathbf{A}^\top))\leq\dim(\operatorname{Col}(\mathbf{R}^\top))\]

Because there are only $r$ rows in $\mathbf{R}$,
\[\dim(\operatorname{Col}(\mathbf{R}^\top))\leq\dim(\operatorname{Col}(\mathbf{A}))\]

Similarly, we can obtain that
\[\dim(\operatorname{Col}(\mathbf{A}))\leq\dim(\operatorname{Col}(\mathbf{R}^\top))\leq\dim(\operatorname{Col}(\mathbf{A}^\top)).\]

Therefore,
\[\dim(\operatorname{Col}(\mathbf{A}))=\dim(\operatorname{Col}(\mathbf{R}^\top))=\dim(\operatorname{Col}(\mathbf{A}^\top)).\]
\end{proof}
\subsection{Rank (秩)}
If $ \mathbf{A} $ is an \( m \times n \) matrix, then the rank of \( \mathbf{A} \), denoted as \( \operatorname{rank}(\mathbf{A}) \) or $\operatorname{rk}(\mathbf{A})$, is the maximum number of linearly independent columns (or rows) in \( \mathbf{A} \), namely:
\[\text{rank}(\mathbf{A})=\dim(\operatorname{Col}(\mathbf{A}))\]
\subsection{Rank-nullity theorem (秩—零化度定理)}
For any $n\times n$ matrix $\mathbf{A}$:
\[\text{rank}(\mathbf{A}) + \dim(\ker(\mathbf{A})) = n\]
\begin{proof}\mbox{}\\
Let \(\mathbf{A}\) be an \(m \times n\) matrix with \(r\) linearly independent columns (i.e., \(\text{rank}(\mathbf{A}) = r\)). We will show that there exists a set of \(n - r\) linearly independent solutions to the homogeneous system \(\mathbf{Ax} = \mathbf{0}\).

To do this, we will produce an \(n \times (n-r)\) matrix \(\mathbf{X}\) whose columns form a basis of the null space of \(\mathbf{A}\).

Without loss of generality, assume that the first \(r\) columns of \(\mathbf{A}\) are linearly independent. So, we can write
\[\mathbf{A} = \begin{pmatrix} \mathbf{A}_1 & \mathbf{A}_2\end{pmatrix},\]
where
\begin{itemize}
\item \(\mathbf{A}_1\) is an \(m \times r\) matrix with \(r\) linearly independent column vectors, and
\item \(\mathbf{A}_2\) is an \(m \times (n-r)\) matrix such that each of its \(n-r\) columns is a linear combination of the columns of \(\mathbf{A}_1\).
\end{itemize}

This means that \(\mathbf{A}_2 = \mathbf{A}_1\mathbf{B}\) for some \(r \times (n-r)\) matrix \(\mathbf{B}\), and hence,
\[\mathbf{A} = \begin{pmatrix} \mathbf{A}_1 & \mathbf{A}_1\mathbf{B}\end{pmatrix}.\]

Let
\[\mathbf{X} = \begin{pmatrix} -\mathbf{B} \\ \mathbf{I}_{n-r} \end{pmatrix},\]
where \(\mathbf{I}_{n-r}\) is the \((n-r) \times (n-r)\) identity matrix. So, \(\mathbf{X}\) is an \(n \times (n-r)\) matrix such that
\[\mathbf{A}\mathbf{X} = \begin{pmatrix}\mathbf{A}_1 & \mathbf{A}_1\mathbf{B} \end{pmatrix}\begin{pmatrix} -\mathbf{B} \\ \mathbf{I}_{n-r} \end{pmatrix} = -\mathbf{A}_1\mathbf{B} + \mathbf{A}_1\mathbf{B} = \mathbf{0}_{m \times (n-r)}.\]

Therefore, each of the \(n-r\) columns of \(\mathbf{X}\) are particular solutions of \(\mathbf{Ax} = \mathbf{0}_{\mathbb{F}^m}\).

Furthermore, the \(n-r\) columns of \(\mathbf{X}\) are linearly independent because \(\mathbf{Xu} = \mathbf{0}_{\mathbb{F}^n}\) will imply \(\mathbf{u} = \mathbf{0}_{\mathbb{F}^{n-r}}\) for \(\mathbf{u} \in \mathbb{F}^{n-r}\):
\[\mathbf{X}\mathbf{u} = \mathbf{0}_{\mathbb{F}^n} \implies \begin{pmatrix}
-\mathbf{B} \\
 \mathbf{I}_{n-r}
\end{pmatrix}\mathbf{u} = \mathbf{0}_{\mathbb{F}^n} \implies \begin{pmatrix}
-\mathbf{B}\mathbf{u} \\
 \mathbf{u}
\end{pmatrix} = \begin{pmatrix}
\mathbf{0}_{\mathbb{F}^r} \\
 \mathbf{0}_{\mathbb{F}^{n-r}}
\end{pmatrix} \implies \mathbf{u} = \mathbf{0}_{\mathbb{F}^{n-r}}.\]

Therefore, the column vectors of \(\mathbf{X}\) constitute a set of \(n-r\) linearly independent solutions for \(\mathbf{Ax} = \mathbf{0}_{\mathbb{F}^m}\).

We next prove that any solution of \(\mathbf{Ax} = \mathbf{0}_{\mathbb{F}^m}\) must be a linear combination of the columns of \(\mathbf{X}\).

Let
\[\mathbf{u} = \begin{pmatrix}
 \mathbf{u}_1 \\
 \mathbf{u}_2
\end{pmatrix} \in \mathbb{F}^n\]
be any vector such that \(\mathbf{Au} = \mathbf{0}_{\mathbb{F}^m}\). Since the columns of \(\mathbf{A}_1\) are linearly independent, \(\mathbf{A}_1\mathbf{x} = \mathbf{0}_{\mathbb{F}^m}\) implies \(\mathbf{x} = \mathbf{0}_{\mathbb{F}^r}\).

Therefore,
\[\begin{aligned}
& \mathbf{A}\mathbf{u} = \mathbf{0}_{\mathbb{F}^m} \\
\implies & \begin{pmatrix}\mathbf{A}_1 & \mathbf{A}_1\mathbf{B}\end{pmatrix} \begin{pmatrix} \mathbf{u}_1 \\ \mathbf{u}_2 \end{pmatrix} = \mathbf{A}_1\mathbf{u}_1 + \mathbf{A}_1\mathbf{B}\mathbf{u}_2 = \mathbf{A}_1(\mathbf{u}_1 + \mathbf{B}\mathbf{u}_2) = \mathbf{0}_{\mathbb{F}^m} \\
\implies & \mathbf{u}_1 + \mathbf{B}\mathbf{u}_2  = \mathbf{0}_{\mathbb{F}^r} \\
\implies & \mathbf{u}_1  =  -\mathbf{B}\mathbf{u}_2\\
\implies & \mathbf{u} = \begin{pmatrix} \mathbf{u}_1 \\ \mathbf{u}_2 \end{pmatrix} = \begin{pmatrix} -\mathbf{B} \\ \mathbf{I}_{n-r} \end{pmatrix}\mathbf{u}_2 = \mathbf{X}\mathbf{u}_2.
\end{aligned}\]

This proves that any vector \(\mathbf{u}\) that is a solution of \(\mathbf{Ax} = \mathbf{0}\) must be a linear combination of the \(n-r\) special solutions given by the columns of \(\mathbf{X}\). And we have already seen that the columns of \(\mathbf{X}\) are linearly independent. Hence, the columns of \(\mathbf{X}\) constitute a basis for the null space of \(\mathbf{A}\). Therefore, the nullity of \(\mathbf{A}\) is \(n - r\). Since \(r\) equals the rank of \(\mathbf{A}\), it follows that \(\text{rank}(\mathbf{A}) + \ker(\mathbf{A}) = n\). This concludes our proof.
\end{proof}
\subsection{Full rank (滿秩) and rank deficiency (秩虧)}
For an $m\times n$ matrix $\mb{A}$, if
\[\text{rank}(\mathbf{A}) = \min(m, n)\]
$\mathbf{A}$ is called to have full rank; otherwise it is called to be rank-deficient (欠秩或秩虧的).

The rank deficiency of a $m\times n$ matrix $\mb{A}$ is defined as
\[\min(m, n)-\text{rank}(\mathbf{A}) \]
\ssc{Property of rank}
\bit
\item For any $m\times n$ matrix $\mb{A}$:
\[\text{rank}(\mathbf{A}) \leq \min(m, n)\]
\item A square matrix is invertible if and only if it has full rank.
\item A matrix is a zero matrix if and only if its rank is zero.
\end{itemize}
\section{Matrix Analysis (矩陣分析)}
\subsection{Eigenvector or characteristic vector (特徵向量) and characteristic polynomial (特徵多項式)}
\subsubsection{Eigenvector or characteristic vector (特徵向量) and eigenvalue (本徵值、固有值、特徵值或特徵根)}
Given a linear transformation $L\colon V\to V$, an eigenvector of it is defined as the vector $v\in V$ such that:
\[\mathbf{A}v = \lambda v,\]
where $\lambda$ is a scalar, which is called the eigenvalue of $\mb{A}$ or the eigenvalue of $v$.
\subsubsection{Characteristic polynomial}
For an $n\times n$ matrix $\mathbf{A}$, the characteristic polynomial of it is defined as
\[p_{\mathbf{A}}(\lambda):=\det(\lambda \mathbf{I}_{n}-\mathbf{A}),\]
where $\lambda$ is a scalar variable.

A scalar $\lambda$ is an eigenvalue of $\mb{A}$ if and only if
\[p_\mathbf{A}(\lambda)=0.\]
\sssc{Property}
\bit
\item The number of distinct eigenvalues of a square matrix is less than or equal to the number of linearly independent eigenvectors.
\item The number of distinct eigenvalues of a $n\times n$ matrix is less than or equal to $n$.
\item The algebraic multiplicity of a root $\lambda$ of the characteristic polynomial of a square matrix is greater than or equal to the number of linearly independent eigenvectors of it whose eigenvalue is $\lambda$.
\end{itemize}
\subsubsection{Cayley–Hamilton theorem (凱萊–哈密頓定理)}
For a square matrix $\mathbf{A}$ and its characteristic polynomial $p_\mathbf{A}$, the Cayley–Hamilton theorem states that:
\[p_\mathbf{A}(\mathbf{A})=0\]
\sssc{Diagonalization (對角化)}
The diagonalization of an $n\times n$ matrix $\mb{A}$ is the construction of an $n\times n$ matrix $\mb{P}$ and an $n\times n$ diagonal matrix $\mb{D}$ such that:
\[\mathbf{A}=\mathbf{P}^{-1}\mathbf{D}\mathbf{P}\]

An $n\times n$ matrix can be diagonalized if it has $n$ distinct eigenvalues.

The steps of the diagonalization of an $n\times n$ matrix $\mathbf{A}$ is as follows:
\begin{enumerate}
\item For each eigenvalue $\lambda$ of the matrix $\mathbf{A}$, solve the eigenvector $v$:
\[(\mathbf{A} - \lambda \mathbf{I})\mathbf{v} = 0.\]
\item Compose matrix \( \mathbf{P} \) with each eigenvector of $\mb{A}$ as a row.
\end{enumerate}

Common application:
\[\mathbf{A}^n=(\mathbf{P}^{-1}\mathbf{D}\mathbf{P})^n=\mathbf{P}^{-1}\mathbf{D}^n\mathbf{P}\]

\subsection{Generalized Definition of Inverse}
\subsubsection{One-sided inverse (單邊逆)}
\begin{itemize}
\item Right inverse (右逆): For any $n\times m$ matrix $A$ with rank $\mathrm {rank}(A)=n$, there exists an $m\times n$ matrix $A_{\mathrm {R} }^{-1}$, called the right inverse of $\mb{A}$, such that
\[AA_{\mathrm {R} }^{-1}=I_{n},\]
where $I_{n}$ is the $n\times n$ identity matrix.
\item Left inverse (左逆): For any $n\times m$ matrix $A$ with rank $\mathrm {rank}(A)=m$, there exists an $m\times n$ matrix $A_{\mathrm {L} }^{-1}$, called the left inverse of $\mb{A}$, such that
\[A_{\mathrm {L} }^{-1}A=I_{m},\]
where $I_{m}$ is the $m\times m$ identity matrix.
\end{itemize}

Properties:
\bit
\item A matrix doesn't necessarily have a right inverse and doesn't necessarily have a left inverse.
\item A square matrix has a right inverse if and only if it has left inverse if and only if it is invertible.
\item For any invertible matrix, there exists only one right inverse and one left inverse, which are its multiplicative inverse.
\eit
\subsubsection{Generalized inverse or g-inverse (廣義逆)}
A matrix $A^{\mathrm {g} }\in \mathbb {C} ^{n\times m}$ is a generalized inverse of a matrix $A\in \mathbb {C} ^{m\times n}$ if:
\[AA^{\mathrm {g} }A=A.\]

Properties:
\bit
\item For all matrices, there exists at least one generalized inverse, but it's not necessarily unique.
\item For any invertible matrix, there exists only one generalized inverse, which is its multiplicative inverse.
\eit
\subsubsection{Reflexive generalized inverse (自反廣義逆)}
A reflexive generalized inverse of a matrix $\mathbf{A}$ is defined as a matrix $\mathbf{A}^g$ such that:
\[\mathbf{A}\mathbf{A}^g\mathbf{A}=\mathbf{A}\]
\[\mathbf{A}^g\mathbf{A}\mathbf{A}^g=\mathbf{A}^g\]

Properties:
\bit
\item For all matrices, there exists at least one reflexive generalized inverse, but it's not necessarily unique.
\item For any invertible matrix, there exists only one reflexive generalized inverse, which is its multiplicative inverse.
\eit
\subsubsection{Moore–Penrose inverse (摩爾—彭若斯廣義逆) or pseudoinverse (偽逆或擬反)}
A Moore–Penrose inverse of a matrix $\mathbf{A}$ is defined as a matrix $\mathbf{A}^+$ such that:
\[\mathbf{A}\mathbf{A}^+\mathbf{A}=\mathbf{A}\]
\[\mathbf{A}^+\mathbf{A}\mathbf{A}^+=\mathbf{A}^+\]
\[\left(\mathbf{A}\mathbf{A}^{+}\right)^{*}=\mathbf{A}\mathbf{A}^{+}\]
\[\left(\mathbf{A}^+\mathbf{A}\right)^{*}=\mathbf{A}^+\mathbf{A}\]

Properties:
\bit
\item For all matrices, there exists only one Moore–Penrose inverse.
\item For any invertible matrix, there exists only one Moore–Penrose inverse, which is its multiplicative inverse.
\eit
\sct{Transformation Matrices}
\ssc{Groups}
\sssc{General linear group}
The general linear group of degree $n$ over some field $F$ is the set of $n\times n$ invertible matrices over $F$, together with the operation of matrix multiplication, denoted as $\operatorname{GL}(n,F)$ or $\operatorname{GL}_n(F)$, or simply $\operatorname{GL}(n)$ if the field is understood.
\sssc{Orthogonal group or general orthogonal group}
The orthogonal group or general orthogonal group of degree $n$ over some field $F$ is the set of $n\times n$ invertible matrices over $F$ such that the inverse equals the transpose, together with the operation of matrix multiplication, denoted as $\operatorname{O}(n,F)$ or $\operatorname{O}_n(F)$, or simply $\operatorname{O}(n)$ if the field is understood. An element of an orthogonal group is called an orthogonal matrix.
\sssc{Special orthogonal group or rotation group}
The special orthogonal group or rotation group of degree $n$ over some field $F$ is a subgroup of the orthogonal group of degree $n$ over some field $F$ such that for any matrix in it, the determinant is 1, that is, it preserves the orientation, denoted as $\operatorname{SO}(n,F)$ or $\operatorname{SO}_n(F)$, or simply $\operatorname{SO}(n)$ if the field is understood. An element of a special orthogonal group is called an rotational matrix.
\ssc{Transformation matrices in two-dimensional space}
\sssc{Degenerate matrices}
Degenerate to 1 dimension ($(a\lor b)\land (d\lor e)\neq 0$):
\[\begin{bmatrix}a & b\\ca & cb\end{bmatrix}\begin{bmatrix}x \\ y\end{bmatrix}=\begin{bmatrix}ax+by \\ c(ax+by)\end{bmatrix}\]
\[\begin{bmatrix}0 & 0\\d & e\end{bmatrix}\begin{bmatrix}x \\ y\end{bmatrix}=\begin{bmatrix}0 \\ dx+ey\end{bmatrix}\]
Degenerate to 0 dimension:
\[\begin{bmatrix}0 & 0\\0 & 0\end{bmatrix}\begin{bmatrix}x \\ y\end{bmatrix}=\begin{bmatrix}0 \\ 0\end{bmatrix}\]
\sssc{Stretching (伸縮/縮放/拉伸)}
A stretch is a linear transformation which enlarges all distances in a particular direction by a constant factor $k$ but does not affect distances in the perpendicular direction. (Note that if $k > 1$, then this really is a "stretch"; if $k < 1$, it is technically a "compression", but we still call it a stretch. Also, if $k = 1$, then the transformation is an identity, i.e. it has no effect
)

The matrix associated with a stretch by a factor $h$ along the $x$-axis and a factor $k$ along the $y$-axis is given by:
\[\begin{bmatrix}
h & 0 \\
0 & k
\end{bmatrix}\]
\sssc{Shearing (推移)}
A shearing in the $x$-direction dependent on the $y$ coordination by a factor $k$ is given by:
\[\begin{bmatrix}
1 & k \\
0 & 1
\end{bmatrix}\]
A shearing in the $y$-direction dependent on the $x$ coordination by a factor $k$ is given by:
\[\begin{bmatrix}
1 & 0 \\
k & 1
\end{bmatrix}\]
\sssc{Rotation (旋轉)}
A rotation by an angle $\theta$ counterclockwise about the origin is given by:
\[\begin{bmatrix}
\cos(\theta) & -\sin(\theta) \\
\sin(\theta) & \cos(\theta)
\end{bmatrix}\]
\sssc{Reflection (鏡射/反射)}
A reflection about a line that passes the original and with $(a,b)=\sqrt{a^2+b^2}(\cos\theta,\sin\theta)$ being a vector in the direction of it is given by:
\[\begin{bmatrix}
\cos(2\theta) & \sin(2\theta) \\
\sin(2\theta) & -\cos(2\theta)
\end{bmatrix}=
\frac{1}{a^2+b^2}\begin{bmatrix}
a^2-b^2 & 2ab \\
2ab & b^2-a^2
\end{bmatrix}\]
\sssc{Orthogona projection (正射影)}
An orthogonal projection about a line that passes the original and with $(a,b)=\sqrt{a^2+b^2}(\cos\theta,\sin\theta)$ being a vector in the direction of it is given by:
\[\begin{bmatrix}
\cos^2(\theta) & \cos(\theta)\sin(\theta) \\
\cos(\theta)\sin(\theta) & \sin^2(\theta)
\end{bmatrix}=
\frac{1}{a^2+b^2}\begin{bmatrix}
a^2 & ab \\
ab & b^2
\end{bmatrix}\]
\sssc{Exmaple 1}
\textbf{Question:} The cricle $\mathrm{C}: x^2+y^2=1$ is transformed by $\mathbf{A}=\begin{bmatrix}2&0\\0&3\end{bmatrix}$ to obtain a new curve $\mathrm{C}'$. Find the equation of $\mathrm{C}'$ and its area.

\textbf{Answer:} 
\[\begin{bmatrix}x'\\y'\end{bmatrix}=\mathbf{A}\begin{bmatrix}x\\y\end{bmatrix}=\begin{bmatrix}2x\\3y\end{bmatrix}\]
\[\begin{cases}x'=2x\\y'=3y\end{cases}\Rightarrow \begin{cases}x=\frac{x'}{2}\\y=\frac{y'}{3}\end{cases}\]
\[\qty(\frac{x'}{2})^2+\qty(\frac{y'}{3})^2=1\Rightarrow \frac{x'^2}{4}+\frac{y'^2}{9}=1\]
\[\mathrm{C}'\colon\frac{x'^2}{4}+\frac{y'^2}{9}=1\]
The area of $\mathrm{C}'$ is $\pi \cdot 2 \cdot 3 = 6\pi$. Since $\det(\mathbf{A}) = 6$, and the area of $\mathrm{C}$ is $\pi$, the area of $\mathrm{C}'$ equals $\det(\mathbf{A})$ multiplied by the area of $\mathrm{C}$.
\sssc{Exmaple 2}
\textbf{Question:} The line $\mathbf{L}: 4x - 3y = 5$ is transformed in the $x$-direction dependent on the $y$ coordination by twice and then in the $y$-direction dependent on the $x$ coordination by negative theee times to obtain a new line $\mathbf{L}'$. Find the equation of $\mathbf{L}'$.

\textbf{Answer:} 
\[\mathbf{A}=\begin{bmatrix}1&2\\-3&1\end{bmatrix}\]
\[\mathbf{A}^{-1}=\frac{1}{7}\begin{bmatrix}1&-2\\3&1\end{bmatrix}\]
\[\begin{bmatrix}x\\y\end{bmatrix}=\mathbf{A}^{-1}\begin{bmatrix}x'\\y'\end{bmatrix}=\begin{bmatrix}\frac{x'-2y'}{7}\\ \frac{3x'+y'}{7}\end{bmatrix}\]
\[\mathbf{L}': 4\frac{x-2y}{7}+3\frac{3x+y}{7}=5\]
\[\mathbf{L}': 5x+11y+35=0\]
\sssc{Example 3}
\textbf{Question:} Reflect the point $\mathrm{P}(5, 2)$ over the line $\mathbf{M}: x - y + 1 = 0$ to obtain a new point $\mathrm{Q}$. Find $\mathrm{Q}$.

\textbf{Answer:} 

Shift one unit to the right: $\mathbf{M}$ becomes $\mathbf{M}':x - y = 0$ and $\mathrm{C}$ becomes $\mathrm{C}'(6, 2)$.

The reflection matrix is: 
\[\mathbf{A} = \begin{bmatrix} 0 & 1 \ 1 & 0 \end{bmatrix}\]
\[\mathrm{D}' = \mathbf{A} \mathrm{C}' = (2, 6)\]

Shift one unit to the left: $\mathrm{D}(1, 6)$
\sssc{Example 4}
\textbf{Question:} The line $x+y-2=0$ is transformed by $\mathbf{A}=\begin{bmatrix}a&b\\1&2\end{bmatrix}\in\mathbb{R}^{2\times 2}$ to obtain a new line $2x+3y-4=0$. Find $a$ and $b$.

\textbf{Answer:} 
\[\mathbf{A}\begin{bmatrix}x\\ -x+2\end{bmatrix}=\begin{bmatrix}x'\\ \frac{-2x'+4}{3}\end{bmatrix}\]
\[x-2x+4=\frac{-2x'+4}{3}\]
\[x'=\frac{3}{2}x-4\]
\[ax-bx-2b=x'=\frac{3}{2}x-4\]
\[(a, b)=(-\frac{1}{2}, -2)\]
\ssc{Transformation matrices in three-dimensional space}
\sssc{Stretching (伸縮/縮放/拉伸)}
The matrix associated with a stretch by a factor $h$ along the $x$-axis, a factor $k$ along the $y$-axis, and a factor $w$ along the $z$-axis is given by:
\[\begin{bmatrix}
h & 0 & 0 \\
0 & k & 0 \\
0 & 0 & w
\end{bmatrix}\]
\sssc{Shearing (推移)}
A shearing in the $x$-direction dependent on the $y$ coordination by a factor $k$ is given by:
\[\begin{bmatrix}
1 & k & 0 \\
0 & 1 & 0 \\
0 & 0 & 1
\end{bmatrix}\]
A shearing in the $x$-direction dependent on the $z$ coordination by a factor $k$ is given by:
\[\begin{bmatrix}
1 & 0 & k \\
0 & 1 & 0 \\
0 & 0 & 1
\end{bmatrix}\]
A shearing in the $y$-direction dependent on the $x$ coordination by a factor $k$ is given by:
\[\begin{bmatrix}
1 & 0 & 0 \\
k & 1 & 0 \\
0 & 0 & 1
\end{bmatrix}\]
A shearing in the $y$-direction dependent on the $z$ coordination by a factor $k$ is given by:
\[\begin{bmatrix}
1 & 0 & 0 \\
0 & 1 & k \\
0 & 0 & 1
\end{bmatrix}\]
A shearing in the $z$-direction dependent on the $x$ coordination by a factor $k$ is given by:
\[\begin{bmatrix}
1 & 0 & 0 \\
0 & 1 & 0 \\
k & 0 & 1
\end{bmatrix}\]
A shearing in the $z$-direction dependent on the $y$ coordination by a factor $k$ is given by:
\[\begin{bmatrix}
1 & 0 & 0 \\
0 & 1 & 0 \\
0 & k & 1
\end{bmatrix}\]
\sssc{Rotation (旋轉)}
A rotation by an angle $\theta$ counterclockwise about a line that passes the original and with $(a,b,c)$ being a vector in the direction of it is given by:
\[\frac{1}{a^2+b^2+c^2}\begin{bmatrix}
a^2(1-\cos\theta)+\cos\theta & ab(1-\cos\theta)-c\sin\theta & ac(1-\cos\theta )+b\sin\theta\\
bc(1-\cos\theta )+c\sin\theta & b^2(1-\cos\theta )+\cos\theta & bc(1-\cos\theta )-a\sin\theta\\
ac(1-\cos\theta )-b\sin\theta & bc(1-\cos\theta )+a\sin\theta & c^2(1-\cos\theta )+\cos\theta \end{bmatrix}\]
\sssc{Reflection (鏡射/反射)}
A reflection about a plane $ax+by+cz=0$ is given by:
\[\mathbf{I}-\frac{1}{a^2+b^2+c^2}\begin{bmatrix}
2a^2 & 2ab & 2ac \\
2ab & 2b^2 & 2bc \\
2ac & 2bc & 2c^2
\end{bmatrix},\]
where $\mathbf{I}$ is the $3\times 3$ identity matrix.
\sssc{Orthogona projection (正射影)}
An orthogonal projection about a plane $ax+by+cz=0$ is given by:
\[\mathbf{I}-\frac{1}{a^2 + b^2 + c^2}\begin{bmatrix}
a^2 & ab & ac \\
ab & b^2 & bc \\
ac & bc & c^2
\end{bmatrix}\]
where $\mathbf{I}$ is the $3\times 3$ identity matrix.
\end{document}
