\documentclass[a4paper,12pt]{report}
\setcounter{secnumdepth}{5}
\setcounter{tocdepth}{3}
\input{/usr/share/latex-toolkit/template.tex}
\begin{document}
\title{Probability Theory}
\author{沈威宇}
\date{\temtoday}
\titletocdoc
\sct{Probability Theory (機率論)}
\subsection{Probability}
\begin{itemize}
\item (Random) Experiment or Trial (（隨機）試驗): A process that can be repeated and the results may be different each time. It has repeatability, that is, the test can be repeated under the same conditions, and randomness, that is, the results of each test may be different, with randomness and uncertainty.
\item Sample space (樣本空間): The set of all possible outcomes of an experiment. For example, the sample space for flipping a coin is $\{\text{head}, \text{tail}\}$.
\item Event (事件): A subset of the sample space. For example, the event of rolling a die and getting an even number is $\{2, 4, 6\}$.
\item Probability (機率): The likelihood of an event occurring, a number between 0 and 1. The closer the probability is to 1, the more likely the event is to occur.
\item Impossible event (空事件): An event with zero probability.
\item Sure event (全事件): An event with one probability.
\item Sum event (和事件): The sum event of event $A$ and event $B$ is $A\cup B$.
\item Product event (積事件): The product event of event $A$ and event $B$ is $A\cap B$.
\item Complement event (餘事件): The complement event of event $A$ in sample space $S$ is $A'=S\setminus A$.
\item Classical probability (古典機率): If the number of all possible outcomes of an event is finite or countable infinite, and the chance of each outcome occurring in the sample space is equal, then the probability of the event occurring can be calculated by:
\[ P(A)= \frac{\text{The number of outcomes of event A}}{\text{The number of all possible outcomes}} \]
\item Objective probability (客觀機率) or Frequency probability (頻率機率): An objective probability value obtained based on past experience or statistical data, usually the frequency of past events or repeated experiments to obtain the probability of an event occurring.
\item Subjective probability (主觀機率): A probability value that is not supported by statistical data.
\item Conditional probability (條件機率): The probability of another event occurring given that a certain event has occurred. Usually expressed as $P(A|B)$, which is the probability of event $A$ occurring given that event $B$ has occurred.$P(A|B)= \frac{P\qty(A\cap B)}{P\qty(B)}$.
\item Mutually exclusive events (互斥事件): Two or more events cannot occur at the same time, that is, $A_1,A_2,\ldots,A_n$ are mutually exclusive events if and only if:
\[\forall J\neq\varnothing\land J\subseteq\{a\mid a\in\mathbb{N}\land 1\leq a\leq n\}\colon P\qty(\bigcap_{j\in J} A_j)=0.\]
\item Independent events (獨立事件): Two or more events have no effect on each other, that is, $A_1,A_2,\ldots,A_n$ are independent events if and only if:
\[\forall J\neq\varnothing\land J\subseteq\{a\mid a\in\mathbb{N}\land 1\leq a\leq n\}\colon P\qty(\bigcap_{j\in J} A_j)=\prod_{j\in J}P\qty(A_j).\]
\item Repeated trials: A trial that consists of repeated independent trials. The probability of an event $A$ where events $A_i$ occured in each trial $i$ is $\prod_iP(A_i)$.
\item Partitions (分割或劃分): If \( \{A_i\}_{i \in I} \) is a set of partitions of the sample space \( \Omega \) then the following conditions are met:
\[\begin{aligned}
& \forall i \in I\colon A_i \subseteq \Omega,\\
& \forall i,\, j \in I \land i\neq j\colon A_i \cap A_j = \emptyset,\\
& \bigcup_{i \in I} A_i = \Omega. 
\end{aligned}\]
\item Bayes' Theorem (貝葉斯定理或貝氏定理): If \( \{A_i\}_{i \in I} \) is a set of partitions of a sample space \( \Omega \), then,
\[ \forall 1\leq j\leq |I|\colon P(A_j|B)= \frac{P\qty(A_j)\times P\qty(B\left| A_j\right.)}{\sum_{k=1}^{\qty|I|} P\qty(A_k)\times P\qty(B\left| A_k\right.)} \]
\end{itemize}
\subsection{Random Variable (隨機變數)}
The random variable is a measurable function of which the domain is the sample space of a probability space and the range is typically a subset of real numbers. If the range of it is finite or countable infinite, it is called a discrete random variable; if the range of it is continuous, it is called a continuous random variable; if the range of it is continuous in some parts and discrete in others, it is called a mixed random variable. Given ramdom variable $X$ and sample space $\Omega$, $X=x$ means that event $\{\omega \in \Omega \mid X(\omega) = x\}$ occured. If a random variable $X$ follows a probability distribution $P$, we write $X\sim P$.
\ssc{Probability Function (機率函數)}
\sssc{Probability Mass Function (PMF) (機率質量函數) or Probability Function of Discrete Random Variable}
The probability mass function, denoted as $P(x)$, is a function of which the domain is the range of a discrete random variable and the codomain is $[0,1]$, indicating the probability of events, such that the probability sum of the probability mass function of all elements in its domain is 1.
\sssc{Probability Density Function (PDF) (機率分布函數) or Probability Function of Continuous Random Variable}
The probability density function, denoted as $f(x)$, is a function of which the domain is the range of a continuous random variable and the codomain is $[0,1]$, indicating the probability of events, such that the probability sum of the probability density function of all elements in its domain is 1.
\subsection{Cumulative Distribution Function (CDF) (累積分布函數)}
Cumulative distribution function is a function $F\colon\mathbb{R}\to[0,1]$ satisfying
\[\lim_{x\to-\infty}F(x)=0,\quad\lim_{x\to\infty}F(x)=1.\]
\sssc{For discrete random variables}
For a discrete random variable \( X \) with probability mass function \( P(x) \), the cumulative distribution function $F$ is given by:
\[F(x) = P(X\leq x) = \sum_{k \leq x} P(X = k).\]
\sssc{For continuous random variables}
For a continuous random variable \( X \) with probability density function \( f(x) \), the cumulative distribution function $F$ is given by:
\[ F(x) = \int_{-\infty}^xf(t)\,\mathrm{d}t.\]
\subsection{Probability space (機率空間)}
A probability space $(\Omega,\Sigma,\mu)$ is a measure space of which $\Omega$ is the sample space of an experiment, $\mu$ is called the probability measure, indicating the probability of events, such that $\mu\colon\Sigma\to [0,1]$, $\mu(\Omega)=1$. The composition function of the random variable and the probability function of an experiment is the probability measure of that experiment.
\subsection{(Mathematical) Expected Value/Expectation/Expectancy (期望值) or Mean (平均值)}
\sssc{For discrete random variables}
For a discrete random variable \( X \) with probability mass function \( P(x) \), the expected value, $E[X]$ or $E(X)$, or mean $\mu_X$, is given by:
\[ E[X] = \mu_X = \sum_{x\in\text{range}(X)}x\cdot P(x).\]
\sssc{For continuous random variables}
For a continuous random variable \( X \) with probability density function \( f(x) \), the expected value, $E[X]$ or $E(X)$, or mean $\mu_X$, is given by:
\[ E[X] = \mu_X = \int_{-\infty}^{\infty}x\cdot f(x)\,\mathrm{d}x.\]
\subsection{Variance (變異數)}
\sssc{For discrete random variables}
For a discrete random variable \( X \) with probability mass function \( P(x) \), the variance, $\text{Var}(X)$ or $\sigma_X^{\phantom{X}2}$, is given by:
\[\text{Var}(X) = E[(X - E[X])^2] = \sum_{x \in \text{range}(X)} (x - E[X])^2 \cdot P(x).\]
\sssc{For continuous random variables}
For a continuous random variable \( X \) with probability density function \( f(x) \), the variance, $\text{Var}(X)$ or $\sigma_X^{\phantom{X}2}$, is given by:
\[\text{Var}(X) = E[(X - E[X])^2] = \int_{-\infty}^{\infty} (x - E[X])^2 \cdot f(x) \, \mathrm{d}x.\]
\subsection{Standard Deviation (標準差)}
The standard deviation, $\text{SD}(X)$ or $\sigma_X$, is the positive square root of the variance, that is,
\[\text{SD}(X) = \sqrt{\text{Var}(X)}.\]
\ssc{Mode (眾數)}
For a discrete or continuous random variable $X$ with probability function $p(x)$, the mode, $\text{Mode}(X)$, is given by:
\[\text{Mode}(X)=\arg\max_x(p(x)).\]
\ssc{Median (中位數)}
\sssc{For all random variables}
For a random variable \( X \), the median, $\text{Median}(X)$, is an element in the range of $X$ that satisfies:
\[P(X\leq \text{Median}(X))\geq\frac{1}{2}\land P(X\geq \text{Median}(X))\geq\frac{1}{2}.\]
\sssc{For continuous random variables}
For a continuous random variable $X$ with probability density function $f(x)$, the median, $\text{Median}(X)$, is an element in the range of $X$ such that the cumulative distribution function $F(x)$ of it is $\frac{1}{2}$, that is,
\[F(\text{Median}(X))=\int_{-\infty}^{\text{Median}(X)}f(x)\,\mathrm{d}x=\frac{1}{2}.\]
\ssc{Entropy (熵)}
\sssc{For discrete random variables}
For a discrete random variable \( X \) with probability mass function \( P(x) \), the entropy, $H(X)$, is given by:
\[H(X)=-\sum_{i\in D_P}P(i)\log P(i).\]
\sssc{For continuous random variables}
For a continuous random variable \( X \) with probability density function \( f(x) \), the entropy, $H(X)$, is given by:
\[H(X)=-\int_{-\infty}^{\infty}f(x)\log f(x)\,\mathrm{d}x.\]
\subsection{Affine Transformation}
There are two random variable $X$ and $Y=aX+b$ where $a,b\in\mathbb{R}$, then,
\[E[X]=aE[X]+b.\]
\[\text{Var}(Y)=a^2\text{Var}(X).\]
\[\text{SD}(Y)=|a|\text{SD}(X).\]
\[\text{Mode}(Y)=a\text{Mode}(X)+b.\]
\[\text{Median}(Y)=a\text{Median}(X)+b.\]
If $X$ is discrete,
\[H(Y)=\begin{cases}H(X),\quad &a\neq 0\\0,\quad &a=0\end{cases};\]
if $X$ is continuous,
\[H(Y)=\begin{cases}H(X)+\log|a|,\quad &a\neq 0\\0,\quad &a=0\end{cases}.\]
\subsection{Bernoulli Trial or Binomial Trial}
\subsubsection{Bernoulli Trial (伯努力試驗) or Binomial Trial (二項試驗)}
A Bernoulli trial is a random experiment with exactly two possible outcomes, "success" and "failure", in which the probability of success is the same every time the experiment is conducted.
\subsubsection{Binomial Distribution}
If the random variable $X$ follows the binomial distribution with number of Bernoulli trials $n\in\mathbb{N}$ and probability of success $p$, we write $X\sim B(n,p)$. The probability of getting exactly $k$ successes in $n$ independent Bernoulli trials (with the same success probability $p$) is given by the probability mass function:
\[P(X=k)=\binom{n}{k}p^k(1-p)^{n-k}.\]
$P(X=k)$ when $X\sim B(n,p)$ has the following property:
\begin{itemize}
\item If $(n+1)p\in\mathbb{N}\land p\neq 1$, the probability mass function has two modes at $k=(n+1)p$ and $k=(n+1)p-1$; if $p=1$, it has one mode at $k=(n+1)p-1$; otherwise, it has one mode at $\lfloor(n+1)p\rfloor$. The probability mass function is strictly increasing for $k<(n+1)p$ and strictly decreasing for $k>(n+1)p$
\begin{proof}
\[K:=\arg\max_{k\in\mathbb{Z},0\leq k\leq n}\left(\binom{n}{k}p^k(1-p)^{n-k}\right),\quad n\in\mathbb{N},p>0.\]
If $p=1$, it has one mode at $k=(n+1)p-1$; otherwise, let:
\[g(k):=\frac{P(X=k+1)}{P(X=k)}=\frac{(n-k)p}{(k+1)(1-p)}.\]
\[g'(k)=\frac{-(n+1)p}{(k+1)^2(1-p)}\leq 0.\]
\[K-1=\{\min\left(k\text{\ s.t.\ }\frac{n-k}{k+1}>\frac{1-p}{p}\right),\min\left(k\text{\ s.t.\ }\frac{n-k}{k+1}\geq\frac{1-p}{p}\right)\}.\]
Solve $g(k*)=0$ for $k*$:
\[\frac{n-(k^*+1)}{(k^*+1)+1}=\frac{1-p}{p}\]
\[np-(k^*+1)p=(k^*+1)+1-(k^*+1)-p.\]
\[k^*=(n+1)p.\]
\end{proof}
\item The expected value of it is $np$.
\begin{proof}
\[\begin{aligned}
E[X]=&\sum_{k=1}^nk\binom{n}{k}p^k(1-p)^{n-k}\\
=&\sum_{k=1}^nn\binom{n-1}{k-1}p^k(1-p)^{n-k}\\
=&np\sum_{k=1}^n\binom{n-1}{k-1}p^{k-1}(1-p)^{n-k}\\
=&np(p+1-p)^{n-1}\\
=&np.
\end{aligned}\]
\end{proof}
\item The variance of it is $np(1-p)$.
\begin{proof}
\[\begin{aligned}
E[X^2]=&\sum_{k=0}^nk^2\binom{n}{k}p^k(1-p)^{n-k}\\
=&np\sum_{k=1}^nk\binom{n-1}{k-1}p^{k-1}(1-p)^{n-k}\\
=&np\sum_{k=1}^{n-1}k\binom{n-1}{k}p^k(1-p)^{n-1-k}+np\\
=&np(n-1)p+np\\
=&n^2p^2-np^2+np
\end{aligned}\]
\[\text{Var}(X)=E[X^2]-(E[X])^2=n^2p^2-np^2+np-n^2p^2=np(1-p).\]
\end{proof}
\end{itemize}
\subsubsection{Geometric Distribution}
If the random variable $X$ follows the geometric distribution with probability of success $p$, we write $X\sim G(p)$ or $X \sim Geo(p)$. The number of Bernoulli trials (with the same success probability $p$) needed to get one success is given by the probability mass function:
\[P(X=k)=p(1-p)^{k-1}.\]
$P(X=k)$ when $X\sim G(p)$ has the following property:
\begin{itemize}
\item The probability mass function is strictly decreasing.
\begin{proof}
\[g(k):=p(1-p)^{k-1}.\]
\[g'(k)=p\ln(1-p)(1-p)^{k-1}<0.\]
\end{proof}
\item $k>j\in\mathbb{N}$,
\[P(X=k|X>j)=P(X=k-j).\]
\begin{proof}
\[\frac{p(1-p)^k}{(1-p)^j}=p(1-p)^{k-j}.\]
\end{proof}
\item The expected value of it is $\frac{1}{p}$.
\begin{proof}
\[E[X]=\sum_{k=1}^{\infty}kp(1-p)^{k-1}=\sum_{k=1}^{\infty}(k-1)p(1-p)^{k-1}+\sum_{k=1}^{\infty}p(1-p)^{k-1}\]
\[(1-p)E[X]=\sum_{k=1}^{\infty}kp(1-p)^k\]
\[E[X]-(1-p)E[X]=pE[X]=\sum_{k=1}^{\infty}p(1-p)^{k-1}=p\frac{1}{1-(1-p)}=1.\]
\end{proof}
\item The variance of it is $\frac{1-p}{p^2}$.
\begin{proof}
\[E[X^2]=\sum_{k=1}^{\infty}k^2p(1-p)^{k-1}=\sum_{k=1}^{\infty}(k-1)^2p(1-p)^{k-1}+2\sum_{k=1}^{\infty}kp(1-p)^{k-1}-\sum_{k=1}^{\infty}p(1-p)^{k-1}.\]
\[(1-p)E[X^2]=\sum_{k=1}^{\infty}k^2p(1-p)^k.\]
\[E[X^2]-2E[X]+1-(1-p)E[X^2]=pE[X^2]-2E[X]+1=0.\]
\[E[X^2]=\frac{2E[X]-1}{p}=\frac{2-p}{p^2}.\]
\[\text{Var}(X)=E[X^2]-(E[X])^2=\frac{1-p}{p^2}.\]
\end{proof}
\end{itemize}
\ssc{Normal distribution (常態分布/正態分布)/Gaussian distribution (高斯分布)}
\sssc{Probability density function}
The general normal distribution or Gaussian distribution given by mean $\mu$ and standard deviation $\sigma$ is a continuous probability distribution for a real-valued random variable defined by the probability density function:
\[f(x|\mu,\sigma^2)=\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{1}{2}\left(\frac{x-\mu}{\sigma}\right)^2}.\]
\sssc{Cumulative distribution function}
\[F(x|\mu,\sigma^2)=\frac{1}{2}\left(1+\operatorname{erf}\left(\frac{x-\mu}{\sigma\sqrt{2}}\right)\right),\]
where $\operatorname{erf}$ is the error function.
\sssc{Mode}
\[\text{Mode}(x)=\mu\]
\sssc{Median}
\[\text{Median}(x)=\mu\]
\sssc{Entropy}
\[H(x)=\frac{1}{2}\log\qty(2\pi e\sigma^2).\]
\sssc{Notation}
If the random variable $X$ follows the general normal distribution given by mean $\mu$ and standard deviation $\sigma$, we write $X\sim\mathcal{N}(\mu,\sigma^2)$.
\sssc{Standard (標準)/Unit (單位) Normal Distribution}
The standard normal distribution or unit normal distribution is the normal distribution given by mean $\mu=0$ and standard deviation $\sigma=1$.
\begin{itemize}
\item The probability density function of standard normal distribution, $\varphi(z)$, is
\[\varphi(z)=\frac{1}{\sqrt{2\pi}}e^{-\frac{z^2}{2}}.\]
\item The cumulative distribution function of standard normal distribution, $\Phi(z)$, is
\[\Phi(z)=\frac{1}{2}\left(1+\operatorname{erf}\left(\frac{z}{\sqrt{2}}\right)\right).\]
\item PDF of general normal distribution:
\[f(x|\mu,\sigma^2)=\frac{1}{\sigma}\varphi\left(\frac{x-\mu}{\sigma}\right).\]
\item CDF of general normal distribution:
\[F(x|\mu,\sigma^2)=\Phi\left(\frac{x-\mu}{\sigma}\right).\]
\end{itemize}
\end{document}
